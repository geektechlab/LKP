1)
    A device driver has three sides: one side talks to the rest of the kernel, one talks to the hardware and one talks to the user
        ==================
        =                =
        =   User         =  ---------
        =                =          |
        ==================          |
            |                       |
            |                       |
            |                       |
        ==================          |
        =                =          |
        =   Kernel       =          |
        =                =          |  Via Device File
        ==================          |
            |                       |
            |Device Driver
            |                       |
        ==================          |
        =                =          |
        =   Hardware     = ---------
        =                =
        ==================
    Traditional way of adding code to the kernel was to recompile the kernel and reboot the system. Instead of that, Loadable Kernel Modules (LKM) are piece of code that can be loaded/inserted and unloaded/removed from the kernel as per the demand/need. In boot process, base kernel is loaded first and then after the kernel image is loaded successfully it has initialized all the hardware peripherals it will start mounting the root file system and then it will load the kernel modules which are present in lib modules. Modules can save you memory, because you have to have them loaded only when you're actually using them ( the base kernel will always be present in the ram in the memory, but with modules we add/remove the code from the base kernel ) As the kernel modules are loaded very late in the boot process, hence core functionality has to go in the base kernel (E.g. Memory Management) ( all the kernel code can not be added as a kernel module because the core functionality like memory management cannot be configured as kernel module because the kernel modules as we have discussed they will be loaded late in the boot ) ( once the kernel is completely initialized and it has mounted the root file system then only the kernel can load the kernel modules hence the core functionality cannot be part of the kernel module ). Whenever we load an out of tree module it taints the kernel.

    All modules are installed in the /lib/modules/<kernel version> directory of the rootfs by default. Find kernel modules count by: cd /lib/modules/`uname -r`/kernel/ | find . -name '*.ko' | wc -l

    To support modules, the kernel must have been built with the following option enabled: CONFIG_MODULES=y. We can check it at cat /boot/config-`uname -r` | grep CONFIG_MODULES

    1. In-Source Tree: Modules present in the Linux Kernel Source Code which are present in the linux kernel source code so we download the source code from kernel.org can be configured as kernel modules )
    2. Out-of-Tree: Modules not present in the Linux Kernel Source Code which are not part of the kernel source code and which are distributed separately which by the vendor. All modules start out as "out-of-tree" developments, that can be compiled using the context of a source-tree.

    1. List Modules: (lsmod) lsmod gets its information by reading the file /sys/modules. ( we can check it by "strace lsmod" ) ( also we can check by "ls /sys/modules" )
    2. Module Information: (modinfo) : prints the information of the module. ( it provides various info such as word magic value and then you have what all parameters which are which can be passed to this module so while loading the module )

    printk() writes to the kernel buffer which can be read by dmesg, whereas printf() writes on the standard output. printk(KERN_log_priority "hello world\n"); printf() is a function in the C Standard Library. printk() is a kernel level function. printk(KERN_log_priority "hello world\n"); Here, log_priority is one of the eight values (predefined in linux/kernel.h, similar to /usr/include/sys/syslog.h)
        EMERG,
        ALERT,
        CRIT,
        ERR,
        WARNING,
        NOTICE,
        INFO,
        DEBUG (in order of decreasing priority).

    To Build Modules ( we are using /lib/modules/`uname -r`/build/Makefile, but it looks for another makefile in current folder which tells which modules to build. We specify this other makefile through M=${PWD} ):
        make -C /lib/modules/`uname -r`/build M=${PWD} modules
    To clean:
        make -C /lib/modules/`uname -r`/build M=${PWD} clean
    Once built, check generated module by
        file ./hello.ko
        modinfo ./hello.ko
    Load it by
        sudo insmod ./hello.ko ( because only root user can load module )
    Check loading by
        lsmod
    We can remove module by
        sudo rmmod hello
    Verify removal by lsmod Or check by ls /sys/module/hello/
    We need to check print messages by printk by
        dmesg

    Kernel modules must have at least two functions:
        a "start" (initialization) function : module_init(test_hello_init); which is called when the module is loaded into the kernel
        an "end" (cleanup) function called : module_exit(test_hello_exit); which is called just before it is removed

    When we do insmod on a module, it performs a series of steps:
    a) It calls init_module() to intimate the kernel that a module is attempted to be loaded and transfers the control to the kernel
    b) In kernel, sys_init_module() [ https://elixir.bootlin.com/linux/latest/source/include/linux/module.h#L76 ] is run. It does a sequence of operations as follows, we can check it by [ strace insmod ./hello.ko ]:
            --> Verifies if the user who attempts to load the module has the permission to do so or not
            --> The load_module function assigns temporary memory and copies the elf module from user space to kernel memory using copy_from_user
            --> It then checks the sanity of the ELF file ( Verification if it is a proper ELF file )
            --> Then based on the ELF file interpretation, it generates offset in the temporary memory space allocated. This is called the convenience variables
            --> User arguments to the module are also copied to the kernel memory
            --> Symbol resolution is done
            --> The load_module function returns a reference to the kernel module.
            --> The reference to the module returned by load_module is added to a doubly linked list that has a list of all the modules loaded in the system
            --> Then the module_init function in the module code is called

    dmesg: Kernel keeps all the logs in a ring buffer. This is done to avoid the boot logs being getting lost until the syslog daemon starts and collects them and stores them in /var/log/dmesg. We will loss the boot up logs if we don't store them in ring buffer. dmesg command is used to control or print kernel ring buffer. Default is to prints messages from the kernel ring buffer on to console. ( we can check by strace dmesg -> it will write to STDOUT )

    when the kernel boots, there is no root file system. This means it does not have capability to write to any file so that's the reason why dmesg stores contents in a ring buffer. Once the system is up, syslog daemon starts and it will collect contents of the ring buffer and store them in a file which is specific to the distribution. So, if you look into the process list we'll be seeing there's a syslog daemon running ( check by [ ps -ef | grep syslog ] ) so which is actually reading periodically reading the kernel buffer contents of kernel buffer and it's writing into some file in /var/log which depends on the distribution ( check by [ cat /var/log/kern.log ] ).

    whenever we load an out of tree module it taints the kernel. Module should specify which license you are using MODULE_LICENSE() macro:
        "GPL"               [GNU Public License v2 or later]
        "GPL v2"            [GNU Public License v2]
        "GPL and additional rights" [GNU Public License v2 rights and more]
        "Dual BSD/GPL"          [GNU Public License v2 or BSD license choice]
        "Dual MIT/GPL"          [GNU Public License v2 or MIT license choice]
        "Dual MPL/GPL"          [GNU Public License v2 or Mozilla license choice]
        "Proprietary"           [Non free products]

    Every kernel module needs to include linux/module.h. for macro expansion of module_init and module_exit linux/kernel.h only for the macro expansion for the printk() log level.

2)
    kernel use kbuild system to build the kernel modules. kbuild system reads the assignment of "obj-m := modulename.o"  from the makefile. Now the kbuild system know that it has to build "modulename.ko" and will look for "modulename.c" for the source. In case these files are not present in the directory passed to "M" , the compiling will stop with an error. If the files are present the source file is compiled to a "modulname.o",  and "modulename.mod.c" is created which is compiled to "modulename.mod.o". The modulename.mod.c is a file that basically contains the information about the module (Version information etc). The modulename.o and the modulename.mod.o are linked together by modpost in the next stage to create the "modulename.ko".

    Other Files:
    - "module.symvers": This will contain any of external symbols that is defined in your module and hence not present in the module.symvers of the kernel .
    - "modules.order" : In case you are compiling multiple modules together, it will list out the order in which the compilation and creation of .ko takes

    - insmod:       Loads the module given 'insmod /path/to/module.ko'. Dependencies if present are not loaded.
    - modprobe – Add or Remove modules from the kernel. Loads the module only in /lib/modules/$(uname -r) 'modprobe /home/test/hello.ko' will not work. modprobe calculates dependencies, loads the dependencies and then the main module. Modprobe depends on depmod tool to calculate dependencies. depmod calculates dependencies of all the  modules present in /lib/modules/$(uname -r) folder, and places the dependency information in /lib/modules/$(uname -r)/modules.dep file ( try to [ cat /lib/modules/$(uname -r)/modules.dep ] ). E.g. kernel/drivers/net/wireless/admtek/adm8211.ko: kernel/net/mac80211/mac80211.ko kernel/net/wireless/cfg80211.ko kernel/drivers/misc/eeprom/eeprom_93cx6.ko

    When you say modprobe adm8211.ko, eeprom_93cx6.ko, cfg80211.ko is loaded first and then adm8211.ko. Modules are loaded right to left and removed left to right. So while removing adm8211.ko is removed, then cfg80211.ko and finally eeprom_93cx6.ko. We can re-load the modules.dep file by running "depmod -a" command

    In insmod, the function passed in the module_init macro is called, and on rmmod, the argument passed in the module_exit is called.

    The command line parameters through argc/argv provides a single linux driver to do multiple things, for example instead of fixing to a single I/O address for read/write, it can provide that as command line argument and allow user to read/write any address, Enable/disable debug logs/printk, Allow user to set the mode if the driver supports multiple modes.

    We can add parameters using module_param macro. Declared in moduleparam.h file
    #define module_param(name, type, perm)              \
        module_param_named(name, name, type, perm)

    name: name of the variable
    type: Type of the Variable. Supported types are charp, bool, invbool, int, long, short, uint, ulong, ushort
    perm: Permissions for the sysfs entry.
    E.g. S_IRUGO : Only read by all users
           0 : No sysfs entry
    You can also use numeric values like 0644 for permission entry.

    we can check our module's argumnets at [ ls sys/module/<module_name>/parameters ]
    and module's argumnets' permissions at [ ls sys/module/<module_name>/parameters ]
    we can check assigned values to arguments via [ cat sys/module/<module_name>/parameters/<argument> ]
    How to pass parameters: sudo insmod ./argument.ko name="EMBED" loop_count=5
    How can we pass arguments which are called by modprobe? modprobe reads /etc/modprobe.conf file for parameters.

    How can we pass string in argument? If we run the following command: "insmod argument.ko name="Linux World". We get the error "Unknown parameter 'World' ignored" in dmesg. This happens because shell removes double quotes and pass it to insmod, to avoid this add a single quotes over the string. Run the following command: "insmod argument.ko name='"Linux World"' to pass the whole string

    To pass multiple parameters we need to pass parameter array. To pass array we need to use module_param_array() function instead of module_param() function: sudo insmod ./parameter_array.ko param_array=1,2,4

    A symbol is a name given to a space in the memory which stores:
     - data (Variables, For reading and writing)
     - instructions (Functions, for executing)
    So symbol in the programming language is either a variable or function.

    What is Symbol table? Data Structure created by compiler containing all the symbols used in the program. Every kernel image that you build has a symbol table with it. The Linux kernel symbol table contains names and addresses of all the kernel symbols. When you install the kernel it will be present in /boot/System.map-<linux_version>

    System.map-<linux_version> is generated while compiling kernel. When we install it, it resides in boot folder.

    How to Export your symbols? When you define a new function in your module, the default behavior of this function is local, only the module in which the function is defined can access it, cannot be accessed by other modules. To export this module we need to use EXPORT_SYMBOL or EXPORT_SYMBOL_GPL. Once you export them, they will be available to other modules to use.

    Difference between EXPORT_SYMBOL and EXPORT_SYMBOL_GPL
    EXPORT_SYMBOL: The exported symbol can be used by any kernel module
    EXPORT_SYMBOL_GPL: The exported symbol can be used by only GPL licensed code.

    What is the difference between System.map and /proc/kallsyms?
    /proc/kallsyms: Contains symbols of dynamically loaded modules as well as builtin modules
    System.map: Contains symbols of only builtin modules. Because System.map-<linux_version> is generated while compiling kernel. When we install it, it resides in boot folder.

    $cat /boot/System.map-* | grep ttyprintk_exit
    $cat /proc/kallsyms | grep ttyprintk_exit

    What is module stacking? New modules using the symbols exported by old modules. Examples of modules stacking in Linux Kernel:
    - Msdos filesystem relies on symbols exported by fat module
    - Parallel port printer driver (lp) relies on symbols exported by generic parallel port driver (parport)

    Vermagic is a magic string present in the Linux Kernel and added into the .modinfo section of the Linux Kernel Modules. This is used to verify whether the kernel module was compiled for the particular kernel version or not. ‘VERMAGIC_STRING’ is generated by the kernel configuration.
    #define VERMAGIC_STRING                         \
        UTS_RELEASE " "                         \
        MODULE_VERMAGIC_SMP MODULE_VERMAGIC_PREEMPT             \
        MODULE_VERMAGIC_MODULE_UNLOAD MODULE_VERMAGIC_MODVERSIONS   \
        MODULE_ARCH_VERMAGIC                        \
        MODULE_RANDSTRUCT_PLUGIN

    we can check Vermagic of module at
    - dmesg once module is loaded
    - modinfo ./vermagic.ko
    we can check Vermic of kernel at [ uname -a ]

3)
    tainting of kernel will be stored in dmesg only once. re-loading will not display that message. At runtime, you can query the tainted state by reading $cat /proc/sys/kernel/tainted. After [reboot], value of above file will be 0. The easiest way to decode that number is the script kernel-chktaint ://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/plain/tools/debugging/kernel-chktaint. Running above script will tell the reason of tainting. For raw taint value decoding from the message, check flags.txt in this directory.

    How to find the version of a compiled kernel module? modinfo can be used for this. [ modinfo <.ko file> ]. Module Metadata:
    - MODULE_DESCRIPTION can be a short synopsis of what your module is trying to accomplish
    - MODULE_AUTHOR declares the module’s author
    - MODULE_VERSION macro sets the version of the module

    When the kernel is tainted, it means that it is in a state that is not supported by the community. In addition, some debugging functionality and API calls may be disabled when the kernel is tainted. Reasons:
    - The use of a proprietary (or non-GPL-compatible) kernel module—this is the most common cause of tainted kernels and usually results from loading proprietary NVIDIA or AMD video drivers
    - The use of staging drivers, which are part of the kernel source code but are not fully tested
    - The use of out-of-tree modules that are not included with the Linux kernel source code
    - Certain critical error conditions, such as machine check exceptions and kernel oopses

    Understanding modinfo output:
    - vermagic: When loading a module, the strings in the vermagic value are checked if they match. If they don't match you will get an error and the kernel refuses to load the module.
    - intree: All kernel modules start their developments as out-of-tree. Once a module gets accepted to be included, it becomes an in-tree module.
    - srcversion: is an MD4 hash of the source code used to compile the kernel module. It is calculated automatically at build time from modpost script. Can be used for checking if given .ko is loaded by user or he is using previous only when we distribute .ko to customer.
    - retpoline: "Retpoline" was introduced to be a solution to mitigate the risk of Spectre bug.

    An ELF object file consists of various named sections. Some of them are basic parts of an object file, for example the .text section contains executable code that a loader loads. To see all the sections: $ objdump --section-headers ./mod_info.ko
    To see the contents of the .modinfo section: $ objdump --section-headers --section=.modinfo --full-contents ./mod_info.ko

    Printk is implemented by using a ring buffer in the kernel with a size of __LOG_BUF_LEN bytes where __LOG_BUF_LEN equals (1 << CONFIG_LOG_BUF_SHIFT). Calling dump_stack() will cause a stack trace to be printed at that point.

    Kernel Panic is an error in the kernel code and will stop running immediately to avoid data loss or other damage. The reason to stop running is to protect your computer. reasons:
    - Hardware or Software Issue (e.g. unable to start init process) ( init process is first process )
    - Bug in the kernel driver ( e.g. NULL pointer dereference )
    - Defective or Incompatible RAM

    When kernel decides to Panic, it calls the panic() function which dumps some debug information and depending on the configuration reboots the system. By default, the kernel will not reboot on Kernel Panic. There are two ways by which you can instruct the kernel to reboot:
    - Kernel Command line: Add "panic=N" to the kernel command line, for the kernel to reboot after N seconds
    - Proc File system: echo N > /proc/sys/kernel/panic , for kernel to reboot after N seconds on reboot. Note this setting is not persistent on reboot.

    How to check kernel commandline to know to reason of boot/reboot:
    - dmesg initial log will have kernel command line printed
    - cat /proc/cmdline

    An OOPS is similar to segfault in user space. Kernel throws oops message when an exception such as accessing invalid memory location happens in the kernel code. Upon OOPS, the kernel performs the following operations:
    - Kills the offending process
    - Prints information which can help the developers to debug
    - Continues execution. Note: After oops, the system cannot be trusted further as the some of the locks or structures may not be cleaned up.

    An OOPS Message contains the following information:
    - Processor Status
    - Contents of the CPU Registers at the time of exception
    - Stack trace
    - Call Trace
    in case of OOPS, system execution continues unlike kernel panic. But system can't be trusted further because some of the locks or structures may not be cleaned up. System continues running further after adding logs to dmesg.

    BUG() macro. we can find many BUG calls in linux source code, [ cd linux-`uname -r` ] [ grep -nr 'BUG' . ]
    - Prints the contents of the registers
    - Prints Stack Trace
    - Current Process dies
    After loading this module using insmod, you cannot unload this. If you try to call rmmod, you will get "Module in use" error.

    WARN() macro:
    - Prints the contents of the registers
    - Prints Stack Trace.
    - But current process will not die unlike BUG()

    Sometimes, an external module uses exported symbols from another external module. kbuild needs to have full knowledge of all symbols to avoid spitting out warnings about undefined symbols. When an external module is built, a Module.symvers file is generated containing all exported symbols which are not defined in the kernel. Use KBUILD_EXTRA_SYMBOLS and provide it the path of the Module.symvers file if it is present in some other directory other than the module directory.

    can I load any file to kernel ? lets do, [ touch hello.ko ], then [ chmod +x hello.ko ], then [ insmod hello.ko ]. It's not possible.

    insmod is user space utility. What happens when we do insmod on a module ? What is a kernel module? Kernel module is a piece of kernel code which can be added to the running kernel when loaded and can be removed from the kernel when the functionality is removed.

4)
    For Linux, a console is a device to which you can write text data and read text data. By default, the console is the screen and keyboard. When one boots his PC, the kernel prints a lot of messages, like "initializing this...", "initializing that...". These all get printed via printk that sends the message to the console driver. In Linux, graphics mode is implemented not inside the kernel but in user space and thus it cannot print messages in graphics mode, but as a usermode process called X sends a message via IPC to the X server and says it how the X server should draw the window. This message passing is implemented in a shared library. To display all kernel messages, [ dmesg -n 5 ].

    [ cat /proc/sys/kernel/printk ] will display 4 numbers. They are associated with the following variables:
        - console_loglevel: level under which the messages are logged on the console device
        - default_message_loglevel: priority level that is associated by default with messages for which no priority value is specified
        - minimum_console_loglevel: minimum level to allow a message to be logged on the console device
        - maximum_console_loglevel: maximum level

    $echo 8 > /proc/sys/kernel/printk Will change the console_loglevel

    pr_error etc. message displays more info than printk().

    When a user-space process uses floating-point instructions, the kernel manages the transition from integer to floating point mode. Many programs don't use floating point or don't use it on any given time slice and saving the FPU registers and other FPU state takes time; therefore an OS kernel may simply turn the FPU off. Presto, no state to save and restore, and therefore faster context-switching.

    If a program attempts an FPU op, the program will trap into the kernel and the kernel will turn the FPU on restore any saved state that may already exist, and then return to re-execute the FPU op. At context switch time, it knows to actually go through the state save logic. (And then it may turn the FPU off again.) The reason that the kernel doesn't particularly need FPU ops and also needs to run on architectures without an FPU at all.

    A function, printk_ratelimit is to restrict the logging using which we can set a limit on the number of prints that we want our program to do. The limit on the number of prints is set in the file /proc/sys/kernel/printk_ratelimit_burst
    $ cat /proc/sys/kernel/printk_ratelimit_burst

5)
    num_online_cpus() gives online cpu.

    process is a program running in memory. Linux kernel internally refers processes as tasks. Kernel stores the list of processes in a circular doubly linked list called the task list. Each task/process is represented in kernel with struct task_struct (defined in <linux/sched.h>). This data structure (task_struct) is huge (1.7 Kilobytes) containing all the information about a specific process. Let's write a module/device driver which reads the circular linked list and prints the following information for us:
        - Process Name
        - Process ID
        - Process State

    Before that, we should know what are the different states a process can be:
        - TASK_RUNNING(R): Process is either currently running or on a run-queue waiting to run
        - TASK_INTERRUPTIBLE(S) sleep: Process is sleeping/blocked for some resource say I/O. Can be runnable/awaken by a signal
        - TASK_UNINTERRUPTIBLE(D) sleep: Similar to TASK_INTERRUPTIBLE, but does not wakeup on a signal. Used in cases where it is waiting for critical hardware ( e.g. I/O resource ) and can't be interrupted.
        - __TASK_STOPPED(T): Process execution has stopped. This happens when the task receives SIGSTOP, SIGTSTP, SIGTTIN or SIGTTOU signal or if it receives any signal while it is being debugged.
    You can find the states using ps -el command.

    While writing a module if we want to get information about the current process that is running in the kernel, we need to read the "task_struct" of the corresponding process. The kernel provides a easy way to do this by providing a macro by the name "current", which always returns a pointer to the "task_struct" of the current executing process. Some architectures stores this in a register some stores them in bottom of kernel stack of process.

    Process Memory Map: struct mm_struct - contains list of process VMAs, page tables, etc. All information related to the process address space is included in an object called the memory descriptor of type mm_struct accessible via current-> mm.
    struct mm_struct {
        /* Pointer to the head of the list of memory region objects */
        struct vm_area_struct * mmap;
        /* Pointer to the root of the red-black tree of memory region objects */
        struct rb_root mm_rb;
        /* Pointer to the last referenced memory region object */
        struct vm_area_struct * mmap_cache;
        ....
    };

    Linux implements a memory region by means of an object of type vm_area_struct
    struct vm_area_struct {
        struct mm_struct * vm_mm;   /* Pointer to the memory descriptor that owns the region */
        unsigned long vm_start;   /* First linear address inside the region */
        unsigned long vm_end;   /* First linear address after the region */
        ....
    };

    Each memory region descriptor identifies a linear address interval. ( vm_end - vm_start ) denotes the length of the memory region. All the regions owned by a process are linked in a simple list. Regions appear in the list in ascending order by memory address. The vm_next field of each vm_area_structelement points to the next element in the list.

    A Kernel Thread is a Linux Task running only in kernel mode. It is not created by running fork() or clone() system calls. run [ ps -ef ] and whatever threads are in [], those are kernel threads. Kernel Threads helps the kernel to perform operations in background. examples of Kernel Thread:
        - ksoftirqd is Per CPU kernel thread runs processing softirqd.
        - kworker is a kernel thread which processes work queues.

    differences between Kernel Thread and User Thread:
    Both Kernel Thread and User Thread are represented by task_struct. The main difference is that there is no address space in kernel threads. mm variable of task_struct is set to NULL ( https://github.com/firmianay/Life-long-Learner/blob/master/linux-kernel-development/chapter-15.md ). Because kernel threads do not have address space and do not have any pages in user-space, they do not have memory descriptor and page tables. ( Read Robert Love. )

    How to Create a Kernel Thread?
        #include <linux/kthread.h>
        struct task_struct *kthread_create(int (*threadfn)(void *data), void *data, const char name[], ...)
            Parameters:
            threadfn -> the function which thread should run
            data -> Argument for thread function
            name -> Printf style format for the name of kernel thread.
            Return Value: Pointer to struct  task_struct
        Note: kthread_create only creates the thread but doesn't run the thread, we need to call wake_up_process() with the return value of kthread_create as an argument to the wake_up_process for the thread function to run.

    kthread_create() only creates thread but pthread_create() creates thread and starts as well. Note: If you don't stop the kernel thread in your module_exit function and kernel thread is running, you will get oops message. kthread_stop is a blocking call, it waits until the function executed by thread exits. kthread_stop() flag sets a variable in the task_struct variable which the function running in while(1) should check in each of its loop.
        int threadfunc(void *data)
        {
             while(!kthread_should_stop())
             {
                        //perform operations here
              }
            return 0;
        }

    we can have same name of threads. When you have multiple processors present in the system, and want to find out on which the processor your driver code is running, use smp_processor_id().

6)
    In Linux everything is considered to be a file so devices are also considered to be a file. In UNIX, hardware devices are accessed by the user through special device files. These files are grouped into the /dev directory, and system calls open, read, write, close, lseek, mmap etc. are redirected by the operating system to the device driver associated with the physical device. In the UNIX world there are two categories of device files and thus device drivers: character and block. This division is done:
    - by the speed,
    - volume and
    - way of organizing the data to be transferred from the device to the system and vice versa.

    In the first category, there are slow devices, which manage a small amount of data, and access to data does not require frequent seek queries. Examples are devices such as keyboard, mouse, serial ports, sound card, joystick. In general, operations with these devices (read, write) are performed sequentially byte by byte.

    The second category includes devices where data volume is large, data is organized on blocks, and search is common. Examples of devices that fall into this category are hard drives, cdroms, ram disks, magnetic tape drives. For these devices, reading and writing is done at the data block level. For the two types of device drivers, the Linux kernel offers different APIs. If for character devices system calls go directly to device drivers, in case of block devices, the drivers do not work directly with system calls. In the case of block devices, communication between the user-space and the block device driver is mediated by the file management subsystem and the block device subsystem.

    ll /dev
    if c at start, it is character device
    if d at start, it is directory
    if b at start, it is block device

    Character Device Driver: A character device typically transfers data to and from a user application — they behave like pipes or serial ports, instantly reading or writing the byte data in a character-by-character stream.

    Command to list all the character device driver: $ls -l /dev/ | grep "^c"

    Command to list all the block device driver: $ls -l /dev/ | grep "^b"

    all ttys are serial ports. sda are block devices.

    Steps in creating a character driver:
    1. Allocate a device number dynamically or statically (dev_t)
    2. Initializing the character device with its file operations (struct cdev, struct file_operations)
    3. Registering the character device with Linux Kernel (cdev_add)

    Connection between the application and the device file is based on the name of the device file. However the connection between the device file and the device driver is based on the number of the device file, not the name. A device ID/number consists of two parts:
    - Major Number : identifies the device type (IDE disk, SCSI disk, serial port, etc.)
    - Minor Number : identifies the device (first disk, second serial port, etc.)

    Most times, the major identifies the driver, while the minor identifies each physical device served by the driver. Certain major identifiers are statically assigned to devices (in the Documentation/admin-guide/devices.txt file from the kernel sources).

    ls -l /dev/ttyS*
    crw-rw---- 1 root dialout 4, 64 Apr 12 23:18 /dev/ttyS0
    crw-rw---- 1 root dialout 4, 65 Apr 12 23:18 /dev/ttyS1
    crw-rw---- 1 root dialout 4, 74 Apr 12 23:18 /dev/ttyS10
    crw-rw---- 1 root dialout 4, 75 Apr 12 23:18 /dev/ttyS11
    crw-rw---- 1 root dialout 4, 76 Apr 12 23:18 /dev/ttyS12
    crw-rw---- 1 root dialout 4, 77 Apr 12 23:18 /dev/ttyS13
    crw-rw---- 1 root dialout 4, 78 Apr 12 23:18 /dev/ttyS14
    In columns 5 and 6 of the result you can see the major, respectively the minor for each device.

    When choosing the identifier for a new device, you can use two methods:
        static (choose a number that does not seem to be used already)
        dynamic (kernel will give you a device number)

    Data Type: A device ID/number is represented using the type dev_t. 12 bit major number + 20 bit Minor number =32 bit dev_t.

    Header File: linux/kdev_t.h
    To obtain the major or minor parts of a dev_t, use: MAJOR(dev_t dev); MINOR(dev_t dev);
    To create a device number from major and minor number: MKDEV(int major, int minor);

    /proc/devices:
    This file displays the various character and block devices currently configured. The output from /proc/devices includes the major number and name of the device. Output is broken into two major sections:
    - Character devices and
    - Block devices.

    Difference between static and dynamic method of major and minor number allocation: Static method is only really useful if you know in advance which major number you want to start with. With Static method, you tell the kernel what device numbers you want (the start major/minor number and count) and it either gives them to you or not (depending on availability).

    With Dynamic method, you tell the kernel how many device numbers you need (the starting minor number and count) and it will find a starting major number for you, if one is available, of course. Partially to avoid conflict with other device drivers, it’s considered preferable to use the Dynamic method function, which will dynamically allocate the device numbers for you.

    Dynamic Allocation will allocate the major number dynamically to your driver which is available. It is advantageous over static method because conflict will not happen and kernel will allocate available number automatically. Minor number is still at our wish.

    Header File: <linux/fs.h>

    we can't use same major number and minor number combination which already has been used. But character and block device can have same major and minr number combination because noth device types are different. Try passing same major number which is already used by finding it from [ cat /proc/devices ]. however, we can use same device name.

    CHRDEV_MAJOR_MAX is an artificial limit foe maximum major number (chosen to be 511).

    Suppose if we create duplicate copy of .ko then even we will not be able to load same module again because, name of module will remain same. check it by [ modinfo <name>.ko ].

    There are 2 segments for free char majors. First starting point can be any, we don't know but ends at 234. Second segment starts at 511 and grows reverse and ends at 384. Marks the top and bottom of the second segment of free char majors. After these two segments utilized, dynamic allocation will start failing. Those are defined here:
    /* fs/char_dev.c */
    #define CHRDEV_MAJOR_MAX 512

    We can check dynamic allocations live using [ dmesg ] by running as daemon [ & ]. also we can check later using [ cat /proc/devices/

    Device file is independent of device driver. We can create device file before creating device driver also. Device file can be created in two ways:
        1. Manual
        2. Automatic

    We can create the device file manually by using mknod. $ mknod -m <permissions> <name> <device type> <major> <minor>
    -m <permissions> – optional argument that sets the permission bits of the new device file to permissions
    <name> – your device file name that should have full path (/dev/name)
    <device type> – Put c or b
        c – Character Device
        b – Block Device
    <major> – major number of your device
    <minor> – minor number of your driver

    Eg.
    $sudo mknod -m 0644 /dev/mydevice c 244 10

    Traditionally, device nodes were stored in the /dev directory on Linux systems. There was a node for every possible type of device, regardless of whether it actually existed in the system. The result was that this directory took up a lot of space.

    udev introduces a new way of creating device nodes. It compares the information made available by sysfs and creates nodes. udev can be further configured using its configuration files to tune the device file names, their permissions, their types, etc.

    /dev provides a way for user process to access device files to read, write, send IOCTL command etc. Whereas, /sys sysfs file system exposes detailed device information, atributes and kernel objects to user space for introspection and configuration which can be read/written for debugging and configuration.

    So, as far as driver is concerned, the appropriate /sys entries need to be populated using the Linux device model APIs declared in <linux/device.h> and the rest would be handled by udev.

    class_create — create a struct class structure
        struct class * class_create (struct module *owner,
                         const char *name);
        owner   -   pointer to the module that is to “own” this struct class
        name    -   pointer to a string for the name of this class.
        Header File: <linux/device.h>
    This is used to create a struct class pointer that can then be used in calls to class_device_create. class_destroy — destroys a struct class structure. void class_destroy (struct class *cls); Now, the name will appear in /sys/class/<name>.

    $ udevadm monitor
    With this command, you can tap into udev in real time and see what it sees when you plug in different devices. For better understanding of process, see example 7.

    Create a device and register it with sysfs ( creates devnode ). struct device * device_create(struct class *class, struct device *parent, dev_t devt, void *drv_data, const char *fmt, ...); This function can be used by char device classes. A struct device will be created in sysfs, registered to the specified class.
        class   --> pointer to the struct class that this device should be registered to (it returns a pointer )
        parent  --> pointer to the parent struct device of this new device, if any
        devt    -->     the dev_t for the char device to be added
        fmt -->     string for the device's name
        ... --> variable arguments

    If a pointer to a parent struct device is passed in, the newly created struct device will be a child of that device in sysfs. device_destroy() — removes a device that was created with device_create. void device_destroy (struct class *class, dev_t devt);

7)
    adding __init / __exit attribute to function creates an entry in sections in generated .ko file. It can be used to create init or exit code. struct file_operations:
        Purpose: Holds pointers to functions defined by the driver that performs various operations on the device.
        Defined in : linux/fs.h
        struct file_operations {
            struct module *owner;
            loff_t (*llseek) (struct file *, loff_t, int);
            ssize_t (*read) (struct file *, char __user *, size_t, loff_t *);
            ssize_t (*write) (struct file *, const char __user *, size_t, loff_t *);
            [...]
            long (*unlocked_ioctl) (struct file *, unsigned int, unsigned long);
            [...]
            int (*open) (struct inode *, struct file *);
            int (*flush) (struct file *, fl_owner_t id);
            int (*release) (struct inode *, struct file *);
            [...]

        E.g. struct file_operations fops =
        {
        .read = device_read,
        .write = device_write,
        .open = device_open,
        .release = device_release
        };

    The above fops structure has defined four function pointers : For reading, writing, opening and closing the device file. It can be noticed that the signature of the function differs from the system call that the user uses. The operating system sits between the user and the device driver to simplify implementation in the device driver.

    for example, int (*open) (struct inode *, struct file *); has two arguments. Whereas, open system call has file path, permissions, and file flags. So parameters are different in user space and kernel space. Kernel will translate this parameter using VFS layer and then call kernel open where we register our own open call viz. device_open().

    open does not receive the parameter path or the various parameters that control the file opening mode. Similarly, read, write, release, ioctl, lseek do not receive as a parameter a file descriptor. Instead, these routines receive as parameters two structures: file and inode.

    struct cdev: In kernel, each character device is represented using this structure.
        Header File: linux/cdev.h
        struct cdev
        {
            /* module */
            struct kobject kobj;
            struct module *owner;
            const struct file_operations *ops;
            struct list_head list;
            dev_t dev;
            unsigned int count;
        } __randomize_layout;

        void cdev_init(struct cdev *, const struct file_operations *); --> initialize a cdev structure
        struct cdev *cdev_alloc(void); --> Allocates and returns a cdev structure
        int cdev_add(struct cdev *, dev_t, unsigned int minor_count); --> add a char device to the system
        void cdev_del(struct cdev *dev); --> remove a cdev from the system

    The owner field of the structure should be initialized to THIS_MODULE to protect against ill-advised module unloads while the device is active. Find cdev_alloc() vs cdev_init() difference.

    Pseudo-Devices: Devices in Linux (and other Unix clones) do not necessarily have to correspond to physical devices. These are known as pseudo-devices. For example,
        - /dev/urandom generates a stream of pseudo-random numbers (try running head /dev/urandom in a terminal)
        - /dev/null produces no output, but accepts and discards any input (if you wanted to test your download speed without writing any data to your disk, you could download a file to /dev/null by running, e.g., wget http://some.website/big.file > /dev/null).
            Read : Returns End of file (read returns 0)
            Write: Data written is discarded
        - /dev/zero: Used by developers to create a file with no meaningful data but a particular size
            Read: Returns endless bytes of zeroes (\0 characters)
            Write: Data written is discarded
    File: drivers/char/mem.c has the implementation for this devices

    go into linux source code and go into [ ls drivers/char/mem.c ] and chek it's called register_chrdev with MEM_MAJOR argument. check MEM_MAJOR using [ grep ] command and it comes out as 1. then [ cat /proc/devices ] and check [ ls -l /dev/zero ], [ ls -l /dev/null ], [ ls -l /dev/urandom ] and check all has major number 1 and only minor number differs. check chr_dev_init() internals and see how device number creation, assignment and registration etc. happens

8)
    How many times device driver open and release will be called in case of fork? The open and release function is only called once. When you do fork(), it will not create a new file structure and close() will call the release method of the driver only when the counter of the file structure becomes zero. Those called only once because open() and close() are related to device file which is related to hardware not related to number of process created.

    vi ~/linux-5.2.8/include/linux/fs.h check struct file -> count variable.

    struct file:
    Header File: <linux/fs.h>
    struct file is different when compared to FILE of user space program. A FILE is defined in the C library and never appears in kernel code. A struct file, on the other hand, is a kernel structure that never appears in user programs.

    The file structure represents an open file. (It is not specific to device drivers; every open file in the system has an associated struct file in kernel space.) Whenver you open a file, kernel creates a struct file in kernel space. It is created by the kernel on open and is passed to any function that operates on the file, until the last close. After all instances of the file are closed, the kernel releases the data structure. An open file is different from a disk file, represented by struct inode.

    Important Fields:
        struct file {
            //The file mode identifies the file as either readable or writable
            fmode_t                 f_mode;
            //The current reading or writing position. loff_t is a 64-bit value
            loff_t f_pos;
            //These are the file flags, such as O_RDONLY, O_NONBLOCK, and O_SYNC. The file mode identifies the file is opened in either read or write mode ?
            unsigned int            f_flags;
            //The operations associated with the file.
            struct file_operations *f_op;
            //The open system call sets this pointer to NULL before calling the open method for the driver.
            //The driver can use the field to point to allocated data, but then must free memory in the release method before the file structure is destroyed by the kernel
            // private_data is a useful resource for preserving state information across system calls
            void *private_data;
        };

    struct inode: we can verify inode related information using stat command. [ stat hello.c ]
        Header File: <linux/fs.h>
    The inode structure is used by the kernel internally to represent files. An inode uniquely identifies a file in a file system. It represents a file on disk. struct inode is different from struct file. Whenever we open a file, struct file gets created. But when file gets created, struct inode gets created.
        struct inode {
            //mode
            umode_t                 i_mode;
            kuid_t                  i_uid;
            kgid_t                  i_gid;
            //inode number
            unsigned long           i_ino;
            //Contains the actual device number
            dev_t                   i_rdev;
            // Kernel representation of char device
            struct cdev *i_cdev
        };

    Kernel developers have added two macros that can be used to obtain the major and minor numbers from an inode.
    ( MAJOR(inode->i_rdev), MINOR(inode->i_rdev) )
    unsigned int iminor(struct inode *inode);
    unsigned int imajor(struct inode *inode);
    Check example 4 for understanding inode better.

    difference between struct inode vs struct file: if we run userapp or kernel driver multiple times then file strucutre pointer will be different whereas inode will same because file structure will get created again but inode will remain same because it points to same file. Because multiple driver instance will have separate file descriptor but inode is associated with file itself.

    Accessing process address space can not be done directly (by de-referencing a user-space pointer). Because kernel driver can't/shouldn't access user space memory. If we try to do that, then page domain fault will happen and OOPs will happen. Maybe because when kernel tries to access user space space memory it might be used by another process or might be swapped out. Direct access of a user-space pointer can lead to
        - incorrect behavior (depending on architecture, a user-space pointer may not be valid or mapped to kernel-space), ( in ARM architecture, EL0 address access might not be allowed from EL1 ? )
        - a kernel oops (the user-mode pointer can refer to a non-resident memory area) or
        - security issues.

    Proper access to user-space data is done by calling the macros / functions below: #include <linux/uaccess.h>
        put_user(type val, type *address);
        get_user(type val, type *address);
        unsigned long copy_to_user(void __user *to, const void *from, unsigned long n);
        unsigned long copy_from_user(void *to, const void __user *from, unsigned long n)

    The copy_from_user function copies a block of data from user space into a kernel buffer. It accepts:
        destination buffer (in kernel space),
        a source buffer (from user space), and
        a length defined in bytes

    The copy_to_user function copies a block of data from the kernel into user space. This function accepts:
        pointer to a user space buffer,
        a pointer to a kernel buffer, and
        a length defined in bytes.

    put_user function is used to write a simple variable from the kernel into user space. It supports simple types like char and int, but not larger data types like structures or arrays. get_user is used to read a simple variable from user space.

9)
    If we pass structure having a pointer member then copying functions will do shallow copy instead of deep copy. Because it will copy contents from user/kernel space to kernel/space of structure. Not the reference of memory pointed to user space. To avoid this issue, copy individual members.

    strlen can't work directly on user space memory. Proper way to do that it to copy in kernel space. strnlen_user gets the size of a NULL-terminated string in user space.
        long strnlen_user (const char __user *s, long   n);
        s   The string to measure.
        n   The maximum valid length

    The *offp should be updated after each data transfer to represent the current file position after successful completion of the system call (copy_to_user, copy_from_user etc. ). Othrwise it will take same one as previous call. That's why previous example was failing.

    If we create multiple device nodes then While allocating the device numbers, we need to specify the number of minor devices in the count argument.
        int alloc_chrdev_region (dev_t * dev, unsigned  baseminor, unsigned  count, const char * name);
        We need to create an array of struct cdev, and register each of the cdev with one minor number.
    check 5 device nodes created:
    ls /dev/msg*

    Compare 12 example with previous examples and check how it is different. Only alloc_chrdev_region() gets updated and all system calls and its argumnets remain unchanged. In our previous program, each one is operation on same user buffer. But if I want private user buffer for each, use this example.

        #define offsetof(TYPE, MEMBER)  ((size_t) &((TYPE *)0)->MEMBER)
        #define container_of(ptr, type, member) ({  \
                                const typeof ( ((type *)0)->member) *__mptr = (ptr); \
                                (type *)( (char *)__mptr - offsetof(type, member));})

    What happens when a user-space application calls write on a device file, for example we wrote our own null device: [ /dev/my_null ]. So, we execute write using: [ echo "Hello" > /dev/my_null ]. Userspace internally calls GLibC write call which calls write System Call.
        Step1: Write system call in kernel is executed which is present in fs/read_write.c, which calls ksys_write
        Step2: The fd passed by user is an index in the file descriptor table present in the kernel, fdget_pos fetches the struct fd of the particular file
            struct fd {
                struct file *file;
                unsigned int flags;
            };
            static inline loff_t file_pos_read(struct file *file)
            {
                return file->f_pos;
            }
            This position extracts the offset within the file, and calls vfs_write, and then the return value of write call is updated with offset
        Step 3: In vfs_write,
            It checks whether the file was opened in read-only mode
            Checks whether this file has write method
            Whether the passed user buffer is a valid buffer for reading
            Verifies the area for writing is valid and for security permissions
            And calls __vfs_write
        Step 4: Finally in __vfs_write, it calls our write function present in the fops (struct file_operations) present in the struct file. So, this is the way, even if we pass only three arguments from user space, kernel reads the offset from the file and pass it to our write function defined in our driver.
    go to ~/linux-5.2.8/fs and grep -nr 'vfs_write' .

10)
    IOCTL is referred as Input and Output Control. Device files are supposed to represent physical devices. Most physical devices are used for output as well as input. We have write and read system calls for input and output. This is not always enough. The major use of this is in case of handling some specific operations of a device for which the kernel does not have a system call by default. Examples:
        1. Ejecting the media from a “cd” drive,
        2. change the Baud Rate of Serial port
        3. Adjust the Volume
        4. Reading or Writing device registers

    The system call ioctl() is provided for device-specific custom commands (such as format, reset and shutdown) that are not provided by standard system calls such as read(), write().
        int ioctl(int fd, unsigned long request, ...);
        Every device can have its own ioctl commands, which can be
            --> read ioctl's (to send information from a process to the kernel)
            --> write ioctl's (to return information to a process)
            --> both or neither

    See ioctl_list(2) for a list of  many  of  the  known  ioctl() calls. [ man 2 ioctl_list ], important, open and check it. On Linux-based systems the size of a block special device can be obtained using the ioctl request BLKGETSIZE ( check in [ man 2 ioctl_list ] ). It returns the device size as a number of 512-byte blocks

    The ioctl driver method has a prototype that differs somewhat from the user space version. generally, IOCTL is used for custom commands implemented for a particular given device. unlocked_ioctl is custom command for a character driver, which we implemented. Se how it gets assigned in device_fops. See what values are received in kernel space.
        long (*unlocked_ioctl) (struct file *filp, unsigned int cmd, unsigned long arg);
    The filp pointer is the value corresponding to the file descriptor fd passed on by the application and is the same parameters to the open method. The cmd argument ( from user space ) is passed from the user unchanged, and the optional arg argument is passed in the form of an unsigned long

    Most ioctl implementations consist of a big switch statement that selects the correct behavior according to the cmd argument. Programmers much choose a number for the integer command representing each command implemented through ioctl. Normally many programmers choose a set of small numbers starting with 0 or 1 and go up from there. Picking arbitrary number is a bad idea, because:
     - Two device nodes may have the same major number
     - An application could open more than one device and mix up the file descriptors, thereby sending the right command to the wrong device.
     - Sending wrong ioctl commands can have catastrophic consequences, including damage to hardware.

    Example: Program might find itself trying to change the baud rate of non-serial port input stream, such as FIFO or an audio device. also header files provide macros, but do not specify of which type those are. Whether those are char, int etc. To help programmers create unique ioctl command codes, ioctl codes have been divided into four bitfields.
    1. type/magic number:
        8 - bit Wide
        Choose one number after looking into  Documentation/ioctl-number.txt and use it throughout the driver
    2. number:
        8-bits wide
        sequential number you assign to your command
    3. direction:
        Direction of data transfer.
        Possible values:
            _IOC_NONE(NO Data Transfer)
            _IOC_READ   --> Reading from the device, driver must write into userspace
            _IOC_WRITE
            _IOC_READ|_IOC_WRITE
    4. size:
        Size of the user data involved.
        Width depends on the architecture: usually 13 or 14 bits
        You can find its value for your specific architecture in the macro _IOC_SIZEBITS

    Header File: <linux/ioctl.h>. This above header file defines macro that help set up the command numbers as follows:
        _IO(type, nr) (for a command that has no argument)
        _IOR(type, nr, datatype) (for reading data from the driver)
        _IOW(type, nr, datatype) (for writing data to the driver)
        _IOWR(type, nr, datatype) (for bidirectional data transfer)
    Type and number fields are passed as arguments and size field is derived by applying sizeof to the datatype argument.

    Macros to decode information from the ioctl command
    _IOC_TYPE(cmd) /* gets the magic number of the device this command targets */
    _IOC_NR( cmd) /* gets the sequential number of the command within your device */
    _IOC_SIZE(cmd) /* gets the size of the data structure */
    _IOC_DIR( cmd) /* gets the direction of data transfer,
                                    can be one of the following:
                                    _IOC_NONE
                                    _IOC_READ
                                    _IOC_WRITE
                                    _IOC_READ | _IOC_WRITE
                                    */
    As per the POSIX Standard, if an inappropriate ioctl command has been issued , then -ENOTTY should be returned.

    We need to take care of the arg parameter in the ioctl function, because it is a user space ( might be pointing to user space memory ). We can use copy_from_user/copy_to_user to safely copy data from user to kernel and vice versa. These functions are less efficient in case of copying small data items. You can use put_user/get_user for copying 1, 2, 4 or 8 bytes. If you only want to verify the address without transferring data, you can use access_ok.
        Header File: <asm/uaccess.h>
        int access_ok(int type, void *addr, unsigned long size);
        type -> VERIFY_READ/VERIFY_WRITE: depending on action: reading the user space memory or writing it
        addr -> user space address
        size -> Depends on the ioctl command.
            If you need to both read and write, use VERIFY_WRITE, it is a superset of VERIFY_READ. https://lkml.org/lkml/2019/1/4/418

    64 bit ioctl will fail for 32 will -ENOTTY. To use older 32 bit ioctl, we can use compact_ioctl instead of unlock_ioctl. compact_ioctl convert to unlock_ioctl arguments. For generating 32 bit applcation on 64 bit machine need to install when compiling for 32 bit in 64 bit machine.
    $ sudo apt-get install g++-multilib
    gcc userapp.c -o userapp -m32

    Sending a signal from module to process via IOCTL.
        int send_sig(int signal, struct task_struct *task, int priv);
        signal --> Signal to send
        task   --> Corresponds to task_struct corresponding to the process
        priv   --> 0 for user applications, 1 for kernel
    see how pid gets updated.

11)
    If you want to support multiple kernel versions, you'll find yourself having to code conditional compilation directives. The way to do this to compare the macro LINUX_VERSION_CODE to the macro KERNEL_VERSION. LINUX_VERSION_CODE:This macro expands to the binary representation of the  kernel version. One byte for each part of the version release number. Eg. 5.0.0 = 0x050000 = 327680
        Header File: <linux/version.h>
        From Kernel Top Level Makefile
        LINUX_VERSION_CODE = $(VERSION) * 65536 + $(PATCHLEVEL) * 256 + $(SUBLEVEL)
        Eg: 5.0.0 = 5*65536+0*256+0 = 327680

    If you want to stop compilation at preprocessing stage. Add this in your makefile EXTRA_CFLAGS=’-save-temps’. This will generate all the intermediate files in generating .ko. If you need only .i file. $ make -C /lib/modules/`uname -r`/build M=${PWD} hello.i

    KERNEL_VERSION macro is used to build an integer code from the individual numbers that build up a version number.
        #define KERNEL_VERSION(a,b,c) (((a) << 16) + ((b) << 8) + (c))
        Header File: <linux/version.h>

    UTS_RELEASE macro expands to a string describing the version of this kernel tree. Header File: #include <generated/utsrelease.h>

    If you find out how many times your driver code is used by application, we can maintain a static counter variable in open system call and later print it. Sometimes a device needs to be opened once at a time; More specifically, do not allow the second open before the release. To implement this restriction, you choose a way to handle an open call for an already open device: it can return an error (-EBUSY), block open calls until a release operation, or shut down the device before do the open.

    Logic: Initialize the atomic value with 1. In open function, decrement and check whether the value is zero, if zero then return success. If the value is not zero, then increment the value and return EBUSY. In release function, increment the value of atomic variable.

    To allow a single user to open a device in multiple processes but allow only one user to have the device open at a time. To achieve this, two items are needed:
        - an open count
        - uid of the owner of the device.
    The open call grants access on first open but remembers the owner of the device.

    Traditionally Linux/Unix only had two level of privileges:
        1. Root
        2. Non-Root

    No security checks where performed for processes running in root user, whereas processes running in non-root user were subjected to security checks. No intermediate solution was existing at that time. setuid was only the option for the non-root processes to get privileges. Giving all privileges when only few were required was not a good solution and is a target for attack. POSIX Capabilities is a concept which divides root privileges into a set of privileges. These privileges/values then can be independently assigned to the processes, by this way the process will only contain the require privileges and some level of security is achieved. File '/usr/include/linux/capability.h' contains list of capabilities available in Linux or [ man capabilities ].

    Command to find which capabilities are set for a particular file? [ getcap <filename> ]
    Command to set capabilities for a particular file? Each process has three sets of capabilities:
        1. Permitted: capabilities that this process can possibly have. Superset of effective
        2. Effective:  capabilities that this process actually has.
        3. Inheritable: capabilities that this process can pass to a child process

    Each capability is implemented as a bit in each of the bitmap, which can be set or unset. $ setcap cap_sys_boot+ep /path/to/executable. The above command sets 'CAP_SYS_BOOT' capabilities in both extended and permitted bitmap. Capabilities are implemented on Linux using extended attributes in the security namespace. All the major file systems such as ext2, ext3, ext4, JFS, XFS etc support extended attributes.

    When a process tries to perform a privileged operation, the kernel checks whether the particular capability for performing the operation is set in the effective capability bitmap, if yes then it allows, else throws 'permission denied' error.

    CAP_DAC_OVERRIDE: Allows a non-root user full file system access. Bypasses file read, write and execute permission check. DAC stands for "discretionary access control".
        $sudo setcap cap_dac_override+ep userapp
        $getcap userapp

    CAP_SYS_ADMIN is the ability to perform all the system administration operations. Almost near to root. Before performing a privileged operation, a device driver should check that the calling process has the appropriate capability or not. Capabilities checks are performed with the capable function.
        Header File: <linux/sched.h>
        int capable(int capability);

    As you know the open system call takes set of flags as second argument that control opening a file and mode as third argument that specifies permission the permissions of a file if it is created.
        int open(const char *pathname, int flags, mode_t mode);
    The do_sys_open function starts from the call of the build_open_flags function which does some checks that set of the given flags is valid and handles different conditions of flags and mode.
        File: fs/open.c
    Let's look at the implementation of the build_open_flags. This function is defined in the same kernel file and takes three arguments:
        flags - flags that control opening of a file;
        mode - permissions for newly created file;
        The last argument - op is represented with the open_flags structure:
            struct open_flags {
                    int open_flag;
                    umode_t mode;
                    int acc_mode;
                    int intent;
                    int lookup_flags;
            };
    which is defined in the fs/internal.h header file and as we may see it holds information about flags and access mode for internal kernel purposes. Implementation of the build_open_flags function starts from the definition of local variables and one of them is:
        int acc_mode = ACC_MODE(flags);
    This local variable represents access mode and its initial value will be equal to the value of expanded ACC_MODE macro.
        #define ACC_MODE(x) ("\004\002\006\006"[(x)&O_ACCMODE])
        #define O_ACCMODE   00000003
            The "\004\002\006\006" is an array of four chars:
                "\004\002\006\006" == {'\004', '\002', '\006', '\006'}
                    So, the ACC_MODE macro just expands to the accession to this array by [(x) & O_ACCMODE] index. As we just saw, the O_ACCMODE is 00000003. By applying x & O_ACCMODE we will take the two least significant bits which are represents read, write or read/write access modes:
        #define O_RDONLY        00000000
        #define O_WRONLY        00000001
        #define O_RDWR          00000002

    When the process terminates, the release function will be called even if we don't call close() from user space.

    Misc drivers: In UNIX, Linux and similar operating systems, every device is identified by two numbers: a “major” number and a “minor” number. These numbers can be seen by invoking ls -l /dev. Every device driver registers its major number with the kernel and is completely responsible for managing its minor numbers. Every driver needs to register a major number, even if it only deals with a single device. Misc (or miscellaneous) drivers are simple char drivers that share certain common characteristics. The kernel abstracts these commonalities into an API (implemented in drivers/char/misc.c), and this simplifies the way these drivers are initialized. All misc drivers are assigned a major number of 10, but each can choose a single minor number. So, if you have a char driver needs to support multiple devices, it's not the candidate for being a misc driver.

    the sequence of initialization steps that a char driver performs:
        1. Allocates major/minor number using alloc_chrdev_region() and friends
        2. Creates /dev and /sys nodes using class_device_create() function
        3. Register itself as a char driver using cdev_init() and cdev_add()
            static struct miscdevice misc_dev ={
                .minor = 10,
                .name = MYDEV_NAME,
                .fops = &mycdrv_fops,
            };
        misc_register(&misc_dev);

    In the above example, I have statically assigned a minor number 10. You can also request for dynamic minor number assignment by specifying MISC_DYNAMIC_MINOR in the minor field. Each misc driver automatically appears under /sys/class/misc without explicit effort from the driver writer.

    The misc API seems to make your life easier when you're writing a small character driver and do not want to need to allocate a new major number only to use one minor number. It simplifies things, but all the file operations are still available using the fops member of struct miscdevice. The basic difference is you only get one minor number per misc device.

    Loading Modules on Demand: Linux offers support for automatic loading and unloading of modules. This feature avoid wasting kernel memory by keeping drivers in core memory when not in use. This ability to request additional modules when they are needed is particularly useful for drivers using module stacking. To request the loading of a module, call request_module:
        int request_module(const char *module_name);
    Note that request_module is synchronous -- it will sleep until the attempt to load the module has completed. This means, of course, that request_module cannot be called from interrupt context. The return value indicates that request_module was successful in running modprobe, but does not reflect the success status of modprobe itself. When the kernel code calls request_module, a new "kernel thread'' process is created, which runs modprobe in the user context. But we can't remove it dynamically through this.

12)
    Virtual File Systems: There are numerous ways for a device driver (or other kernel component) to provide information to the user or system administrator. One useful technique is the creation of virtual files, in debugfs, /proc or elsewhere. Virtual files can provide human-readable output that is easy to get at without any special utility programs.

    virtual file system reside in RAM memeory. They do not take any space in harddisk or any storage media. /proc/ /debugfs and /sys file systems are example of this. But SATA etc. take space in storage media.

    Proc File System: Proc is a pseudo file system for interfacing with the kernel internal data structures. As a user, you can use proc files for system diagnostics – CPU, memory, Interrupts and many more. You can also configure a lot of parameters like scheduler parameters, kernel objects, memory and more. The common interaction with proc is using cat and echo from the shell. For example:
        # cat /proc/cpuinfo
        # echo "50"> /proc/sys/kernel/sched_rr_timeslice_ms

    It has proc has may modules such as modules, cpuinfo, meminfo. procfs is for processes. sysfs is for kernel. But still some legacy are using procfs.
        cat /proc/cmdline
        cat /proc/<pid>
        cat /proc/misc
        cat /proc/devices
        cat /proc/memeinfo
        cat /proc/uptime

    Creating Directory in /proc:
        Header Files: <linux/proc_fs.h>
        struct proc dir entry* proc_mkdir(const char *name, struct proc dir entry * parent);
            name: The name of the folder that will be created under /proc.
            Parent: In case the folder needs to be created in a sub folder under /proc a pointer to the same is passed else it can be left as NULL

    Creating a proc file:
        Header File: <linux/proc_fs.h>
        static inline struct proc_dir_entry *proc_create(const char *name,
                                 umode_t mode,
                                 struct proc_dir_entry *parent,
                                 const struct file_operations *proc_fops);
            name: The name of the proc entry
            mode: The access mode for proc entry
            parent: The name of the parent directory under /proc
            proc_fops: The structure in which the file operations for the proc entry will be created.

    Removing a proc entry: When a module is removed from the kernel, it should also remove any proc entries it created. The function that enables the removal of proc entry is "remove_proc_entry" which has the following prototype:
        void remove_proc_entry(const char *name, struct proc_dir_entry *parent);
            name: Name of the proc entry that has to be removed.
            parent: In case the proc entry lies in a subdirectory under the proc filesystem, we can pass the subdirectories here.

    Theory4,5,6,7: see example 4,5,6,7 to get complete picture.

    If you only need a single function entry (call) to produce all the desired proc-fs output, just use single_open() and single_release(). single_open() gets a parameter that is the "show" function for the data that is to be written to /proc.
        int single_open(struct file *file, int (*show)(struct seq_file *m, void *p), void *data);

    The "show" function does everything that is needed to write the data, all in one function call. The data value given to single_open() can be found in the private field of the seq_file structure. This is useful either for writing small amounts of data to /proc, for cases in which the output is not iterative, or for cases in which recursion is more appropriate, since the non-single methods don't fit well with recursive techniques. When using single_open(), the programmer should use single_release() instead of seq_release() in the file_operations structure to avoid a memory leak.
        struct proc_dir_entry *proc_create_data(const char *name, umode_t mode,struct proc_dir_entry *parent,
                                                const struct file_operations *proc_fops,void *data);
            name: The name of the proc entry
            mode: The access mode for proc entry
            parent: The name of the parent directory under /proc
            proc_fops: The structure in which the file operations for the proc entry will be created.
            data: If any data needs to be passed to the proc entry.

    To access the data in the proc_dir_structure we need to make use of the function PDE_DATA to which we pass the file pointer. The function in turn returs a pointer to the data that was passed during the creation of the proc entry. See how proc_fs_mul prints 3 times in this example.

    SEQ_START_TOKEN is a special value which can be returned by the start() function. It can be used if you wish to instruct your show() function to print a header at the top of the output. SEQ_START_TOKEN should only be used if the offset is zero, however.

13)
    What is physical address space? The entire range of memory addresses accessible by processors is often referred to as physical address space. 32 Bit systems can have address space of 2^32 = 4 GB. This Physical address space is used by
        --->    RAM
        --->    BIOS
        --->    APIC
        --->    PCI
        --->    Other Memory Mapped I/O Devices
        +------------------+  <- 0xFFFFFFFF (4GB)
        |      32-bit      |
        |  memory mapped   |
        |     devices      |
        |                  |
        /\/\/\/\/\/\/\/\/\/\

        /\/\/\/\/\/\/\/\/\/\
        |                  |
        |      Unused      |
        |                  |
        +------------------+  <- depends on amount of RAM
        |                  |
        |                  |
        | Extended Memory  |
        |                  |
        |                  |
        +------------------+  <- 0x00100000 (1MB)
        |     BIOS ROM     |
        +------------------+  <- 0x000F0000 (960KB)
        |  16-bit devices, |
        |  expansion ROMs  |
        +------------------+  <- 0x000C0000 (768KB)
        |   VGA Display    |
        +------------------+  <- 0x000A0000 (640KB)
        |                  |
        |    Low Memory    |
        |                  |
        +------------------+  <- 0x00000000
    $ cat /proc/iomem. This file shows you the current map of the system's memory for each physical device.

    Virtual Address Space for 32-bit processors: On Linux, every memory address is virtual. They do not point to any address in the RAM directly. Whenever you access a memory location, a translation mechanism is performed in order to match the corresponding phyical memory. On Linux Systems, each process owns a virtual address space. Size of the virtual address space is 4GB on 32-bit systems (even on a system with physical memory less than 4 GB). Linux divides this virtual address space into:
        --->    an area for applications, called user space
        --->    an area for kernel, called kernel space/process space
    The split between the two is set by a kernel configuration parameter named PAGE_OFFSET. This is called 3G/1G Split.
          .------------------------. 0xFFFFFFFF
          |                        | (4 GB)
          |    Kernel addresses    |
          |                        |
          |                        |
          .------------------------.CONFIG_PAGE_OFFSET
          |                        |(x86: 0xC0000000, ARM: 0x80000000)
          |                        |
          |                        |
          |  User space addresses  |
          |                        |
          |                        |
          |                        |
          |                        |
          '------------------------' 00000000
    IMP: User address space is allocated per process, so that each process runs in a sandbox, separated from others.
    IMP: The kernel address space is same for all process; there is only one kernel.

    Why kernel shares its address space with every process:
        --> Every single process uses system calls, which will involve the kernel
        --> Mapping the kernel's virtual memory address into each process virtual address space allows us to avoid the cost of switching out the memory address space on each entry to and exit from the kernel

    if one process updates golbal variable in kernel, then if second process comes and sees variable. Then it will see updated variable. 64- bit memory map:
        ===========================================================================================
            Start addr    |   Offset   |     End addr     |  Size   | VM area description
        ===========================================================================================
                          |            |                  |         |
         0000000000000000 |    0       | 00007fffffffffff |  128 TB | user-space virtual memory
        __________________|____________|__________________|_________|______________________________
                          |            |                  |         |
         0000800000000000 | +128    TB | ffff7fffffffffff | ~16M TB | non-canonical ( what is this ? )
        __________________|____________|__________________|_________|______________________________
                          |            |                  |         |
         ffff800000000000 | -128    TB | ffffffffffffffff |  128 TB | kernel-space virtual memory
        __________________|____________|__________________|_________|______________________________

    Documentation/x86/x86_64/mm.txt. how much RAM installed in machine ? [ free -m ] or /proc/meminfo. [ make menuconfig ] gives kernel configurations in QEMU.

    Kinds of Memory: Kernel and user space work with virtual addresses. These virtual addresses are mapped to physical addresses by memory management hardware (MMU).
        Header File: #include <asm/io.h>
            phys_addr = virt_to_phys(virt_addr);
            virt_addr = phys_to_virt(phys_addr);
        [ uname -a ] shows 64-bit / 32-bit machine type. i686=32-bit, x86_64=64-bit. buildroot generates linux images for lightweight targets. Yocto is for heavy weight.

    Pages: Virtual address space (0x00000000 to 0xffffffff) is divided into pages of 4096 bytes. The page size may differ in other systems. But on ARM and x86 it is fixed. The size of a page is defined in the kernel through the PAGE_SIZE macro. Pages in the virtual address space are mapped to physical addresses by the Memory Management Unit(MMU), which uses page tables to perform the mapping.

    Memory Page/Virtual Page/Page:
        Refers to a fixed length contiguous block of virtual memory.
        Kernel data structure to represent a memory page is struct page.

    Frame/Page Frame:
        Refers to a fixed length contiguous block of physical memory on top of which the OS maps a memory page.
        Each page frame is given a page frame number (PFN).
        Given a page, you can easily get its PFN and vice versa, using page_to_pfn/pfn_to_page macros.

    Page Table:
        Kernel and architecture data structure used to store the mapping between virtual addresses and physical addresses.
        Each entry describes key pair page/frame.

    Command to find out page size: $ getconf PAGESIZE or $ getconf PAGE_SIZE

    Kernel represents every virtual page on the system with struct page structure.
        Header File: <linux/mmtypes.h>
        struct page {
                unsigned long flags;
            atomic_t      _count;
            void          *virtual;
            ....
        };

    Flags: Status of the page: Dirty, locked in memory.
        Values: <linux/page-flags.h>
        _count : Usage count of the page. How many references are to this page. When page is free _count is negative one
        virtual: Page's virtual Address.

    with 4KB Page Size and 4GB of Physical Memory = 1048576 Pages
    Each page is taking 64 bytes = 1048576*64 = 64 MB is used to store all the physical pages

    Kernel memory is managed in a fairly straightforward way. It is not demand-paged, meaning that, for every allocation using kmalloc() or similar function, there is real physical memory. Kernel memory is never discarded or paged out. Linux employs a lazy allocation strategy for user space, only mapping physical pages of memory when the program accesses it. For example, allocating a buffer of 1 MiB using malloc(3) returns a pointer to a block of memory addresses but no actual physical memory. A flag is set in the page table entries such that any read or write access is trapped by the kernel. This is known as a page fault. Only at this point does the kernel attempt to find a page of physical memory and add it to the page table mapping for the process.

    See example 4 to check page fault.

    A page fault is generated when the kernel traps an access to a page that has not been mapped. In fact, there are two kinds of page fault: minor and major. With a minor fault, the kernel just has to find a page of physical memory and map it into the process address space. A major page fault occurs when the virtual memory is mapped to a file, for example using mmap(2). Major faults are much more expensive in time and system resources. Here, program pagefault diffrence is 256 for 1kb request and 1024 for 4kb.

    User space virtual address space:
             address|-------------------| command-line arguments
                    |-------------------| and environment variables
                    |        stack      |
                    |-------------------|
                    |                   |
                    |                   |
                    |                   |
                    |-------------------|
                    |       heap        |
                    |-------------------|
                    |uninitialized data | initialized to
                    |               (bss| zero by exec
                    |-------------------|
                    | initialized data  | read from
                    |-------------------| program file
                    |       text        | by exec
        low address |-------------------|
                Typical memory arrangement

    we can check it using [ $ cat /proc/pid/maps ]. [ cat /proc/self/maps ] gives address space for current process bash.

    Low and High Memory: The Linux kernel has its own virtual address space, as every user mode process does. The kernel code and data structures must fit into that space, but the biggest consumer of kernel address space is virtual mappings for physical memory. The kernel to access physical memory should first map it into the kernel's virtual address space.

    Maximum amount of physical memory handled by the kernel = amount that could be mapped into the kernel's portion of virtual address space - Space used by kernel code.

    As, a result x86 based Linux systems could work with a maximum of a little under 1 GB of physical memory. The virtual address space of the kernel (1 GB sized in a 3G/1G split) is divided into two parts:
        --> Low memory or LOWMEM, which is the first 896 MB
        --> High memory or HIGHMEM, represented by the top 128 MB

                                               Physical mem
           Process address space    +------> +------------+
                                    |        |  3200 M    |
                                    |        |            |
        4 GB+---------------+ <-----+        |  HIGH MEM  |
            |     128 MB    |                |            |
            +---------------+ <---------+    |            |
            +---------------+ <------+  |    |            |
            |     896 MB    |        |  +--> +------------+
        3 GB+---------------+ <--+   +-----> +------------+
            |               |    |           |   896 MB   | LOW MEM
            |     /////     |    +---------> +------------+
            |               |
        0 GB +---------------+

    Low Mem: The first 896 MB of kernel address space constitutes the low memory region. Early in the boot, the kernel permanently maps this 896MB. Addresses that result from this mapping are called logical addresses. These are virtual addresses, but can be translated into physical addresses by subtracting a fixed offset, since the mapping is permanent and known in advance. You can convert a physical address into a logical address using the __pa(address) macro, and then revert it with the __va(address) macro. Low memory matches with the lower bound of physical addresses. In fact, to serve different purposes, kernel memory is divided into a zone. We can then identify three different memory zones in the kernel space:
        ZONE_DMA: This contains page frames of memory below 16 MB, reserved for Direct Memory Access (DMA). Useful for device that can't access other memory regions.
        ZONE_NORMAL: This contains page frames of memory above 16 MB and below 896 MB, for normal use
        ZONE_HIGHMEM: This contains page frames of memory at and above 896 MB
    On a 512 MB system, there will be no ZONE_HIGHMEM, 16 MB for ZONE_DMA, and 496 MB for ZONE_NORMAL. [ check it on QEMU, shown in video 43:00 ]

    High Memory: The top 128 MB of the kernel address space is called the high memory region. It is used by the kernel to temporarily map physical memory above 1 GB. When physical memory above 1GB (or more precisely 896MB) needs to be accessed, the kernel uses those 128MB to create a temporary mapping to its virtual address space, thus achieving the goal of being able to access all physical pages. The physical memory above 896 MB is mapped on demand to the 128 MB of the HIGHMEM region. Mapping to access high memory is created on the fly by the kernel, and destroyed when done. This makes high memory access slower. Concept of high memory does not exist on 64-bit systems, due to the huge address range (2^64), where the 3G/1G split does not make sense anymore. High memeory concep is required only because of 4GB restriction.

    Memory allocation mechanism:
        ----------------------------------
        |   Kernel                       |
        |   Module                       |
        ----------------------------------
            |           |   |       |
            v           |   |       v
        ----------      |   |   -----------
        |         |     |   |   |         |
        |kmalloc  |     |   |   |vmalloc  |
        |allocator|     |   |   |allocator|
        ----------      |   |   -----------
           |            |   |
           v            v   |
        -----------------   |
        | slab          |   |
        | allocator     |   |
        ----------------    |
            |               |
            v               v
        ------------------------------------------
        |   Page Allocator                       |
        |Allocate physical memory by chunk of 4k |
        ------------------------------------------
                |
                |
                v
        ---------------------------------------
        |   Main Memory                       |
        |                                     |
        ---------------------------------------

    There is an allocation mechanism to satisfy any kind of memory request. Depending on what you need memory for, you can choose the one closest to your goal. The main allocator is the page allocator, which only works with pages (a page being the smallest memory unit it can deliver). Then comes the SLAB allocator which is built on top of the page allocator, getting pages from it and returning smaller memory entities (by mean of slabs and caches). This is the allocator on which the kmalloc allocator relies. [ cat /proc/slabinfo ] gives info about slab.

    kmalloc family allocation: kmalloc is a kernel memory allocation function, such as malloc() in user space. Memory returned by kmalloc is contiguous in physical memory and in virtual memory:

                        virtual memory    physical memory
                        ----------        --------
                   - - -|        |------- |      |
        kmalloc   /     |        |        |      |
        ---------/      |        |        |      |
        |       |       |        |        |      |
        |       |   --- |        |------- |      |
        |       |  /    |        |        |      |
        |       | /     |        |        |      |
        ---------/      |        |        |      |
                        |        |        |      |
                        |        |        |      |
                        |        |        |      |
                        |        |        |      |
                        |        |        |      |
                        ----------        --------

    kmalloc allocator is the general and higher-level memory allocator in the kernel, and relies on SLAB Allocator. Memory returned from kmalloc has a kernel logical address because it is allocated from the LOW_MEM region, unless HIGH_MEM is specified.
        Header File: #include <linux/slab.h>
        void *kmalloc(size_t size, int flags);
            size: specifies the size of the memory to be allocated (in bytes).
            flags: determines how and where memory should be allocated.
                Available flags are the same as the page allocator (GFP_KERNEL, GFP_ATOMIC, GFP_DMA, and so on)
            Return Value: On Success, returns the virtual address of the chunk allocated, which is guaranteed to be physically contiguous.  On error, it returns NULL
        Flags:
            GFP_KERNEL: This is the standard flag. We cannot use this flag in the interrupt handler because its code may sleep. It always returns memory from the LOM_MEM zone (hence a logical address).
            GFP_ATOMIC: This guarantees the atomicity of the allocation. The only flag to use when we are in the interrupt context.
            GFP_USER: This allocates memory to a user space process. Memory is then distinct and separated from that allocated to the kernel.
            GFP_HIGHUSER: This allocates memory from the HIGH_MEMORY zone.
            GFP_DMA: This allocates memory from DMA_ZONE.

    kfree:The kfree function is used to free the memory allocated by kmalloc.
        void kfree(const void *ptr)
        Memory corruption can happen:
            --->    on a block of memory that already has been freed
            --->    on a pointer that is not an address returned from kmalloc()
    Always balance allocations and frees to ensure that kfree() is called exactly once on the correct pointer

14)
    zones: Linux kernel divides physical RAM into a number of different memory regions: zones. What memory regions(zones) there are depends on whether your machine is 32-bit or 64-bit and also how complicated it is. Zones:
        1. DMA: low 16 MBytes of memory. At this point it exists for historical reasons. There were hardware that could only do DMA into this area of physical memory.
        2. DMA32:   exists only in 64-bit Linux. It is the low 4 GBytes of memory, more or less. It exists because the transition to large memory 64-bit machines has created a class of hardware that can only do DMA to the low 4 GBytes of memory.
        3. Normal: It is different on 32-bit and 64-bit machines. On 64-bit machines, it is all RAM from 4GB or so on upwards. On 32-bit machines it is all RAM from 16 MB to 896 MB for complex and somewhat historical reasons. Note that this implies that machines with a 64-bit kernel can have very small amounts of Normal memory unless they have significantly more than 4GB of RAM. For example, a 2 GB machine running a 64-bit kernel will have no Normal memory at all while a 4 GB machine will have only a tiny amount of it. We will have only DMA zone.
        4. HighMem: exists only on 32-bit Linux. It is all RAM above 896 MB, including RAM above 4 GB on sufficiently large machines.

    Within each zone, Linux uses a buddy-system allocator to allocate and free physical memory.
    Buddy Allocator: Memory is broken up into large blocks of pages where each block is a power of two number of pages (2^order). Starts from 4KB page. All free pages are split into 11 (MAX_ORDER) lists, each contains a list of 2^order pages. [ check all 11 columns in /proc/buddyinfo ] [ watch video 9:00-12:00 ].

    When an allocation request is made for a particular size, the buddy system looks into the appropriate list for a free block, and returns its address, if available. However, if it cannot find a free block, it moves to check in the next high-order list for a larger block, which if available it splits the higher-order block into equal parts called buddies, returns one for the allocator, and queues the second into a lower-order list. When both buddy blocks become free at some future time, they are coalesced to create a larger block. [ /proc/buddyinfo ] shows all zones present.

    Using the buddy algorithm, each column represents the number of pages of a certain order (a certain size) that are available at any given time. check available RAM using [ free -m ]

    # cat /proc/buddyinfo
    Node 0, zone      DMA      1      1      0      1      2      1      1      0      1      1      3
    Node 0, zone   Normal      1      1      1      1      3      1      1      2      3      4    207
    Node 0, zone  HighMem     22      8      4      1      1      1      1      1      1      2     34

    highmem will not be present if it has only 512 MB memory. Verify by configuring QEMU and then running /proc/buddyinfo ( video 5:00 ). This means, zone DMA, there are 1 of 2^(0*PAGE_SIZE) free chunks of memory, 1 of 2^(1)*PAGE_SIZE, 0 of 2^(2)*PAGE_SIZE and so on upto 3*(2^10)*PAGE_SIZE = Nearly 16 MB

    virtual kernel memory layout:
    x86: You can see in dmesg | grep -A 10 'virtual kernel memory layout'
    x86_64: Documentation/x86/x86_64/mm.rst

    can I use virt_to_phys for user space memory in kernel module? Can I use virt_to_phys to get the physical address returned by malloc ?
        virt_to_phys: The returned physical address is the physical (CPU) mapping for the memory address given. It is only valid to use this function on addresses directly mapped or allocated via kmalloc. It means It is used by the kernel to translate kernel virtual address (not user virtual address) to physical address

    What is the maximum size allocatable using kmalloc? The upper limit (number of bytes that can be allocated in a single kmalloc request), is a function of:
        the processor – really, the page size – and the number of buddy system freelists (MAX_ORDER). ( check in include/linux/slab.h [ IMP: video 24:00 - 29:00 ] )

    On both x86 and ARM, with a standard page size of 4 Kb and MAX_ORDER of 11
        #define KMALLOC_SHIFT_MAX       (MAX_ORDER + PAGE_SHIFT - 1)
        MAX_ORDER = 11 , PAGE_SHIFT = 12 = 11 + 12 -1 = 22
        /* Maximum allocatable size */
        #define KMALLOC_MAX_SIZE        (1UL << KMALLOC_SHIFT_MAX) = 2^22 = 4*1024*1024 = 4MB

    What happens if we don't free the memory allocated by kmalloc? Kernel memory is never freed automatically, even after module removal. [ RAM size/available can be fetched [ cat /proc/meminfo ] ]. ( watch video 25:00 - 35:00 ). Unlike this, in user space, malloc allocated memory gets leaked but claimed back when process is removed.

    kmalloc may internally round up allocations and return more memory than requested. ksize() can be used to determine the actual amount of memory allocated. The caller may use this additional memory, even though a smaller amount of memory was initially specified with the kmalloc call. This function is not often needed; callers to kmalloc() usually know what they allocated. It can be useful, though, in situations where a function needs to know the size of an object and does not have that information handy.

    kzalloc works like kmalloc, but also zero the memory.
        void *kzalloc(size_t size, gfp_t flags);

    Memory allocated by kmalloc() can be resized by:
        void *krealloc(const void *p, size_t new_size, gfp_t flags);

    The kmalloc() function returns physically and virtually contiguous memory. Physically contiguous memory has two primary benefits.
            1.  many hardware devices cannot address virtual memory.
            2.  a physically contiguous block of memory can use a single large page mapping. This minimizes the translation lookaside buffer (TLB) overhead of addressing the memory
    Allocating physically contiguous memory has one downside: it is often hard to find physically contiguous blocks of memory, especially for large allocations.

    vmalloc: Memory returned by vmalloc is only contiguous in virtual memory and not in physical memory.
                        virtual memory    physical memory
                        ----------        --------
                   - - -|        |--------|      |
        vmalloc   /     |        |        |      |
        ---------/      |        |--------|      |
        |       |       |        |\       |      |
        |       |   --- |        | \      |      |
        |       |  /    |        |\ ----- |      |
        |       | /     |        | \      |      |
        ---------/      |        |  ------|      |
                        |        |        |      |
                        |        |        |      |
                        |        |        |      |
                        |        |        |      |
                        |        |        |      |
                        ----------        --------
    The returned memory always comes from HIGH_MEM zone. HIGH_MEM zone is in only 32-bit machines. 64-bit machines don't have it.

    What is the maximum size allocatable using vmalloc? Unlike kmalloc(), it will keep allocating till RAM size. Theoratically, /proc/meminfo -> vmalloctotal shows how much can we allocate.
        [ bc -q ] launches calculator ?
        [ /fs/proc/meminfo ] has vmalloc related proc calls.

    Can i use ksize with vmalloc? According to the documentation, "The caller must guarantee that objp points to a valid object previously allocated with either kmalloc() or kmem_cache_alloc()."

    Maximum Amount of memory can be allocated using vmalloc: The vmalloc upper limit is, in theory, the amount of physical RAM on the system. Kernel reserves an architecture (cpu) specific “range” of virtual memory for the purpose of vmalloc: from VMALLOC_START to VMALLOC_END.
        Header file: <asm/pgtable.h>

    Differences between vmalloc and kmalloc:
        1. Physical Memory:
            kmalloc: Guarantees the pages are physically contiguous and virtually contiguous
            vmalloc: It allocates virtually contiguous but not necessarily physically contiguous
        2. Low Mem vs High Mem:
            kmalloc: Returns from Low Memory
            vmalloc: Returns from High Memory
        3. Usage:
            kmalloc: Memory returned Can be used by hardware devices(DMA, IMP: PCI ( https://stackoverflow.com/questions/116343/what-is-the-difference-between-vmalloc-and-kmalloc ))
            vmalloc: Memory returned Cannot be used by hardware devices
        4. Interrupt Context:
            kmalloc: can be used in interrupt context with 'GFP_ATOMIC'
            vmalloc: cannot be used in interrupt context
        5. Allocator:
            kmalloc: Uses slab allocator which in turn use Page Allocator
            vmalloc: Directly uses Page Allocator
        6. Overhead:
            kmalloc: less overhead
            vmalloc: more overhead, as each vmalloc requires page table changes and a translation look aside buffer invalidation.
        7. Size:
            kmalloc: Cannot give large memory
            vmalloc: Useful for allocating large memory and no requirement of physical contiguous

    kmalloc(0) returns a special ZERO_SIZE_PTR value. It is a non-NULL value which looks like a legitimate pointer, but which causes a fault on any attempt at dereferencing it. Any attempt to call kfree() with this special value will do the right thing, of course.

    Kernel Stack. In a Linux System, every process has 2 stacks:
        User stack
        Kernel stack ( when process run in kernel space for given space )

    User Stack in x86: Resides in user address space (0-3GB in 32-bit x86)
    Kernel Stack in x86: Resides in kernel address space(3GB-4GB in 32-bit x86)

    User space is afforded the luxury of a large, dynamically growing stack, whereas the kernel has no such luxury. The kernel's stack is small and fixed. Size of the per-process kernel stacks depends on both the architecture and a compile-time option. When the option is enabled, each process is given only a single page - 4KB on 32-bit architectures and 8KB on 64-bit architectures. Why only one page?
        1. Less memory consumption per process
        2. As uptime increases, it becomes increasingly hard to find two physically contiguous unallocated pages.

    Historically, interrupt handlers also used the kernel stack of the process they interrupted. As it placed tighter constraints on the already smaller kernel stack. Kernel developers implemented a new feature: interrupt stacks. Interrupts use their own stacks. It consumes only a single page per processor. Now, we have a kernel stack size of 16KB from Linux 3.15 in x86_64.

    CONFIG_FRAME_WARN: This kernel configuration option passes an option to the compiler to cause it to emit a warning when a static stack size for a routine is detected that is larger than the specified threshold. It requires gcc version 4.4 or later in order to work. The gcc option used is "-Wframe-larger-than=xxx". By default, CONFIG_FRAME_WARN has the value of 1024, but you can set it to any value from 0 to 8192. Linux kernel defines stack size 8192 bytes for each process. The sum of the stack frames of all active functions should not overflow 8192 bytes. This warning does not guarantee that you will overflow the stack space; it just shows that this function makes an overflow more likely (when used together with other big-frame functions, or with many smaller functions).

    There's a way to get a list of how much stack space each function in your program uses. checkstack.pl. It prints out a list of functions and their stack usage, biggest first.
        $ objdump -D hello.ko | perl ~/linux-5.2.8/scripts/checkstack.pl
    Note: it can't take into account recursive functions. It only performs static analysis

15)
    Linux Kernel Code provides several data structures: Linked lists, Queues, Maps, Binary trees. These generic data structures are provided to encourage code reuse.

    Linked list is the simplest and most common data structure in the Linux Kernel. Linked list allows to have variable number of elements called nodes of the list. Elements in Linked List are dynamically created at run time, and they do not necessarily occupy contiguous regions in memory. Therefore elements needs to be linked together. Each element in the list contains a pointer to the next element. As elements are added and removed from the list, the pointer to the next node is simply adjusted. Types:
        1. Singly Linked Lists
        2. Doubly Linked Lists
        3. Circular Linked Lists

    instead of referrig to this course, can we go through just: https://demystifyingme.wordpress.com/2017/06/10/kernel-data-structures-linkedlist/ ?

16)
        int *ptr = NULL;

        int* getmem(){
            if (!ptr) {
                ptr = kmalloc(sizeof(int), GFP_KERNEL);
                if (!ptr)
                    return -ENOMEM;
            }
            return ptr;
        }

    concurreny issue: because ptr is global, two threads share this varibale and if one updates it, other thread will also get updated value and then concurreny issues may happen.

    concurrency: the ability to handle multiple outstanding tasks/process with the illusion or reality of simultaneity. Single Core Environment(fake parallelism): concurrency is achieved via a process called context-switching i.e., at a particular time period, only a single task gets executed. If we see multiple processes running on single processor, then only one is running and other are waiting. Because processor can run a single process at a time.

    Multi Core Environment(true parallelism): Multiple processes executing simultaneously on multiple processors/CPU's.

    How to find out how many cores you have? $ grep -c ^processor /proc/cpuinfo. $ nproc

    How to find the processor number in which process is running? $ ps -eaF. The PSR column shows which <process> is running on which processor <number>

    Multiprocessing systems:
    initial approach:
        Each CPU has its own OS. The simplest possible way to organize a multiprocessor operating system. statically divide memory into as many partitions as there are CPUs and give each CPU its own private memory and its own private copy of the operating system. One obvious optimization is to allow all the CPUs to share the operating system code and make private copies of only the data

            cpu1            cpu2            cpu3           cpu4         Memory               IO
            --------        ---------       ---------      ----------   -----------          ---------
            |Private|       |private|       |Private |     |Private |   |1   |2    |         |        |
            |OS     |       |OS     |       |OS      |     |OS      |   |Data|Data |         |        |
            --------        ---------       ---------       ----------  ---------- |         |        |
              | |             | |             | |             | |       |3   |4    |         |        |
              | |             | |             | |             | |       |Data|Data |         |        |
              | |             | |             | |             | |       -----------|         ----------
              | |             | |             | |             | |       |OS Code   |              | |
              | |             | |             | |             | |       -----------               | |
              | |             | |             | |             | |            | |                  | |
              | |             | |             | |             | |            | |                  | |
            --  --------------  --------------   --------------  ------------  -------------------   -----
                        BUS
            -----------------------------------------------------------------------------------------------

        Problems with this approach:
            1. When a process makes a system call, the system call is caught and handled on its own CPU using the data structures in the operating system's tables. So, one CPU can't handle other's system calls.
            2. since each operating system has its own tables, it also has its own set of processes that it schedules by itself. There is no sharing of processes. As a consequence, it can happen that CPU 1 is idle while CPU 2 is loaded with work.
            3. no sharing of pages. It can happen that CPU 1 has pages to spare while CPU 2 is paging continuously. There is no way for CPU 2 to borrow some pages from CPU 1 since the memory allocation is fixed.
            4. if the operating system maintains a buffer cache of recently used disk blocks, each operating system does this independently of the other ones. Thus it can happen that a certain disk block is present and dirty in multiple buffer caches at the same time, leading to inconsistent results.

    A master-slave multiprocessor model:
        One copy of OS and its tables are present on CPU1 and not on any of the others. All system calls are redirected to CPU 1 for processing there. CPU 1 may also run user processes if there is CPU time left over. This model is called master-slave since CPU 1 is the master and all the others are slaves.

            cpu1            cpu2/slave      cpu3/slave      cpu4/slave    Memory              IO
            --------        ---------       ---------       ----------    -----------         ---------
            |Master |       |user   |       |user    |      |user    |    |User      |        |        |
            |runs OS|       |process|       |process |      |process |    |Processes |        |        |
            --------        ---------       ---------       ----------    |          |        |        |
              | |             | |             | |             | |         |          |        |        |
              | |             | |             | |             | |         |          |        |        |
              | |             | |             | |             | |         -----------|        ----------
              | |             | |             | |             | |         |OS Code   |            | |
              | |             | |             | |             | |         -----------             | |
              | |             | |             | |             | |            | |                  | |
              | |             | |             | |             | |            | |                  | |
            --  --------------  --------------   --------------  ------------  -------------------   -----
                        BUS
            -----------------------------------------------------------------------------------------------

        The master-slave model solves most of the problems of the first model.
            1. There is a single data structure (e.g., one list or a set of prioritized lists) that keeps track of ready processes.When a CPU goes idle, it asks the operating system for a process to run and it is assigned one.  Thus it can never happen that one CPU is idle while another is overloaded.
            2. Similarly, pages can be allocated among all the processes dynamically and there is only one buffer cache, so inconsistencies never occur.

        Problem:
            The problem with this model is that with many CPUs, the master will become a bottleneck. After all, it must handle all system calls from all CPUs. If, say, 10% of all time is spent handling system calls, then 10 CPUs will pretty much saturate the master, and with 20 CPUs it will be completely overloaded. Thus this model is simple and workable for small multiprocessors, but for large ones it fails.

    Symmetric Multiprocesors (SMP):
        One copy of the OS  is in memory, but any CPU can run it.

        UP: User Process
            cpu1            cpu2            cpu3           cpu4         Memory               IO
            --------        ---------       ---------      ----------   -----------          ---------
            |Shared |       |Shared |       |Shared  |     |Shared  |   |          |         |        |
            |OS/UP  |       |OS/UP  |       |OS/UP   |     |OS/UP   |   |          |         |        |
            --------        ---------       ---------      ----------   |          |         |        |
              | |             | |             | |             | |       |          |         |        |
              | |             | |             | |             | |       |          |         |        |
              | |             | |             | |             | |       -----------|         ----------
              | |             | |             | |             | |       |OS Code   |              | |
              | |             | |             | |             | |       -----------               | |
              | |             | |             | |             | |            | |                  | |
              | |             | |             | |             | |            | |                  | |
            --  --------------  --------------   --------------  ------------  -------------------   -----
                        BUS
            -----------------------------------------------------------------------------------------------

        Advantage: eliminates the master CPU bottleneck, since there is no master

        Problems:
        Imagine two CPUs simultaneously picking the same process to run or claiming the same free memory page. The simplest way around these problems is to associate a mutex (i.e., lock) with the operating system, making the whole system one big critical region. When a CPU wants to run operating system code, it must first acquire the mutex. If the mutex is locked, it just waits. In this way, any CPU can run the operating system, but only one at a time. It is called big kernel lock.

        With 20 CPUs, there will be long queues of CPUs waiting to get in. Fortunately, it is easy to improve. Many parts of the operating system are independent of one another. For example, there is no problem with one CPU running the scheduler while another CPU is handling a file system call and a third one is processing a page fault. This observation leads to splitting the operating system up into independent critical regions that do not interact with one another. Each critical region is protected by its own mutex, so only one CPU at a time can execute it.

    Preemption means forcefully taking away of the processor from one process and allocating it to another process. Switching. Whereas, Switching from one running task/process to another/process is known as context switch. In the Linux kernel, the scheduler is called after each timer interrupt (that is, quite a few times per second). It determines what process to run next based on a variety of factors, including priority, time already run, etc.

    Difference between preemption and context switch?
        Preemption: Firing of timer interrupt is preempting the current running process and running the interrupt service routine of timer interrupt.
        Context Switch: what happens when the kernel alters the state of the processor (the registers, mode, and stack) between one process or thread's context and another.

    context_switch() function is called in the kernel. Under Linux, user-space programs have always been preemptible: the kernel interrupts user-space programs to switch to other threads, using the regular clock tick. So, the kernel doesn't wait for user-space programs to explicitly release the processor. This means that an infinite loop in an user-space program cannot block the system.

    Kernel Space: one can enable or disable it using the CONFIG_PREEMPT option. If CONFIG_PREEMPT is enabled, then kernel code can be preempted everywhere, except when the code has disabled local interrupts. An infinite loop in the code can no longer block the entire system.

    my understanding: so there is no relation between user space and kernel space process. Only kernel scheduler selects those and runs on SMP using different CPUs. So, kernel and user processes are just different in process types, nothing else ?

    Kernel preemption can occur:
        When returning to kernel-space from an interrupt handler
        When kernel code becomes preemptible again
        If a task in the kernel explicitly calls schedule()
        If a task in the kernel blocks (which results in a call to schedule())

    Case1:
        While process A executes an exception handler (necessarily in Kernel Mode), a higher priority process B becomes runnable. This could happen, for instance, if an IRQ occurs and the corresponding handler awakens process B. As the kernel is preemptive, a forced process switch replaces process A with B. The exception handler is left unfinished and will be resumed only when the scheduler selects again process A for execution.

    Case2:
        consider a process that executes an exception handler and whose time quantum expires. As the kernel is preemptive, the process may be replaced immediately.

    Motivation for making the kernel preemptive: reduce the dispatch latency of the User Mode processes. the delay between the time they become runnable and the time they actually begin running. Processes performing timely scheduled tasks (such as external hardware controllers, environmental monitors, movie players, and so on) really benefit from kernel preemption, because it reduces the risk of being delayed by another process running in Kernel Mode.

    scheduler itself also is a process. It is scheduled by timer. When timer interrupt fires, in interrupt handler, scheduler gets called usually. Otherwise, by yielding also it can run. So, scheduler will run interrupt context/process context based on invokation path. at boot u-boot bootloader ( others also can based on design ) might start kernel. Then check [ cat /proc/interrupts ] for timer interrupt. Bringing Kernel from harddisk to RAM is done by bootloader like U-boot, GRUB ( x86 uses GRUB ). on x86, BIOS also. The moment kernel starts, it configures timer and then frequently gets updated.

    A kernel control path denotes the sequence of instructions executed by the kernel to handle a system call, an exception, or an interrupt. Linux kernel is reentrant. This means that several processes may be executing in Kernel Mode at the same time. On uniprocessor systems, only one process can progress, but many can be blocked in Kernel Mode when waiting for the CPU or the completion of some I/ O operation.

    Example:
        --> after issuing a read to a disk on behalf of a process, the kernel lets the disk controller handle it and resumes executing other processes.
        --> An interrupt notifies the kernel when the device has satisfied the read, so the former process can resume the execution.

    Reentrancy in Linux Kernel:
        1. Reentrant functions : They don't use/modify global data structures.
        2. Non reentrant functions: Modify global data structures but use locking mechanism

    ( IMP: difference between thread safe v/s reentrant function ) (https://stackoverflow.com/a/33445858, https://uvdn7.github.io/reentrant/ )

    Synchronization and Critical Regions: Implementing a reentrant kernel requires the use of synchronization. If a kernel control path is suspended while acting on a kernel data structure, no other kernel control path should be allowed to act on the same data structure unless it has been reset to a consistent state. Otherwise, the interaction of the two control paths could corrupt the stored information.

    When the outcome of a computation depends on how two or more processes are scheduled, the code is incorrect. We say that there is a race condition. Any section of code that should be finished by each process that begins it before another process can enter it is called a critical region

    Causes of concurrency:
        1. Interrupts: An interrupt can occur asynchronously at almost any time; interrupting the currently executing code.
        2. Softirqs and tasklets: Kernel can raise or schedule a softirq or tasklet at almost any time.
        3. Kernel preemption: Because the kernel is preemptive, one task in the kernel can preempt another
        4. Sleeping and synchronization with user space: Task in the kernel can sleep and thus invoke the scheduler, resulting in running of a new process.
        5. Symmetrical multiprocessor: Two or more processors can execute kernel code at exactly the same time.

    Solutions:
        1. Kernel Preemption Disabling
                disabling kernel preemption
                critical region start
                ......
                .....
                critical region end
                enable kernel preemption
            Problem:
                On Multiprocessor, two kernel paths running on different CPUs can concurrently access the same global data.
        2. Disabling Hardware Interrupts
                Disabling Hardware Interrupts
                critical region start
                ......
                ......
                critical region end
                enable hardware interrupts
            Problem:
                If the critical region is large, interrupts can remain disabled for a relatively long time, potentially causing all hardware activities to freeze/ might get missed.
                On a multiprocessor system, disabling interrupts on the local CPU is not sufficient, and other synchronization techniques must be used.

    Maximum number of processors that an SMP kernel could support: $ grep NR_CPUS /boot/config-`uname -r`. You can override this with the nr_cpus kernel parameter in the bootloader command line. nr_cpus=12. Use num_online_cpus() function to get the number of cpus online.

    smp_processor_id() gives you the current processor number on which kernel is running. IMP: video 46:00 - 50:00

    Per CPU Variables: The simplest and most efficient synchronization technique consists of declaring kernel variables as per-CPU Variables. Basically a per CPU Variables is an array of data structures, one element  per each CPU in the system. A CPU should not access the elements  of the array corresponding to other CPU. It can freely read and modify its own element without fear of race conditions, because it is the only CPU Entitled to do so. The elements of the per-CPU array are aligned in main memory so that each data structure falls on a different line of the hardware cache.
        get_cpu() on top of returning the current processor number also disables kernel preemption.
        put_cpu() enables kernel preemption.

    Why is disabling kernel preemption needed ( look in program how it is getting called ? ). Just consider, for instance, what would happen
        if a kernel control path gets the address of its local copy of a per-CPU variable, then it is preempted and moved to another CPU: the address still refers to the element of the previous CPU.

    percpu interface: The 2.6 kernel introduced a new interface, known as percpu, for creating and manipulating per-CPU data. Creation and manipulation of per-CPU data is simplified with this new approach. The previously discussed method of creating and accessing per-CPU data is still valid and accepted. This new interface, however, grew out of the needs for a simpler and more powerful method for manipulating per-CPU data on large symmetrical multiprocessing computers.
        Header File: <linux/percpu.h>. To create a per-CPU variable at compile time, use this macro
        DEFINE_PER_CPU(type, name);
            This creates an instance of a variable of type type, named name, for each processor on the system.

    You can access another processor's copy of the variable with: per_cpu(variable, int cpu_id); If you write code that involves processors reaching into each other's per-CPU variables, you, of course, have to implement a locking scheme that makes that access safe.

    Per-CPU Data at Runtime: Dynamically allocated per-CPU variables are also possible. While per-CPU variables provide protection against concurrent accesses from several CPUs, they do not provide protection against accesses from asynchronous functions (interrupt handlers and deferrable functions). In these cases, additional synchronization primitives are required.

17_1)
    Atomic Operations: Several assembly language instructions are of type “read-modify-write”. they access a memory location twice, the first time to read the old value and  the second time to write a new value.

    Suppose that two kernel control paths running on two CPUs try to “read-modify-write” the same memory location at the same time by executing nonatomic operations. At first, both CPUs try to read the same location, but the memory arbiter (a hardware circuit that serializes accesses to the RAM chips) steps in to grant access to one of them and delay the other. However, when the first read operation has completed, the delayed CPU reads exactly the same (old) value from the memory location. Both CPUs then try to write the same (new) value to the memory location; again, the bus memory access is serialized by the memory arbiter, and eventually both write operations succeed. However, the global result is incorrect because both CPUs write the same (new) value. Thus, the two interleaving “read-modify-write” operations act as a single one.

        Kernel Thread1              Kernel Thread2
        ------------------------------------------------------
        read i (5)
                                    read i(5)
        increment i(5 -> 6)
                                    increment i (5 -> 6)
        write i(6)
                                    write i(6)

    Atomic Operations: The easiest way to prevent race conditions due to “read-modify-write” instructions is by ensuring that such operations are atomic at the chip level. Every such operation must be executed in a single instruction without being interrupted in the middle and avoiding accesses to the same memory location by other CPUs. Most CPU instruction set architectures define instruction opcodes that can perform atomic read-modify-write operations on a memory location. In general, special lock instructions are used to prevent the other processors in the system from working until the current processor has completed the next action.
        Header File: <asm/atomic.h>
        typedef struct { volatile int counter; } atomic_t;
    Why a new user defined data type atomic_t is needed? Because the atomic data types are ultimately implemented with normal C types, the kernel encapsulates standard variables in a structure that can no longer be processed with normal operators such as ++.

    What happens to atomic variables when the kernel is compiled without SMP Support? it works the same way as for normal variables (only atomic_t encapsulation is observed) because there is no interference from other processors

        atomic_t:
        Initialization: atomic_t i;  //define i
        Increment/Decrement: void atomic_inc(atomic_t *i);  //Add 1 to *i
        Set/Read: void atomic_set(atomic_t *i, int j); //Atomically set counter i to value specified in j
        Add/Sub: void atomic_add(int val, atomic_t *i); //Atomically add val to atomic counter i
        Atomic Operation and test: int atomic_dec_and_test(atomic_t *i); //atomic Subtract 1 from *i and return 1 if the result is zero; 0 otherwise
        Atomic Add/Subtract and return: int atomic_add_return(int val, atomic_t *i);// Atomically add val to *i and return the result.
        //Atomically adds val to i and return pre-addition value at i:  int atomic_fetch_add(int val, atomic_t *i);
        //Atomically subtracts val from i, and return pre-subtract value at i: int atomic_fetch_sub(int val, atomic_t *v);
        //Reads the value at location i, and checks if it is equal to old; if true, swaps value at v with new, and always returns value read at i: int atomic_cmpxchg(atomic_t *i, int old, int new);
        //Swaps the oldvalue stored at location i with new, and returns old value i: int atomic_xchg(atomic_t *i, int new);

    Common use of atomic operations:
        A common use of the atomic integer operations is to implement counters
        Protecting a sole counter with a complex locking scheme is overkill, so instead developers use
        atomic_inc() and atomic_dec(), which are much lighter in weight.

    64-bit Atomic Operations: Many processor architectures have no 64-bit atomic instructions, but we need atomic64_t in order to support the perf_counter subsystem. This adds an implementation of 64-bit atomic operations using hashed spinlocks to provide atomicity.
        typedef struct {
            long long counter;
        } atomic64_t;

    Atomic Bitwise Operations: In addition to atomic integer operations, the kernel also provides a family of functions that operate at the bit level.
        Header File: <asm/bitops.h>
    These functions operate on generic pointer. There is no equivalent of the atomic integer atomic_t.

17_2)
    Problem with atomic instructions:
            Can only work with CPU word and double word size.
            Cannot work with shared data structures of custom size.

    In real life, critical regions can be more than one line. And these code paths such execute atomically to avoid race condition. To ensure atomicity of such code blocks locks are used.

    Spinlocks: The most common lock in the Linux kernel is the spin lock. Spinlocks are used to protect short code sections that comprise just a few C statements and are therefore quickly executed and exited. A spin lock is a lock that can be held by at most one thread of execution.

    When the thread tries to acquire lock which is already held? The thread busy loops/spins waiting for the lock to become available.
    When the thread tries to acquire lock which is available? The thread acquires the lock and continues.

    Spinlock Methods:
    initially, spinlock is unlocked.
        Header File: <linux/spinlock.h>
        Data Structure: spinlock_t
        Methods: DEFINE_SPINLOCK(my_lock);   == spinlock_t my_lock = __SPIN_LOCK_UNLOCKED(my_lock);

        From <linux/spinlock_types.h>:
            #define DEFINE_SPINLOCK(x)      spinlock_t x = __SPIN_LOCK_UNLOCKED(x)
            //To lock a spin lock: void spin_lock(spinlock_t *lock);
            //To unlock a spin lock: void spin_unlock(spinlock_t *lock);

        To initialize spin lock at run time. void spin_lock_init(spinlock_t *lock);

    on uniprocessor machines, spin lock exist when CONFIG_PREEMPT is not set/kernel preemption disabled. Spinlocks are defined as empty operations because critical sections cannot be entered by several CPUs at the same time. When CONFIG_PREEMPT is set:
        spin_lock  = preempt_disable
        spin_unlock = preempt_enable

    What happens if i acquire a lock which is already held by the same CPU? Spin locks are not recursive. Unlike spin lock implementations in other operating systems and threading libraries, the Linux kernel’s spin locks are not recursive. This means that if you attempt to acquire a lock you already hold, you will spin, waiting for yourself to release the lock. But because you are busy spinning, you will never release the lock and you will deadlock.

    int spin_trylock(spinlock_t *lock); Tries to acquire given lock.
        If not available, returns zero.
        If available, it obtains the lock and returns nonzero

    Can i use spinlock when the resource is shared between kernel control path in process context vs interrupt context?
        1. Your driver is executing and has taken a lock.
        2. Device the driver is handling issues a interrupt.
        3. Interrupt handler also obtains the same lock.
        - Problem: Happens when the interrupt handler runs in the same processor which driver code with lock is running. More profound in single processor system. Interrupt handler will spin forever, as the non interrupt code will not be able to run to release the lock. This results in deadlock.
        - Solution:
            - Disable interrupts before acquiring the spin lock
            - Enable them back after releasing the spin lock.

    The kernel provides an interface that conveniently disables interrupts and acquires the lock.
        DEFINE_SPINLOCK(my_lock);
        unsigned long flags;
            spin_lock_irqsave(&my_lock, flags);
            /* critical region ... */
            spin_unlock_irqrestore(&my_lock, flags);

    Why additional argument of flags is needed? What if the interrupts were disabled before you acquire a spinlock, if we don't have flags, we will enable them after unlocking. spin_lock_irqsave()saves the current state of interrupts, disables them locally, and then obtains the given lock. current stata is flags and those indicate which interrupts are enabled/disabled. Conversely, spin_unlock_irqrestore() unlocks the given lock and returns interrupts to their previous state. If you always know before the fact that interrupts are initially enabled, there is no need to restore their previous state.
        DEFINE_SPINLOCK(mr_lock);
        spin_lock_irq(&mr_lock);
        /* critical section ... */
        spin_unlock_irq(&mr_lock);

    Use of spin_lock_irq() is not recommended, as it is hard to ensure interrupts are always enabled in any kernel code path.

    Is the kernel preemption disabled when the spinlock is acquired? Any time kernel code holds a spinlock, preemption is disabled on the relevant processor. Otherwise any holidng task/process may get preempted and other scheduled process might be always waiting for that spinlock. This scenario is more profound in uniprocssor system where only single process can be scheduled at a time. Lock/unlock methods disable/enable kernel preemption.

    Important Points with Spinlocks:
        1. If a lock is acquired but no longer released, the system is rendered unusable.
            All processors— including the one that acquired the lock — sooner or later arrive at a point where they must enter the critical region.
            They go into the endless loop to wait for lock release, but this never happens and deadlocks.
        2. On no account should spinlocks be acquired for a longer period because all processors waiting for lock release are no longer available for other productive tasks.
        3. Code that is protected by spinlocks must not go to sleep.
            it must also be ensured that none of the functions that are called inside a spinlocked region can go to sleep!
            Ex: kmalloc with GFP_KERNEL.

    Simple Implementation:
        A spinlock is a mutual exclusion device that can have only two values:
            Locked
            Unlocked
        It is usually implemented as a single bit in an integer value. Code wishing to take out a particular lock tests the relevant bit. If the lock is available, the "locked" bit is set and the code continues into the critical section. If, instead, the lock has been taken by somebody else, the code goes into a tight loop where it repeatedly checks the lock until it becomes available. This loop is the "spin" part of a spinlock.

    Real Implementation: The "test and set" operation must be done in an atomic manner so that only one thread can obtain the lock, even if several are spinning at any given time. Spinlocks are built on top of hardware-specific atomic instructions. The actual implementation of spinlock is different for each architecture the linux supports.

18_1)
    Mutexes: Almost all semaphores found in the Linux kernel are used for mutual exclusion by count of 1. Using semaphore for mutual exclusion is overhead, so kernel provides a new interface: mutex. The mutex subsystem checks and enforces the following rules:
        Only one task can hold the mutex at a time.
        Whoever locked a mutex must unlock it.
        That is you cannot lock a mutex in one context and unlock in another.
        Recursive locks and unlocks are not allowed.
        Process cannot exit while holding a mutex.
        Mutex cannot be acquired from an interrupt handler.

    Differences between mutexes and semaphores: What happens when a process tries to acquire a mutex lock? When acquiring a mutex, there are three possible paths that can be taken:
                1. Fast Path
                2. Mid Path
                3. Slow Path
        The path which will be taken depends on the state of the mutex.
        Fast Path:
            Taken when no process has acquired the mutex
        Mid Path:
            When the mutex is not available, it tries to go for mid path. also called as optimistic spinning. This path will be only executed if there are no other processes ready to run having high priority and the owner of mutex is running. In this path, tries to spin using MCS lock hoping the owner will release the lock soon. Avoids expensive context switch
        Slow Path:
            last resort. This path acts as a semaphore lock. If the lock is unable to be acquired by the process, the task is added to wait queue. It sleeps until woken up by the unlock path.

        Documentation: Documentation/locking/mutex-design.txt
        Implementation: kernel/locking/mutex.c
        Header File:  <linux/mutex.h>
        Data structure: struct mutex
        struct mutex {
                atomic_long_t           owner;
                spinlock_t              wait_lock;
                struct list_head        wait_list;
        };
            owner -> used for both holding lock state, and reference to owner(task_struct) who has acquired it
            wait_lock -> used for atomic updating wait_list
        Initialization:
        Static:
            DEFINE_MUTEX(name)
        Dynamic:
            mutex_init(mutex)
        void mutex_lock(struct mutex *lock);
        void mutex_unlock(struct mutex *lock);
        int mutex_trylock(struct mutex *lock);
            Tries to acquire the given mutex
            Return:
                1   --> Successful
                0   --> Otherwise

        int mutex_lock_interruptible(struct mutex *lock);
            places the calling process in the TASK_UNINTERRUPTIBLE state when it sleeps
        Return value:
            0 -> mutex is acquired
            -EINTR  -> If the task receives a signal while waiting for mutex

        int mutex_lock_killable(struct mutex *lock);
            places the calling process in the TASK_KILLABLE state when it sleeps, only fatal signal can interrupt
        Return value:
            0 -> mutex is acquired
            -EINTR  -> If the task receives a fatal signal while waiting for mutex

    Mutex semantics are fully enforced when CONFIG DEBUG_MUTEXES is enabled.

    Test if the mutex is taken:
        int mutex_is_locked(struct mutex *lock);
        Return:
            1 -> Locked
            0 -> Unlocked

    Which one do you choose in between semaphores and mutexes? Start with mutex and move to semaphore only if the strict semantics of mutexes are unsuitable.

    spinlock vs mutexes:
        Requirement                         Recommended Lock
        =========================================================
        Low overhead locking                Spinlock
        Short lock hold time                Spinlock
        Long lock hold time                 Mutex
        Need to lock from interrupt context Spinlock
        Need to sleep while holding lock    Mutex

18_2)
    Semaphores: Semaphores in Linux are sleeping locks. What happens when the semaphore lock is unavailable?
            the semaphore places the task onto a wait queue and puts the task to sleep.
            the processor is then free to execute other code

    What happens after the semaphore becomes available?
            one of the tasks on the wait queue is awakened so that it can then acquire the semaphore.

    Entering critical section: A process wishing to enter a critical section will call on the relevant semaphore. if the semaphore's value is greater than zero, that value is decremented by one and the process continues. If, instead, the semaphore's value is 0 (or less), the process must wait until somebody else releases the semaphore.

    Exiting critical section: this function increments the value of the semaphore and, if necessary, wakes up processes that are waiting

    Types of Semaphore: Spin locks allow only one task to hold the lock at a time. With semaphores,number of tasks to hold the lock at a time can be specified while initializing/declaring semaphore. This value is called as usage count or simply count.
        Count = 1   --> Binary Sempahore. Used for mutual exclusion
        Count > 1   --> Counting Semaphore

    Can i use counting semaphores in critical section ? Counting semaphores are not used to enforce mutual exclusion because they enable multiple threads of execution in the critical region at once. Instead, they are used to enforce limits in certain code. They are not used much in the kernel.
        kernel/locking/semaphore.c
        Data structures:
            Header File: <linux/semaphore.h>
            Data structures: struct semaphore
            struct semaphore {
                    raw_spinlock_t          lock;
                    unsigned int            count;
                    struct list_head        wait_list;
            };
            lock - spinlock for a semaphore data protection;
            count - amount available resources;
            wait_list - list of processes which are waiting to acquire a lock.

    Initialization:
        Dynamic:
            void sema_init(struct semaphore *sem, int val);
                    where val is the initial value to assign to a semaphore.
        Static:
            DEFINE_SEMAPHORE(name)
            #define DEFINE_SEMAPHORE(name)  \
            struct semaphore name = __SEMAPHORE_INITIALIZER(name, 1)
            void down(struct semaphore *sem);
            void up(struct semaphore *sem);

    down():
        decrements the count by one
        If count >= 0 task can enter the critical region
        Else task is placed on the wait queue

    up():
        increments the count by one

    int down_interruptible(struct semaphore *sem);
    down vs down_interruptible:
        down() places the calling process in the TASK_UNINTERRUPTIBLE state when it sleeps.
        down_interruptible() places the calling process to sleep in the TASK_INTERRUPTIBLE state
        If the task receives a signal while waiting for the semaphore, it is awakened and down_interruptible() returns -EINTR.

    int down_trylock(struct semaphore *sem); If the semaphore is not available at the time of the call, down_trylock returns immediately with a nonzero return value.

    int down_timeout(struct semaphore *sem, long jiffies); Attempts to acquire the semaphore. If no more tasks are allowed to acquire the semaphore, calling this function will put the task to sleep. If the semaphore is not released within the specified number of jiffies, this function returns -ETIME. It returns 0 if the semaphore was acquired.

    int down_killable(struct semaphore *sem); The down_killable function does the same as the down_interruptible function. Only the fatal signals can be delivered like kill signal. What are fatal signals? Any signal listed with a default action of “terminate” or “dump core” is fatal, unless it’s ignored or handled explicitly. Eg. SIGCONT, SIGCHLD, SIGSTP ..

    Important points while using semaphore:
        1. Semaphores are well suited to locks that held for a long time. As the tasks trying to acquire the lock sleep if it is not available.
        2. Semaphores are not suited for locks that held for a short time.
            Due to overhead of
                a. sleeping
                b. maintaining the wait queue.
                c. waking back up
            total time can easily overweigh the total lock hold time.
        3. As we sleep if the lock is not available, cannot be used in interrupt context.
        4. Semaphores do not disable kernel preemption, and consequently code holding a sempahore can be preempted.

    Advantages of semaphore over spinlock:
        better processor utilization as no time is spent busy looping

    Which one to choose for critical region: spin lock vs semaphore?
        1. Sleep: Semaphore is the only option.
        2. Lock hold time: Sempahores are good for longer lock hold times
                   Spinlocks are useful when the lock hold time is small
        3. Scheduling latency: As semaphores do not disable kernel preemption, scheduling latency is better
                       when compared to spinlocks.

19_1)
    Problem: Readers-writer lock is a special lock mechanism which allows concurrent access for read-only operations. An exclusive lock is needed for writing or modifying data. A writer process can't acquire a lock as long as at least one reader process which acquired a lock holds it. This may lead to a problem called writer starvation, where writer process may sometimes need to wait long time for the lock.

    Lock free and wait free synchronization plays a major role in RTOS, where time guarantees must be given. Two new synchronization mechanisms added in 2.6 Kernel to totally remove locking on the reader side:
        1. Sequence Lock
        2. Read Copy Update (RCU)

    seqlocks/sequence locks:
        Objective: Provide Fast and lock-free access to shared resources.
        Differences between reader-writer locks and sequence locks:
            --> Writer is given a higher priority when compared to reader
            --> Writer is allowed to modify the shared data, even when there are readers in critical section

    How readers handle the data corruption when writer updates it during read ?
        Readers are in charge to check if they read valid data.
        If a write access took place while the data was read, the data is invalid and has to be read again.
        Identification of write accesses is realized with a counter.

    What happens when a writer is already in critical section and another writer arrives? A writer uses a spinlock for mutual exclusion and hence will not interfere the other writer.

    When to use?
        A small amount of data is to be protected
        Your data has a lot of readers/frequently accessed
        Your data has a few writers
        It is important that writers not be starved for access

    How it works? It uses Sequence Counter (Integer) & Spin lock
        Data structure: seqlock_t
        Header File: linux/seqlock.h
        typedef struct seqcount {
                unsigned sequence;
        } seqcount_t;
        typedef struct {
                struct seqcount seqcount;
                spinlock_t lock;
        } seqlock_t;
            seqcount -> sequence counter
            lock     -> lock to atomic update in case of writers
        Initialization:
            Static:         DEFINE_SEQLOCK(x)
            Dynamic:        seqlock_init(x)

    spinlock is used only in write operation. Write operation: Writers must take out exclusive access before making changes to the protected data.
        write_seqlock(&the_lock);
        /* Make changes here */
        write_sequnlock(&the_lock);

        write_seqlock() locks the spinlock and increments the sequence number.
        write_sequnlock() increments the sequence number again, then releases the spinlock.

    Read Operation: No locking is required when you are using trying to read. Each reader must read the sequence number twice:
        before reading the data
        after reading the data
    And verify whether they both are same or not.

    When there was no writer during the read operation? value of the sequence counter will be same.
    When there was writer during the read operation?
        value of the sequence counter will not be same
        The important point here is that while the writer is operating, the sequence number would be odd.

        unsigned int seq;
        do {
            seq = read_seqbegin(&the_lock);
            /* Make a copy of the data of interest */
        } while read_seqretry(&the_lock, seq);
        read_seqbegin   --> Returns the current sequence number.
        read_seqretry   --> Returns 1 if the value of seq local variable is odd.

    Is Kernel Preemption Disabled?
            Readers: No
            Writers: Yes because it acquires a spinlock
    spinlock disables kernel preemption.

    Limitations: Seqlocks cannot be used for pointers, it can only be used for normal data like integers, booleans. Because, there can be pointer which can be already freed, dereferencing such pointer will cause oops. Also, writer might be freeing without informing to reader.

    Who uses seqlocks in Linux Kernel?
        Jiffies: Variable that stores a Linux machine's uptime.
        kernel/time.c, kernel/time/tick-common.c

            u64 get_jiffies_64(void)
            {
                unsigned int seq;
                u64 ret;

                do {
                    seq = read_seqbegin(&jiffies_lock);
                    ret = jiffies_64;
                } while (read_seqretry(&jiffies_lock, seq));
                return ret;
            }
            EXPORT_SYMBOL(get_jiffies_64);

    Seqlocks have other variants to use in interrupts.
        write_seqlock_irq(seqlock_t *sl);
        write_sequnlock_irq(seqlock_t *sl);
        write_seqlock_irqsave(seqlock_t *sl);
        write_sequnlock_irqrestore(seqlock_t *sl, unsigned long flags);

19_2)
    The synchronization techniques discussed till now have one drawback. They do not differentiate between situations in which:
            data structures are simply read
            data structures are actively updated
        Read access can be provided to multiple tasks concurrently
        Write access should be provided to one task at a time.

    Kernel provides additional semaphore and spinlock versions for the above requirement, as read operation is performed more often than write operation:
        1. Reader/Writer Spinlocks
        2. Reader/Writer Semaphores

    These are also known as shared/exclusive or concurrent/exclusive locks:
        shared      --> readers
        exclusive   --> writers

        Header File: <linux/rwlock_types.h>
        Data Structure: struct rwlock_t
        Initialization:
            static: DEFINE_RWLOCK(x)
            Dynamic: rwlock_init(lock);

        Lock/Unlock:
            Readers:
                read_lock(&mr_rwlock);
                /* critical section (read only) ... */
                read_unlock(&mr_rwlock);

            Writers:
                write_lock(&mr_rwlock);
                /* critical section (read and write) ... */
                write_unlock(&mr_lock);

    What happens when we run the below code snippet?
        read_lock(&mylock);
        write_lock(&mylock);
    Executing these two functions will cause deadlock, as write_lock will spin until it all readers have released the lock. Write lock can only be acquired when reader has completed read unlock.

    It is safe for the same thread to recursively obtain the same read lock. What happens when a read lock is held and a writer is waiting for exclusive access and a new reader arrives? Who is given a chance? reader/writer. first in first out happens. Whoever come first, it is given chance.

        Header File: <linux/rwsem.h>
        Data Structure: struct rw_semaphore
        Implementation : kernel/locking/rwsem.c
        Initialization:
            Static:     DECLARE_RWSEM(name)
            Dynamic:    init_rwsem(struct rw_semaphore *sem)

    All reader-writer semaphores are mutexes—that is, their usage count is one. They enforce mutual exclusion only for writers, not readers.

    Lock/Unlock:
        Readers:
            void down_read(struct rw_semaphore *sem);
                critical section
                ....
            void up_read(struct rw_semaphore *sem);
        Writers:
            void down_write(struct rw_semaphore *sem);
                critical section
                ....
            void up_write(struct rw_semaphore *sem);
    Note: down_read/down_write may put the calling process into an uninterruptible sleep.

        int down_read_trylock(struct rw_semaphore *sem);
        int down_write_trylock(struct rw_semaphore *sem);

    Note: Both return 1 if the lock is successfully acquired and 0 if it is currently contended. This is the opposite of normal semaphore behavior!

    downgrade_write():

    Reader-writer semaphores have a unique method which is not present in reader-writer spinlocks.
        void downgrade_write(struct rw_semaphore *sem);

    This function atomically converts an acquired write lock to a read lock. Where can i use this? Used in situation where writer lock is needed for a quick change, followed by longer period of read-only access

20)
    Read Copy Update / RCU:
        IMP: 29:00 - 32:00

    RCU supports concurrency between a single updater/writer and multiple readers. Used in same scenario as in seqlock. Often used to update linked lists,which are used all over the kernel. RCU Can be used in the following scenario:
        Lot of Reads
        Rare Writes
        Write should have priority when compared to Read

    Problem with Read/Write Locks:
        Expensive (as they use atomic increment/decrement for reader count)

    Problem with Seq Locks:
        Cannot be used with pointers, only works on basic types like integer etc
        Reader needs to retry the operation

    RCU solves the above problem:
        to have pointer as a shared resource and
        No locks in the reader ( same as in seqlock )
        Avoid reader to retry the operation

    Constraints of RCU:
        Access to shared resource should be mostly read, and very rare write
        Cannot sleep into region protected by RCU
        Protected resource should only be accessed via pointer

    Read Operation:
        No Locking is required

    Write Operation:
        Locking is required

    RCU allows read access to happen concurrently with write updates
    Note: RCU Updaters/writers cannot block readers or force them to retry their accesses like seqlocks.

    Linux kernel uses many data structures that are read and updated intensively, especially in
        1. Virtual File System : Directory entry caches (dentry)
        2. Networking          : Routing Tables. Every outgoing packet requires to check routing table to determine which interface ( ethernet/Wifi etc. ) should be used.

    How it works:
        As readers do not check if the data they read is consistent(like the seqlock), writer have to
        apply all their changes with one atomical operation. RCU keep tracks of all users of the pointers to the shared data structure. When a shared data structure is going to change, it:
            first create a copy of the structure
            perform the change
            After all the readers have finished reading on the old copy, pointer is updated

    Why it is called RCU? When the writing thread needs to be changed,
        it makes a copy
        changes the copy
        updates the pointer to point to it

    Initial linked list
           HEAD
            |
            V
        --------------          -----------------               -------------
        |            |          |               |               |           |
        |   A        |--------->|   B           |-------------->|    C      |
        |            |          |               |               |           |
        --------------          ------------------              -------------

    Reader is reading
                 Reader
           HEAD   /
            |    /
            V   V
        --------------          -----------------               -------------
        |            |          |               |               |           |
        |   A        |--------->|   B           |-------------->|    C      |
        |            |          |               |               |           |
        --------------          ------------------              -------------

    Request to delete node B when there is reader at B
                Updater        Reader
           HEAD   /              |
            |    /               |
            V   V                v
        --------------          -----------------               -------------
        |            |          |               |               |           |
        |   A        |--------->|   B           |-------------->|    C      |
        |            |          |               |               |           |
        --------------          ------------------              -------------

    First phase of update (Element B unlinked from List)
             Updater          Reader
           HEAD   /             |
            |    /              |
            V   V               v
        --------------          -----------------               -------------
        |            |          |               |               |           |
        |   A        |---       |   B           |-------------->|    C      |
        |            |  |       |               |            -->|           |
        --------------  |       ------------------           |  -------------
                        |                                    |
                        |                                    |
                        --------------------------------------

    Second phase of update (Updater deletes the node after grace period)
                                    Updater                     Reader
           HEAD                        |                           |
            |                          |                           |
            V                          V                           v
        --------------          -----------------               -------------
        |            |          |               |               |           |
        |   A        |---       |   B           |-------------->|    C      |
        |            |  |       |               |            -->|           |
        --------------  |       ------------------           |  -------------
                        |                                    |
                        |                                    |
                        --------------------------------------

    After node is deleted
           HEAD
            V
        --------------                                  -----------------
        |            |                                  |               |
        |   A        |--------------------------------->|   C           |
        |            |                                  |               |
        --------------                                  -----------------

    suppose there is a reader at A, at request comes to delete B. If it gets deleted then when reader moves to B, it will dereference NULL pointer and crashes/OOPS. Here we have to point A to node C. So, reader will move directly to A to C. We should not delete node B untill reader moves from there. This is how RCU read copy update works. ) video ( 7 - 13 ).

    RCU Design: Core of RCU is based on two primitives:
        1. RCU Read-side critical sections : rcu_read_lock/rcu_write_lock
        2. RCU Synchronization : synchronize_rcu/call_rcu()

    Developers can use RCU Read-Side Critical sections and RCU Synchronization to build data structures that allow concurrent reading during updates.
        Header File: <linux/rcupdate.h>

    Core RCU API:
        a.  rcu_read_lock()
        b.  rcu_read_unlock()
        c.  synchronize_rcu() / call_rcu()
        d.  rcu_assign_pointer()
        e.  rcu_dereference()

    Write operation:
        Direct assign a new pointer to a pointer is protected by RCU is forbidden. You need to use rcu_assign_pointer() function.
            struct my_data
            {
                int key;
                int val;
            };
            struct my_data *global = NULL;

        Write operation:
            struct my_data *new_ptr;
            new_ptr = kmalloc(sizeof(struct my_data), GFP_KERNEL);
            new_ptr->key = 1;
            new_ptr->val = 1234;
            rcu_assign_pointer(global, new_ptr);

    Why can't i directly write to the pointer/global shared resource? or Why should i use rcu_assign_pointer? Consider the below code fragment:
        struct my_data
        {
            int key;
            int val;
        };
        struct my_data *global = NULL;
        ....
        struct my_data *ptr;
        ptr = kmalloc(sizeof(struct my_data), GFP_KERNEL);
        ptr->key = 1;
        ptr->val = 1201;
        global = ptr;

    The problem here is CPU/Compiler can perform optimizations and can execute the above last three lines in any order. If assignment of ptr to global happens before initialization of ptr fields, concurrent readers can could see the unitialized values.
        ptr = kmalloc(sizeof(struct my_data), GFP_KERNEL);
        global = ptr;
        ptr->key = 1;
        ptr->val = 1201;

    We need to keep use memory barriers to keep the things in order. rcu_assign_pointer will internally use memory barriers and make everything happen as per the order.

    Read operation: It is forbidden to simply de-reference the pointer, protected by RCU region. You need to use rcu_dereference() function. Additionally, the code that de-references the pointer and uses the result needs to be embraced by calls to rcu_read_lock() and rcu_read_unlock().
        rcu_read_lock() - mark the beginning of an RCU read-side critical section
        rcu_read_unlock() - marks the end of an RCU read-side critical section.

        struct my_data
        {
            int key;
            int val;
        };
        struct my_data *global = NULL;
        global = kmalloc(sizeof(struct my_data), GFP_KERNEL);

        Read operation:
            struct my_data *tmp;
            rcu_read_lock();
            tmp = rcu_dereference(global);
            pr_info("key:%d\t val:%d\n", tmp->key, tmp->val);
            rcu_read_unlock();

    Why can't i directly dereference to the pointer/global shared resource? or Why should i use rcu_dereference? Consider the below code fragment:
        struct my_data
        {
            int key;
            int val;
        };
        struct my_data *ptr;
        ptr = global;
        add_key_val(ptr->key, ptr->val);

    Due to compiler optimization, value of ptr->key, ptr->val are fetched before the value of ptr. Compiler tries to guess value of ptr.

    retry:
        ptr = guess(global)
        add_key_val(ptr->key, ptr->val);
        if (ptr != global)
            goto retry;

    rcu_dereference() internally uses memory barrier instructions/compiler directives to avoid this.
        rcu_read_lock();
        ptr = rcu_dereference(global);
        if (ptr != NULL) {
            add_key_val(ptr->key, ptr->val);
        }
        rcu_read_unlock();

    What is the problem in the below code?
    Memory Leak
    - at write thread we are allocating memeory but not freeing.

    When should i free memory? RCU should free after waiting For Pre-Existing RCU Readers to Complete. RCU waits on "RCU read-side critical sections".

    RCU read-side critical sections:
        rcu_read_lock();
        .....
        critical section
        .....
        rcu_read_unlock();

    Basic idea behind RCU is to split updates/write into two phases:
            1. Removal
            2. Reclamation

    Removal:
            Replaces references to data items with the latest

    Reclamation:
            Freeing the old reference.
            It should happen only when all the readers completed accessing using old reference

    void synchronize_rcu(void);
    Calling process is blocked until all pre-existing RCU read-side critical sections on all CPUs have completed. After the function returns, it is safe to free the memory associated with the old pointer. Note that synchronize_rcu will not necessarily wait for any subsequent RCU read-side critical sections to complete.

            CPU 0                  CPU 1                 CPU 2
             ----------------- ------------------------- ---------------
         1.  rcu_read_lock()
         2.                    enters synchronize_rcu()
         3.                                               rcu_read_lock()
         4.  rcu_read_unlock()
         5.                     exits synchronize_rcu()
         6.                                              rcu_read_unlock()

    void call_rcu(struct rcu_head *head, rcu_callback_t func);

    Another way to avoid blocking is to register a callback which will be called when all the read-side critical sections are completed. This requires that an instance of rcu_head is embedded.

    Can the RCU read-side critical sections be nested?
    Yes, as long as that code does not explicitly block or sleep.

    How does RCU/synchronize_cpu() know that all readers have finished reading?
    We know RCU read-side critical sections delimited by rcu_read_lock() and rcu_read_unlock() are not permitted to block or sleep. Therefore, when a given CPU executes a context switch, we are guaranteed that any prior RCU read-side critical sections will have completed. synchronize_cpu() returns as soon as all the CPU's have completed one context switch.

    RCU Terminology:
    - Quiescent state: Any code that is not in an RCU read-side critical section. Readers could see stale data if they enter the read-side critical section before the writer finished updating. Writer has to wait until all the readers drop their references to the stale data or they have entered the quiescent state.
    - Grace Period:   The above time span is called the grace period. Grace period ends after all CPUs execute a context switch.
                    ------------------  Context Switch
                    |   RCU          |          |
                    V       Reader   V          V
        CPU0        ----------------------------------------

                        ------------------  Context Switch
                        |   RCU          |          |
                        V       Reader   V          V
        CPU1        ---------------------------------------------------

                        Synchronize_rcu()
                            |
                            V
        CPU2        ---------------------------------------------------------
                            |   Grace               |
                            |   Period              |
                            -------------------------

    RCU Variants of Linked List API:
        Header File: <linux/rculist.h>

        void INIT_LIST_HEAD_RCU(struct list_head *list);
        void list_add_rcu(struct list_head *new, struct list_head *head);
        void list_add_tail_rcu(struct list_head *new, struct list_head *head);
        void list_del_rcu(struct list_head *entry);
        list_entry_rcu(ptr, type, member);
        list_for_each_entry_rcu(pos, head, member);

    Advantages of RCU's:
        1. Performance
        2. Deadlock Immunity: As they do not use locks
        3. Realtime Latency: As they do not use locks

    Do readers need to take lock while operating inside the critical section? Locks are not required in case of readers using RCU.

    Do writers need to take lock while operating inside the critical section? Locks are required in case of writers using RCU to avoid another writer concurrently entering critical section.

    What is quiescent state?
    ============================
    ----
    threads are not in a read side critical section

    What is grace period? from start of synchronize RCU to end of synchronize RCU.

    Do the reader needs to retry the read operation like seqlock when the writer is also in critical section? Not required. Writer needs to handle this and atomically update the shared resource to the latest.

    What is the disadvantage of the RCU when compared to seqlock? As we are performing copy operation to update the data structure, increased memory cost.

    Can I sleep inside a region protected by RCU ? No, you cannot sleep inside a region protected by RCU.

    What is the similarity between read-write locks and RCU's? Both have read-side critical sections that can execute in parallel.

    To read more: https://free5gc.org/blog/20231129/20231129/, http://lastweek.io/notes/linux/linux-rcu/, https://medium.com/@boutnaru/the-linux-process-journey-rcub-read-copy-update-boost-7b0bb62b4454

21_1)
    Schedule() implements the scheduler.
        Implementation: kernel/sched.c
    The most important data structure used by scheduler is run queue. Run Queue: Contains the list of all processes which are in TASK_RUNNING state. Each CPU has its own run queue and each active process will be present on just one run queue.

    How it is invoked?
        1. Direct Way: When the current process do not have the resource it needs, and want to block itself until it acquires the resource, it calls schedule
        2. Lazy Way: When the time quantum/time slice of the current process is completed.

    Sleeping in Linux: At times, processes needs to wait until a certain event occurs, for example
            1. Device to Initialize
            2. I/O completion
            3. Fix Interval of time to elapse
        In such cases, the process is said to sleep on that event. A process can go to sleep by calling schedule(). When a process is put to sleep, it is marked as being in a special state and removed from the scheduler's run queue. Until something comes along to change that state, the process will not be scheduled on any CPU and, therefore, will not run.

    Process States:
        TASK_RUNNING : Running or ready to run
        TASK_ZOMBIE  : Task has terminated, but waiting for parent to call wait
        TASK_STOPPED : Process Execution has stopped; happens when it receives SIGSTOP
        TASK_INTERRUPTIBLE
        TASK_UNINTERRUPTIBLE

    TASK_INTERRUPTIBLE vs TASK_UNINTERRUPTIBLE
    ===========================================
        TASK_INTERRUPTIBLE                          TASK_UNINTERRUPTIBLE

        1. Process is sleeping/blocked              1. Process is sleeping/blocked
           waiting for some condition                  waiting for some condition
           to exist                                    to exist
        2. When the condition becomes true          2. When the condition becomes true
           the process is set to TASK_RUNNING          the process is set to TASK_RUNNING
           by kernel                                   by kernel
        3. When signal is send to this process      3. When signal is send to this process
           it wakes up and becomes runnable            it doesn't wake up

    TASK_UNINTERRUPTIBLE is  mostly used by device drivers waiting for disk or network I/O. How to find out what wait channels processes are waiting on?
        $ ps -l (to see processes associated with the current shell)
        $ ps -el (to see all processes on the system)

    If a process is in Sleep state, the WCHAN field shows the system call that the process is waiting on.
    WCHAN(Wait Channel) - name of the kernel function in which the process is sleeping. State:
        D    uninterruptible sleep (usually IO)
        S    interruptible sleep   (waiting for an event to complete)

    schedule() function invokes scheduler and it then picks up the next process/task from the run queue. process invoking the schedule() voluntarily yields the processor, as the process is still in run queue, it would be scheduled again. You must call set_current_state() before calling schedule() to move it from the run queue.
        Header File: <linux/sched.h>

    When the schedule() function is called with the state as TASK_INTERRUPTIBLE or TASK_UNINTERRUPTIBLE, an additional step is performed:
        the currently executing process is moved off the run queue before another process is scheduled.
        The effect of this is the executing process goes to sleep, as it no longer is on the run queue.
        Hence, it never is scheduled by the scheduler. And, that is how a process can sleep.
    There is no one trying to wake up the process, hence the process is sleeping unconditionally.

    Waking up:
        Given a reference to a task structure, the process could be woken up by calling:
            wake_up_process(sleeping_task);
            Implementation: kernel/sched/core.c
                --> this sets the task state to TASK_RUNNING
                --> puts the task back on the run queue.
        Note:   the process runs only when the scheduler looks at it the next time around.
        Return:
            1 if the process was woken up,
            0 if it was already running.
    example is a device driver showing skeep and wake of process.

    schedule_timeout():
        long schedule_timeout ( signed long  timeout);
        timeout - timeout value in jiffies
    Make the current task sleep until timeout jiffies have elapsed. The routine will return immediately unless the current task state has been set using set_current_state. The current task state is guaranteed to be TASK_RUNNING when this routine returns.
        Return value:
            0   when the timer has expired
            remaining time in jiffies, if the signal is received or process is woken up

    Lost Wake Up Problem:
        processes go to sleep after checking some condition.
        Lost wakeup problem arises out of a race condition that occurs while a process goes to conditional sleep.
        Process A                                   Process B
        ====================                        ========================
        if (list_empty(&mylist_head))               list_add_tail_rcu(&mylist_head);
        {                                           wake_up_process(task_a);
            set_current_state(TASK_INTERRUPTIBLE);
            schedule();
        }
        //Rest of the Code which performs operation on list

    What is the problem in the above code?
        If process A which was executing list_empty  on one processor found that the list is empty and it entered the if loop. At the same time, process B started on another processor starts and executes all its instructions. It calls wake_up_process on process A which has not yet slept. Now process A sets the state to TASK_INTERRUPTIBLE and goes to sleep. Thus, a wake up from process B is lost. This is known as lost-wakeup problem. Process A sleeps, even though there are nodes available on the list.
    Solution: Problem goes if we modify our code

        Process A                                   Process B
        ====================                        ========================
        set_current_state(TASK_INTERRUPTIBLE);
        if (list_empty(&mylist_head))               list_add_tail_rcu(&mylist_head);
        {                                            wake_up_process(task_a);

            schedule();
        }
        //Rest of the Code which performs operation on list

    How?
        Whenever wake_up_process() is called on process whose state is TASK_INTERRUPTIBLE/TASK_UNINTERRUPTIBLE, and the process has not yet called schedule(), it changes the state to TASK_RUNNING. Even if the wake_up_process is called after list_empty, as the state is TASK_RUNNING, it will not put the process into sleep. We can't spinlock because schedule will put process into sleep state. But can use semaphore but above solution is better.

    The following is a snippet from kernel/kthread.c:
        set_current_state(TASK_INTERRUPTIBLE);
        if (list_empty(&kthread_create_list))
            schedule();
        __set_current_state(TASK_RUNNING);
    Here in this code, it is checking the condition only after setting the state to TASK_INTERRUPTIBLE. Why do you think last line is needed? It is setting the task state to RUNNING, as there are chances that the list_empty condition may fail() and schedule() will never be called.

21_2)
    Sometimes processes may need to go to Sleep state for a particular amount of time
        void msleep(unsigned int msecs);
        Header File : <linux/delay.h>
    msleep will put the processor into uninterruptible sleep. [ ps ] will show you 'D' as the state.

    unsigned long msleep_interruptible(unsigned int msecs); will put the process into interruptible sleep
        Return value :
            Normally 0
            If the process is awakened early, then the return value is number of milliseconds remaining in the original requested sleep period.

    void ssleep(unsigned int seconds);
        Header File: <linux/delay.h>
    it is uninterruptible sleep.

22)
    How are semaphores implemented? Look at kernel/locking/semaphore.c
        struct semaphore {
                raw_spinlock_t          lock;
                unsigned int            count;
                struct list_head        wait_list;
        };

    Issue using schedule/wake_up_process:
        1. Need to write code carefully to avoid lost-wakeup problem
        2. Waker process needs to know the task_struct of the sleeping process.
           This can become tedious when there are more than one process involved in sleeping.

    Wait queues: Wait queues are a higher-level mechanism which handles
        1. putting processes to sleep and
        2. waking them up.

    Wait queues are used to enable processes to wait for a particular event to occur without the need for constant polling. A wait queue is a simple list of processes waiting for an event to occur. Processes sleep during wait time and are woken up automatically by the kernel when the event takes place.
        Header File: <linux/wait.h>
        Data Structure: wait_queue_head_t
            struct wait_queue_head {
                    spinlock_t              lock;
                    struct list_head        head;
            };
        typedef struct wait_queue_head wait_queue_head_t;
            lock -->    used to protect its own resources from being accessed by multiple processes at the same time
            head -->    List of wait_queue_entry

    Types of sleeping processes:
        1. Exclusive Process: Processes are selectively woken up by the kernel when the event happens
        2. Non Exclusive Process: All the processes present in the wait queue are woken up by the kernel on event.

    Static:
        DECLARE_WAIT_QUEUE_HEAD(name);
        #define __WAIT_QUEUE_HEAD_INITIALIZER(name) {                                   \
                .lock           = __SPIN_LOCK_UNLOCKED(name.lock),                      \
                .head           = { &(name).head, &(name).head } }
        #define DECLARE_WAIT_QUEUE_HEAD(name) \
                struct wait_queue_head name = __WAIT_QUEUE_HEAD_INITIALIZER(name)
    Dynamic:
        wait_queue_head_t my_queue;
        init_waitqueue_head(&my_queue);

    API's:
        wait_event(queue, condition);
            queue -->   wait queue head to use.
            condition --> arbitrary boolean expression that is evaluated by the macro before and after sleeping;
    The process goes to sleep only if the condition evaluates to false. Care is taken to avoid the lost wake-up problem. Some other thread of execution (a different process, or an interrupt handler, perhaps) has to perform the wakeup for you, since your process is, of course, asleep.

    void wake_up(wait_queue_head_t *queue); Note: until condition evaluates to a true value, the process continues to sleep.

    wait_event_interruptible(queue, condition); It can be interrupted by signals. This version returns an integer value that you should check; a nonzero value means your sleep was interrupted by some sort of signal, and your driver should probably return -ERESTARTSYS.

    void wake_up_interruptible(wait_queue_head_t *queue);

    wait_event_timeout(queue, condition, timeout)
        The process is put to sleep (TASK_UNINTERRUPTIBLE) until the condition evaluates to true.
        The condition is checked each time the waitqueue is woken up.
        Return Value:
            0 if the condition evaluated to false after the timeout elapsed
            1 if the condition evaluated to true after the timeout elapsed
            remaining jiffies (at least 1) if the condition evaluated to true before the timeout elapsed.

    wait_event_interruptible_timeout(queue, condition, timeout)
        process is put to sleep (TASK_INTERRUPTIBLE) until the condition evaluates to true or a signal is received. The condition is checked each time the waitqueue wq is woken up.
        Return value:
            0 if the condition evaluated to false after the timeout elapsed
            1 if the condition evaluated to true after the timeout elapsed
            the remaining jiffies (at least 1) if the condition evaluated to true before the timeout elapsed,
            or -ERESTARTSYS if it was interrupted by a signal.

23)
    Little issue while waking:
        Processes which are blocked/sleeping using wait_event() are moved to running state using wake_up(). Scheduler will run them in future. There is no guarantee when these waking-up processes will be allocated CPU time nor in what order. The above can happen to multiple processes at the same time, if they are non-exclusive. Once woken up, there is no guarantee that the condition which the process is waiting for using wait_event() is true. That is if the process was waiting for condition = 1 after waking up, this may not be true. Another process in the wait queue can change the condition = 0 after they woke up. So, the waking processes needs to check the state of the condition after waking up and act accordingly.

    Thundering Herd Problem:
        When a process calls wake_up on a wait queue, all processes waiting on the wait queue are made runnable. Consider a scenario in which a set of processes are sleeping on wait queue, wanting to acquire lock.
            --->    The process that has acquired the lock is done with it, releases the lock
            --->    All the processes that are sleeping for it will wake up
            --->    All processes try to grab lock
            --->    Only one of these acquires the lock and the rest goes back to sleep
        If the number of procesess in the wait queue is large, it seriously degrades the performance of system. As, it consumes valuable CPU cycles and incur context-switching overheads.

    To address the thundering herd problem, we need an exclusive sleeping system that only wakes up one task from the wait queue at a time. Exclusive wait can be set up by using this macro:
        wait_event_interruptible_exclusive(wait_queue_head_t wq, int condition);

        void wake_up_all (wait_queue_head_t *wq);
        void wake_up_interruptible_all (wait_queue_head_t *wq);
            The above functions will wake up all threads.

    Why are the wait_event() implemented as macros?
        1. Use of macros will expand the code and removes a function call/return pair.
        2. Using macros avoid the race condition which can happen
            CPU1            CPU2
            wait_event      wake_up

           How?
            wait_event(condition) {
                prepare_to_wait();            //add ‘current’ to wait-queue, set state to !runnable
                if (!condition) schedule(); //check condition, possibly give up the CPU
                finish_wait();                    //remove from wait-queue, set state to runnable
            }

            Condition can be:
                1. A variable that evaluates to true/false (eg. x)
                2. Test ( eg x == 1234)
                3. Function (atomic_read(&counter))
            If wait_event() was implemented as a function, condition argument will be passed as a value
            and it would be stale/old when evaluated at if (!condition) schedule();

            It will have the value when wait_event will be called and not the latest value of condition. Using macros, condition argument is always latest value.

    wake_up_interruptible() --> Can only wake up Tasks which are sleeping in interruptible state
    wake_up()   --> Can wake up both tasks which are in interruptible/non interruptible

    waitqueue_active -- locklessly test for waiters on the queue. it checks if waitqueue is empty or not. It checks it by checking if HEAD linked list is empty or not.

    In many situations, wait_event() does not provide enough flexibility. Alternative is to do full manual sleep.
        wait_queue_entry_t:
        Wait queue = Wait Queue Head + Wait Queue Elements
    A wait queue is a doubly linked list of wait_queue_entry_t structures. Each element in the wait queue list represents a sleeping process, which is waiting for some event to occur;
        struct wait_queue_entry {
                unsigned int            flags;
                void                    *private;
                wait_queue_func_t       func;
                struct list_head        entry;
        };
        typedef struct wait_queue_entry wait_queue_entry_t;
            flags = 1 -> Exclusive process
            flags = 0 -> Non Exclusive process
        private -> used to store task_struct
        func -> Function which will wake up the sleeping process
        entry -> used for linked list

        ---------------
        |    lock     |    --------------------------------------------------------------
        |             |    |                                                            |
        ---------------    |                                                            |
        |             |<----            -----------                     ----------      |
        |    head     |<--------------->|entry    |<------------------->|entry   |<------
        ---------------                 |---------|                     |--------|
                                        |func     |                     |func    |
        wait_queue_head_t               |---------|                     |--------|
                                        |private  |                     |private |
                                        |---------|                     |--------|
                                        |flags    |                     |flags   |
                                        -----------                     ---------
                                        wait_queue_entry_t      wait_queue_entry_t

    Initialization of wait queue entry:
        Static:
            DEFINE_WAIT(wait);
                Declares a new wait_queue_entry variable and initialize its with the descriptor of the process
                currently executing and function is assigned to autoremove_wake_function()
            #define DEFINE_WAIT(name) DEFINE_WAIT_FUNC(name, autoremove_wake_function)
            #define DEFINE_WAIT_FUNC(name, function)                                        \
                    struct wait_queue_entry name = {                                        \
                            .private        = current,                                      \
                            .func           = function,                                     \
                            .entry          = LIST_HEAD_INIT((name).entry),                 \
                    }

        Dynamic:
            void init_waitqueue_entry(struct wait_queue_entry *wq_entry, struct task_struct *p);
            static inline void init_waitqueue_entry(struct wait_queue_entry *wq_entry, struct task_struct *p)
            {
                    wq_entry->flags         = 0;
                    wq_entry->private       = p;
                    wq_entry->func          = default_wake_function;
            }
            int wake_up_process(struct task_struct *p)
            {
                    return try_to_wake_up(p, TASK_NORMAL, 0);
            }
            int default_wake_function(wait_queue_entry_t *curr, unsigned mode, int wake_flags,
                                      void *key)
            {
                    return try_to_wake_up(curr->private, mode, wake_flags);
            }
            int autoremove_wake_function(struct wait_queue_entry *wq_entry, unsigned mode, int sync, void *key)
            {
                    int ret = default_wake_function(wq_entry, mode, sync, key);

                    if (ret)
                            list_del_init(&wq_entry->entry);

                    return ret;
            }

    Adding an element into wait queue: Once an element is defined, it must be inserted into a wait queue. Two different functions are used to add sleeping processes into a wait queue.
            add_wait_queue()
            add_wait_queue_exclusive()
        add_wait_queue() function inserts a nonexclusive process in the first position of a wait queue list.
        void add_wait_queue(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry);
        add_wait_queue_exclusive() function inserts an exclusive process in the last position of a wait queue list.
        void add_wait_queue_exclusive(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry);
        The remove_wait_queue( ) function removes a process from a wait queue list.
        void remove_wait_queue(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry);
            Implementation: kernel/sched/wait.c

    Change the state of the process: You don't need to fiddle with current->state. prepare_to_wait() and finish_wait() will do that.
        void prepare_to_wait(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry, int state);
        void prepare_to_wait_exclusive(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry, int state);
    The above functions set the process state to the value passed as the third parameter. Running prepare_to_wait() when you're already on the waitqueue_head is fine. After this, we schedule out the process by invoking the schedule() API.

    Cleaning up: Once schedule returns, it is cleanup time.
        void finish_wait(wait_queue_head_t *queue, wait_queue_t *wait);
            DEFINE_WAIT(wait);
        prepare_to_wait(wq_head, &wait, TASK_UNINTERRUPTIBLE);
        if (!condition)
            schedule();
        finish_wait(wq_head, &wait);

    signal_pending: The above function can be used to check whether the wait was interrupted by signal.
        Returns : 1 -> If the process has pending signals
                0     -> If no signal
        static inline int signal_pending(struct task_struct *p)
        {
                return unlikely(test_tsk_thread_flag(p,TIF_SIGPENDING));
        }
        kernel/signal.c(signal_wake_up/signal_wake_up_state)    ->  Sets the TIF_SIGPENDING whenever a signal is delivered.

    Does this code have lost wake up problem?
        DEFINE_WAIT(wait);
        add_wait_queue(queue, &wait);
        while (!condition) {
            prepare_to_wait(&queue, &wait, TASK_INTERRUPTIBLE);
            if (signal_pending(current))
                /* handle signal */
            schedule();
        }
        finish_wait(&queue, &wait);
    What will happen if a wake_up comes just before prepare_to_wait() and after the while condition? The wakeup will be lost. prepare_to_wait() must be called before the condition is checked.

        DEFINE_WAIT(wait); // defining wait queue entry
        add_wait_queue(queue, &wait); // adding wait queue entry
        while (!condition) { // checking condition if list is empty or not, if list is not empty, come inside
            prepare_to_wait(&queue, &wait, TASK_INTERRUPTIBLE); // set state to interruptible
            if (signal_pending(current)) //
                /* handle signal */
            schedule();
        }
        finish_wait(&queue, &wait);
    at line no.11 signal may get lost if process is preempted
    ( video : 35:00 - 40:00 )

24_1)
    Completions: Completions is a code synchronization mechanism which is preferable to any misuse of locks/semaphores and busy loops.

    When to use them ? If you have one or more threads that must wait for some kernel activity to have reached a point or state. Completions can provide a race-free solution to this problem. Completions are built on top of the waitqueue and wakeup infrastructure of the Linux scheduler. The event the threads on the waitqueue are waiting for is reduced to a simple flag in 'struct completion', appropriately called "done".
        Implementation: kernel/sched/completion.c

    Without completions:
        int condition = 0;
        DECLARE_WAIT_QUEUE_HEAD(queue);

        Thread 1;                                       Thread 2
        ============                                    =============
        .....                                           ......
        wait_event_interruptible(queue, condition);     condition = 1;
        ....                                            wake_up_interruptible(queue);
        ....                                            ......

    With Completions:
        DECLARE_COMPLETION(my_comp);

        Thread 1;                       Thread 2
        ============                        =============
        .....                           ......
        wait_for_completion(&my_comp);              complete(&my_comp);

    Can't i use this semaphore for this purpose or What is the difference between semaphore and completion? Yes, you can use semaphore for the above scenario. You have a semaphore initially in the locked state. Waiting process will call down() and gets blocked. The one who is waking up will call up().

    Advantages of Completions over Semaphores:
        1. the semaphores are optimized (on purpose) for the non-contention case. Means initially it will not be in locked state. It will be in unlocked state initially. completion usage has the opposite default case.
        2. Multiple threads can wait for a completion, and they can be released with one call. It's more complex to have a semaphore wake up an unknown number of threads.
        3. Semaphores usage is for mutual exclusion. Completion usages is typically for synchronization

    Header File: <linux/completion.h>
    Data Structure: struct completion
    struct completion {
            unsigned int done;
            wait_queue_head_t wait;
    };
    wait --> wait queue to place tasks on for waiting
    done --> indicating whether it is completed or not

    Usage:
    There are three main parts to using completions:
        1. the initialization of the 'struct completion' synchronization object
        2. the waiting part through a call to one of the variants of wait_for_completion(),
        3. the signaling side through a call to complete() or complete_all().

    Initialization:
        Static:     DECLARE_COMPLETION(my_comp);
        Dynamic:    init_completion(&my_comp);

    In initialization:
        1. we initialize the waitqueue
        2. set done to 0, i.e. "not completed" or "not done".

    Waiting for Completion: For a thread to wait for some concurrent activity to finish, it calls wait_for_completion() on the initialized completion structure.
        void wait_for_completion(struct completion *done);

    When some other part of your code has decided that the completion has happened, it can wake up anybody
        void complete(struct completion *comp);

    A typical usage scenario is:
        CPU#1                                   CPU#2
        struct completion setup_done;
        init_completion(&setup_done);
        initialize_work(...,&setup_done,...);
        /* run non-dependent code */            /* do setup */
        wait_for_completion(&setup_done);       complete(setup_done);

    What happens when a call to complete happens before wait_for_completion? the waiting side simply will continue immediately as all dependencies are satisfied. If not, it will block until completion is signalled by complete().

    wait_for_completion will places the task in 'TASK_UNINTERRUPTIBLE' state, if you want the process to be placed in 'TASK_INTERRUPTIBLE' state.
        int wait_for_completion_interruptible(struct completion *done)

    Other Variants:
        unsigned long wait_for_completion_timeout(struct completion *done, unsigned long timeout)
            The task is marked as TASK_UNINTERRUPTIBLE and will wait at most 'timeout' jiffies.
            Return Value: 0 on Timeout, else the remaining time in jiffies

        long wait_for_completion_interruptible_timeout(struct completion *done, unsigned long timeout)
            This function passes a timeout in jiffies and marks the task as TASK_INTERRUPTIBLE.
            Return value: 0 on Timeout
                      Remaining time in jiffies if complete() was called
                       -ERESTARTSYS on receiving a signal

        long wait_for_completion_killable(struct completion *done)
        long wait_for_completion_killable_timeout(struct completion *done, unsigned long timeout)
            Uses state as TASK_KILLABLE

        bool try_wait_for_completion(struct completion *done);
            Returns
                false: if the thread not to be blocked and will not put into the wait queue
                true: if the thread consumes one posted completion.

        bool completion_done(struct completion *done)
            Returns:
                False: If there are waiters
                True: otherwise

    complete_all: void complete_all(struct completion *); complete_all, wakes up all processing waiting for the completion.

    what is the meaning of done in struct completion? Each time complete is called, the counter is incremented by 1, The wait_for functions only puts the caller to sleep if done is not equal to 0. complete_all works similarly, but sets the counter to the largest possible value (UINT_MAX);
        void complete_all(struct completion *x)
        {
                unsigned long flags;

                spin_lock_irqsave(&x->wait.lock, flags);
                x->done = UINT_MAX;
                __wake_up_locked(&x->wait, TASK_NORMAL, 0);
                spin_unlock_irqrestore(&x->wait.lock, flags);
        }
        EXPORT_SYMBOL(complete_all);

    Can wait_for_completion() and its variants are safe to use in atomic/interrupt contexts? No, as they sleep.

    If complete() is called multiple times then this will allow for that number of waiters to continue - each call to complete() will simply increment the done field.

    Can i call complete() or complete_all() from interrupt/atomic context? Signaling completion from IRQ context is fine as it will appropriately lock with spin_lock_irqsave()/spin_unlock_irqrestore() and it will never sleep.

    static inline void reinit_completion(struct completion *x)
    {
            x->done = 0;
    }
    This inline function should be used to reinitialize a completion structure so it can be reused. This is especially important after complete_all() is used. Calling init_completion() on the same completion object twice is most likely a bug.

24_2)
    Each instruction in C is translated into machine instruction. To complete each machine instruction, the processor goes through these (and more) stages:
        1. Fetch: Read the next instruction
        2. Decode: Determine the meaning of the instruction
        3. Execute: Perform the 'real work' of the instruction
        4. Store: Store results into memory

            Instruction1        Fetch   ->  Decode  ->  Execute ->  Store
            Instruction2                    Fetch   ->  Decode  ->  Execute ->  Store
            Instruction3                                Fetch   ->  Decode  ->  Execute  -> Store

    Each instruction in the above takes four clock cycles to complete execution. With pipeline, you will have the total execution of 1 clock cycle/instruction. Modern processors have pipelines with 10-31 stages. For optimum performance, it is very important to keep all the stages as busy as possible.

    Branch Prediction: Branches are instructions that can change the flow of a program's execution.
        if (i < 0)
            i = 0;
        else
            i = 1;
    Branches (i.e., conditional jumps) present a difficulty for the processor pipeline. After fetching a branch instruction, the processor needs to fetch the next instruction. With 'if' we will be having two possibilities of the next instruction. Instead of stalling the pipeline until the branch instruction is fully executed, modern processors attempt to predict/guess the branch.

    Branch Predictor: Digital Circuit that tries to guess which way a branch will go before this is known definitively. Branch predictor plays a critical role in achieving high effective performance in many modern pipelined microprocessor architecture such as x86. If the guess/prediction found to be wrong, then the processor will simply discard the partially executed instructions that are in pipeline and starts over with the correct branch, incurring a delay.

    Command: $ perf stat <command>. for example, [ perf stat ls ] it will show page fault, context switches, branch misses etc. Run on Direct Machine without any virtualization.

    likely/unlikely: The gcc has __builtin_expect function using which you can provide the compiler/CPU with branch prediction information.
        long __builtin_expect (long exp, long c)
    This construct tells the compiler that the expression 'exp' most likely will have the value 'c'.
        Return value: return value of exp.

    How it optimizes? It optimizes things by ordering the generated assembly code correctly, to optimize the usage of the processor pipeline. Arranges the code so that the likeliest branch is executed without performing any jmp instruction. Kernel has two macros which internally uses builtin_expect to provide branch prediction information.
        #define likely(x)       __builtin_expect(!!(x), 1)
        #define unlikely(x)     __builtin_expect(!!(x), 0)

    !! converts it to boolean. Just think

        Header File: <linux/compiler.h>
        Examples:  if (likely(sem->count > 0))


    CONFIG_PROFILE_ANNOTATED_BRANCHES: By enabling CONFIG_PROFILE_ANNOTATED_BRANCHES in the kernel build config file, all the likely() and unlikely() macros will be recorded to see how many times they were correct or not.

    $ cat /sys/kernel/debug/tracing/trace_stat/branch_annotated

    This will show what branches are correct or not. for higher performance, we use likely/unlikely. watch video [ 22 - 24 ]

25)
    What is the default operation in Linux when you call read() and there is no data: block/nonblock ? Default: Blocking
        Read:
            No data is available, the process must block. Data available, process is awakened, even if available data < requested data.
        Write:
            No space is available in write buffer, process must block. Space available, process is awakened, even if available space < requested write data

    Do we need a separate buffer in the kernel for write operation? No. The data can remain in the user space buffer. Benefit of a separate buffer for write: Reduced number of context switches and user-level/kernel-level transitions.

    Consider a slow device where the hardware can only accepts data transfer in few bytes. Kernel Driver without output buffer.
        1. User Process tries to write more than 100 bytes, as there is no output buffer, process writes few bytes:write()
        2. Kernel driver tries to write few bytes to the hardware and puts the process to sleep until write completes
        3. During this time, the context switch can happen, another process is given a chance
        4. Once the write on the hardware completes, the process is resumed (context switch)
        5. write() returns (kernel level to user level transition)
        6. User process loops until it performs the same operation.
    With an output buffer in the kernel.
        1. Write call succeed with a single system call
        2. Data is copied into the kernel output buffer
        3. Buffered data will be pushed later to device.
    So, as no going back to user space for second or third write and increased performance.

    How do i specify a non blocking I/O in linux through my user application? Explicitly nonblocking I/O is indicated by the O_NONBLOCK flag in filp->f_flags.
        Header File in user space : <fcntl.h>
            Read:
                    No data is available, return -EAGAIN
                    Data available, it is returned even if available data < requested data
            Write:
                    No space is available in write buffer, return -EAGAIN
                    Space available, it is written even if available space < requested write data

    Problem with blocking/non-blocking I/O: If we want to read from multiple devices, read system call with blocking is not a good solution. If one device has no data, the process will be blocked even though there is data available from the other device.
            Application                     Kernel
       ---- read    ------------------->    no data available --------
       |                                                              |
       |                                                              | Wait for data
       | process                                                      |
       | blocks             data ready     ----------------------------
       |  in                copy data      ----------------------------
       |  read                |                                       |  Copy data from
       |                      v                                       |  kernel to user
       ---- process <-------------------    copy complete   -----------
        data

    Another option is open the device in nonblocking mode, and continuously check whether there is data available, if there are more devices, it will consume lot of CPU. So, this is waste of time.
            Application                     Kernel
            read    ------------------->    no data available ------------
       |            <------------------                                   |
       |    read    ------------------->    no data available             | Wait for data
       |            <------------------                                   |
       |    read    ------------------->    data ready     ---------------
       |-----                               copy data      ---------------
       |    process                           |                           | Copy data from
       |    blocks                            v                           | kernel to user
       ---- process <-------------------    copy complete   --------------
            data

    I/O Multiplexing:
        poll and select allow a process to determine whether it can read from or write to one or more open files without blocking. They are often used in applications that use multiple input or output streams without getting struck on any one of them.
                     Application                     Kernel
                ---- select  ------------------->    no data available --------
    process    |                                                               |
    blocks     |                                                               | Wait for data
    waiting    |                                                               |
    one of many|                                                               |
    fds        |                                                               |
               -----        <------------------      data available -----------
               |    read    ------------------->     data ready     ------------
               |-----                                copy data      ------------
               |   process                            |                        |  Copy data from
               |    blocks                            v                        |  kernel to user
               ---- process <-------------------    copy complete   -----------
               ---  read

    What are the advantages of multithreading over I/O Multiplexing: I/O Multiplexing is better over purely multithreaded/multiprocess approaches in which a thread/process is launched to handle each file descriptor, since each thread/process requires additional memory, context switching overhead, etc. which may not scale well in handling large numbers of concurrent connections.

    process gives all file descriptor it wants to read/write. Then select system call comes from userspace to kernel space and checks if any file decsriptors are available. If no, then wait for data. If one the file descriptor is available to read, then it will return a file decscriptor available to read in select system call. Then user can now give read system call.

    select system call:
        int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);
    Select system call is used to instruct kernel:
        1. what file descriptors we are interested in (for reading, writing or an exception)
        2. How long to wait.
        Arguments:
            timeout: tells the kernel how long to wait for one of the specified descriptors to become ready
        Header file: <sys/time.h>
        struct timeval {
                   long    tv_sec;         /* seconds */
                   long    tv_usec;        /* microseconds */
               };
        Three possiblities with timeout:
                1. Wait forever (timeout is specified as a null pointer): Return only when one of the specified descriptors is ready for I/O.
                2. Wait up to a fixed amount of time (timeout points to a timeval structure). Return when one of the specified descriptors is ready for I/O but do not wait beyond the number of seconds and microseconds specified in the timeval structure.
                3. Do not wait at all (seconds = 0, microseconds = 0) Return immediately after checking the descriptors. This is called polling.
            Note: The wait in the first two scenarios is normally interrupted if the process catches a signal
        Descriptor Sets: (readfds,writefds, exceptfds). Descriptor sets are arrays of integers, each bit in each integer corresponding to a descriptor.
            readfds     --> Notify when data is available to read
            writefds    --> Notify when write buffer is having space to write
            exceptfds   --> Used in sockets (out-of-band data)
        nfds:
            It should be set to the highest numbered file descriptor in any of the three sets, plus 1. The indicated file descriptors in each set are checked up to this limit. ex: if we have 5 file descriptors but the highest number is 100, the select will check any bit from 0 to 100. Reason for this argument exists is for efficiency. Although each fd_set has room for many descriptors, typically 1024, this is much more than the number used by a typical process. The kernel gains efficiency by not copying unneeded portions of the descriptor set between the process and the kernel, and by not testing bits that are always 0.
        Macros for fd_set datatype:
            – FD_ZERO(fd_set *fdset);  // clear all bits in fdset
            – FD_SET(int fd, fd_set *fdset);  // turn on the bit for fd in fdset
            – FD_CLR(int fd, fd_set *fdset);   // turn off the bit for fd in fdset
            – int FD_ISSET(int fd, fd_set *fdset);  // is the bit for fd on in fdset?
        Return:
            select modifies the descriptor sets pointed to by the readset, writeset, and exceptset pointers. When we call the function, we specify the values of the descriptors that we are interested in, and on return, the result indicates which descriptors are ready. We use the FD_ISSET macro on return to test a specific descriptor in an fd_set structure. Any descriptor that is not ready on return will have its corresponding bit cleared in the descriptor set. The return value from this function indicates the total number of bits that are ready across all the descriptor sets. If the timer value expires before any of the descriptors are ready, a value of 0 is returned. A return value of –1 indicates an error (which can happen, for example, if the function is interrupted by a caught signal).

    Support for select/poll system call requires support from the device driver: poll method.
        unsigned int (*poll) (struct file *, poll_table *);
        Header file for poll_table structure : <linux/poll.h>

    Driver Poll Implementation:
    It needs to perform the following two operations:
        1. Call kernel's poll_wait() function on one or more wait queues
        2. Return a bitmask describing operations that could be immediately performed without blocking.
         void poll_wait (struct file *, wait_queue_head_t *, poll_table *);

    Bitmask:
        POLLIN  -->  This bit must be set if the device can be read without blocking.
        POLLRDNORM  -->  This bit must be set if "normal'' data is available for reading.
                            A readable device returns (POLLIN | POLLRDNORM).
        POLLOUT  --> This bit is set in the return value if the device can be written to without blocking.
        POLLWRNORM  -->   This bit has the same meaning as POLLOUT, and sometimes it actually is the same number.
                            A writable device returns (POLLOUT | POLLWRNORM).
    Device Drivers need not worry about poll_wait internals. They must use it as an opaque object.

    Problem with select:
        - What happens when i have file descriptors:1005, 1006 which i want to verify whether they are ready to read from. I will call select with nfds of 1007 and the appropriate value of readfds. Kernel gets a request from a user space program to monitor some file descriptors for reading. It knows that the file descriptors are smaller than 1007, but that's all it knows. To figure out which file descriptors the program is interested, the kernel needs to check all 1007 file descriptors, safely one by one. Checking 1007 file descriptors when the program really only cares about two is quite inefficient. What happens when the file descriptors were 10010 and 100019. select() performs very poorly once the file descriptors get large. maximum limit of file descriptor is [ uname -r ].
        - Second Problem with select: How many file descriptors should fd_set be able to hold? Ideally, it should be able to hold as many file descriptors as a process can have open. $ ulimit -a. Traditionally, Linux allowed only 1,024 file descriptors per process, so this was reasonable. From man page of select: select() can monitor only file descriptors numbers that are less than FD_SETSIZE. Executing FD_CLR() or FD_SET() with a value of fd that is negative or is equal to or larger than FD_SETSIZE will result in undefined behavior. See Bugs section of select man page.
        - Third problem with select: select modifies the readfds/writefds/exceptfds passed as input, you should either
            1. Reinitialize them for the next call
            2. Backup your readfds/writefds/exceptfds

    overcomes previous problems. poll function: poll() performs a similar task to select(2): it waits for one of a set of file descriptors to become ready to perform I/O.
        Header File: #include <poll.h>
        int poll(struct pollfd *fds, nfds_t nfds, int timeout);
            Arguments:
                fds: must point to an array of struct pollfd. Each element in the array specifies a file descriptor that the program is interested in monitoring, and what events on that file descriptor the program would like to know about.
            struct pollfd {
                int     fd;       /* descriptor to check */
                short   events;   /* events of interest on fd */
                short   revents;  /* events that occurred on fd */
            };
            nfds: Number of items which are present in the fds argument.
            timeout: the number of milliseconds that poll() should block waiting for a file descriptor to become ready
            Returns:
                count of ready descriptors on success
                0 on timeout
                -1 on error

    events/revents:
        events: input parameter, a bit mask specifying the events the application is  interested in for the file descriptor fd
        revents: output parameter, filled by the kernel with the events that actually  occurred.

    Input to Events:
        POLLIN          There is data to read.
        POLLOUT         Writing is now possible

    Error value in revents:
        POLLERR:        This is set if an error condition has occurred on the file descriptor.
        POLLHUP:        This is set when the file descriptor refers to a terminal that has been hung upon.
        POLLNVAL:       Invalid request: fd not open

26)
    Developers, Maintainers in Linux Kernel: Most parts of the kernel have an associated maintainer. The maintainer is the individual (or individuals) who is in charge of specific parts of the kernel. Example:
        1. Each individual driver has an associated maintainer
        2. Each Kernel subsystem for example networking subsystem also has an associated maintainer. Subsystems may have multiple maintainers. Examples of subsystems:
            a) Memory Management
            b) Networking
            c) scheduling
            d) USB
            e) PCI
        These maintainers decide which patch goes to the mainline kernel. The maintainer for a specific driver or subsystem is usually listed in the file MAINTAINERS, which is also located in the root of the kernel source tree. There is a special type of maintainer known as the kernel maintainer. This individual actually maintains the kernel tree. [ vi MAINTAINERS ] has list of maintainers in Linux.

    What is Linux Kernel Tree? The Linux Kernel source-tree is a directory which contains all of the kernel source. You could build a new kernel, install that, and reboot your machine to use the rebuilt kernel. There are several main and susbsystem git repositories of Linux Trees.

    Mainline Kernel Tree: Maintained by Linus Torvalds. This is the tree where Linux releases mainline kernels and RC releases.

    Stable Tree: Maintained by Greg Kroah-Hartman. This tree consists of stable release branches. Stable releases are based on this tree.

    linux-next Tree: Maintained by Stephen Rothwell. Before updates from subsystem trees are merged into the mainline tree, they need to be integration-tested. This tree is used for integration testing. The linux-next tree contains the latest version of the staging tree.

    Staging Tree: Used to hold stand-alone drivers and filesystems that are not ready to be merged into main portion of
    Linux Kernel tree at this point due to coding standards or quality issues. Resolves the "hundreds of different download sites" problem that most out-of-tree drivers had in past. Location: drivers/staging folder.

    Submitting Patches: The development process itself happens entirely over emails.  Every kernel subsystem has a mailing list. Patches touching a specific driver/subsystem are sent to the maintainer listed in MAINTAINER. The MAINTAINER file consists of subsystem git information and mailing lists for each of the subsystem. Contributors send patches to mailing lists through email. While sending the patch, carbon copy (cc) to  linux-kernel@vger.kernel.org (Linux Kernel Mailing List):
        Subject of the mail: "[PATCH] brief description"
        Body of the mail: Technical details of your changes patch makes and reasons behind those changes
        Add Kernel version to the email.
        Attach the patch as plain text.
        Note: If your patch is large or contains several logical changes, you should break the patch into chunks with each chunk representing a logical change.
        Each Email should send one patch.

    Linux Kernel Release Cycle:
        1. Linus Torvalds releases a new kernel and opens a 2-week merge window.
        2. Subsystem maintainers collects patches ahead of time and send them to Linus during this merge window.
        3. Nearly 10,000 patches get pulled into linus's tree during these 2 weeks.
        4. At the end of two weeks Linus declares the merge window has closed ande releases the first release candidate known as rc1.
        5. At this point, the release cycle moves into a bug fixes-only mode, with a series of release candidate(rc) from Linus.
        6. Every week a new rc is released with the name 5.12.rc1, 5.12.rc2, and so on.
        7. Finally after the end of these rc weeks the kernel is stable and ready for release as version 5.12.
        8. The whole development is a matter of 10-12 weeks and we get a new version in every three months. References: https://www.kernel.org/doc/html/latest/process/2.Process.html

    Types of Releases:
        1. Prepatch/RC:
            Mainline kernel pre-releases
            Contains new features which must be tested before they can put into a stable release
            Maintained and released by Linus Torvalds
            https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/refs/tags?h=v5.4-rc6
        2. Mainline:
            Maintained by Linus Torvalds
            Tree where all the new features are introduced.
            New mainline kernels are released every 2-3 months
        3. Stable:
            After each mainline kernel is released, it is considered "stable."
            Any bug fixes for a stable kernel are backported from the mainline tree and applied by a designated stable kernel maintainer.
            Stable kernel updates are released on as-needed basis, usually once a week.
            There are usually only a few bugfix kernel releases until next mainline kernel becomes available
        4. LongTerm:
            Stable releases are selected for long term maintenance to provide critical bug fixes for older kernel trees
            https://www.kernel.org/category/releases.html

    Linux Versioning:
    Mainline Kernels: x.y
    Stable Kernels: x.y.z

    How to find out whether my kernel is a distribution kernel or not ? Unless you downloaded, compiled and installed your own version of kernel from kernel.org, you are running a distribution kernel.
        # uname -r
        5.0.0-32-generic
    If you see anything at all after the dash, you are running a distribution kernel.

27)
    Building Kernel: The process of building a kernel has two parts
        1. Configuring the kernel options
        2. Building the source with those options

    Configuring Kernel: The kernel configuration is kept in a file called .config in the top directory of the kernel source tree. After downloading the sources, there will be no .config file, it needs to be created. It can be created from
        1. Scratch
        2. Default configuration from a running kernel version ( it is present in /boot/config-`uname -r` )

    Copying the configuration file of your distribution is the safest approach for the very first kernel install on any system. Run the following command to generate a kernel configuration file based on the current configuration.
        $ make oldconfig
        [ make oldconfig V=1 ]
    The above command reads the existing .config file and prompts the user for options in the current kernel source that are not found in this file.

    To build the kernel in a multithreaded way, use the -j option to the make program. It is best to give a number to the -j option that corresponds to twice the number of processors in the system. Example with four processors.
    $ make -j8

    Installing the new kernel: Once the kernel compilation is complete, install the new kernel with:
        $ sudo make modules_install

    This will install all the modules that you have built and place them in the proper location in the filesystemfor the new kernel to properly find. Modules are placed in the /lib/modules/kernel_version directory.

    kernel_version is the kernel version of the new kernel you have just built. After the modules have been successfully installed, the main kernel image must be installed.
    $ sudo make install

    The above command will perform the below steps:
        1. Kernel build system will verify that the kernel has been successfully built properly
        2. Kernel build system will install the static kernel into the /boot directory and name it based on the kernel version
        3. Any needed initial ramdisks images will be automatically created, using the modules that have been just installed during the modules_install phase
        4. The bootloader program will be properly notified that a new kernel is present, and it will be added to the appropriate menu so the user can select it next time the machine is booted. ( for example, GRUB is updated ).
        5. After this is finished, the kernel is successfully installed, and you can safely reboot and try out your new kernel image.

    IMP Note: Installation does not overwrite older kernel images. Very IMP: watch linux boot process at ( video 47 - 50 )

    Different Architecture: The kernel build system allows you to specify a different architecture from the current system with the ARCH= argument.

    There are many methods available for configuring the kernel.
        $ make defconfig
    Creates a ".config" file with default options from the ARCH supplied defconfig. Default Configurations are generally stored in the directory: arch/$(ARCH)/configs. When you run "make defconfig" on "x86" machine, it copies the configuration options from arch/x86/configs/i386_defconfig.

    If you diff the defconfig and .config, you will find they are not same why? .config vs defconfig:
    The .config file is a full config file: it contains the value for all options. A defconfig stores only the values for options for which the non-default value is chosen. When .config file is being generated, kernel build system goes through all Kconfig files (from all subdirs), checking all options in those Kconfig files:
        if option is mentioned in defconfig, build system puts that option into .config with value chosen in defconfig.
        if option isn't mentioned in defconfig, build system puts that option into .config using its default value, specified in corresponding Kconfig.

    make config:
        Text-based Configuration.
        Options are prompted one after another.
        All options need to be answered
        Access to former options is not possible

    make menuconfig:
        Menu-driven user interface
        Allows to navigate forwards and backward directly between features
        Allows to load and save files with filenames different from ".config"
        Provides search feature
        It uses ncurses library for GUI.
        If the ncurses library is not installed, make menuconfig option fails.
        To install ncurses library on Ubuntu: sudo apt-get install libncurses5-dev
        [ ] --> Yes/No
            [ ]  excluded
            [*]  built-in
        < > -> built-in/module/excluded
            <M>  Module
            <*>  Built-In
            < >  Left out altogether

    $ make clean
        Remove most generated files, keeps the config, enough build support to build external modules.

    $ make mrproper
        Remove all generated files, Removes config, Removes various backup files (include/config, include/generated, scripts/basic, scripts/fixdep).

    $ make distclean
        Performs all operations of make mrproper, Deletes temporary code navigation files:tags, cscope*, Deletes files generated as a side-effect of working with patches: *.orig *.rej *.bak, Deletes core dump files.

    Building Only a Portion of the Kernel: Kernel build system allows you to easily build a portion of the kernel. For example, if you want to build the files in drivers/pci.
        $ make drivers/pci
    The above command will not build the modules in that directory. To build modules
        $ make M=drivers/pci
    Finally execute
        $ make
    to have the build system check all changed object files and do the final kernel image link properly. To build only a particular module
        $ make drivers/usb/serial/usb-serial.ko
    The build system will build all needed files for the usb-serial.ko kernel module, and do the final link to create the module.

31)
    I/O Memory: The most widely supported form of IO is memory mapped IO. A part of the CPU’s address space is interpreted not as accesses to memory, but as accesses to a device. Some architectures define devices to be at a fixed address, but most have some method of discovering devices. Advantage of memory mapped I/O is that it keeps the instruction set small. Logic of creating separate I/O address space initially because the memory address space of processors was quite limited. When x86 moved to 32-bit, the address space was still same 64KB, even after it moved to 64-bit. Examples of I/O Memory:
        a) Holding Video Data
        b) Ethernet Packets
        c) Device Registers

    Requesting I/O Memory: Functions equivalent to request_region() and release_region(), but for I/O memory
        struct resource *request_mem_region(
                unsigned long start,
                unsigned long len,
                char *name);
        void release_mem_region(
                unsigned long start,
                unsigned long len);
        request_mem_region:
            ---> Informs kernel that your driver is going to use this range of I/O addresses
            ---> This prevents other drivers from using it through request_mem_region
        cat /proc/iomem lists all the kernel drivers requested 'request_mem_region'

    Can we access(read/write) MMIO memory directly? No, we can't . Kernel is running in virtual address space. Like user space, the kernel accesses memory through page tables. So, when kernel code needs to access memory-mapped I/O devices, it must first set up an appropriate kernel page-table mapping.
        void *ioremap(unsigned long phys_addr, unsigned long size);
        void iounmap(void * addr);
    A successful call to ioremap() returns a kernel virtual address corresponding to start of the requested physical address range. Return address of ioremap is not normally meant to be dereferenced directly, though, for a number of (often architecture-specific) reasons.

    Functions to read and write data using memory mapped by ioremap()
        Read:
            unsigned int ioread8(void *addr); unsigned int ioread16(void *addr); unsigned int ioread32(void *addr);
        Write:
            void iowrite8(u8 value, void *addr); void iowrite16(u16 value, void *addr); void iowrite32(u32 value, void *addr);

    Why do we need to call ioread instructions why can't i directly access by dereferencing? Compiler can perform optimizations. For example, the below logic:
        *reg  = 1; *reg = 2;
            will be converted to
                *reg = 2;
    MMIO registers will have side effects, so you must force the compiler to avoid optimizations by using volatile and avoid hardware caching using barriers. ioread functions internally perform these operations.

    Ports as I/O Memory: Linux kernel provides a function ioport_map which maps I/O ports and make them appear as I/O Memory.
        void *ioport_map(unsigned long port, unsigned int count);
    To unmap:
        void ioport_unmap(void *addr);
    Note: I/O Ports must still be allocated with request_region before they can be remapped in this way.

    Accessing mmio from user space: /dev/mem is a character device file that is an image of the main memory of the computer. ( main memory is not just RAM but includes all other devices also ). Byte addresses in /dev/mem are interpreted as physical memory addresses. [ man 4 mem ] for more information Implemented by drivers/char/mem.c. Accessing /dev/mem from command line:
        $ hexdump -C /dev/mem
        $ cat /dev/mem | strings
        $ cat /dev/mem | strings -n 20

    CONFIG_STRICT_DEVMEM kernel configuration option limits the areas which can be accessed through /dev/mem
        # cat /boot/config-`uname -r` | grep CONFIG_STRICT_DEVMEM

    It is enabled by default x86/x86_64 and ARM platforms. Enabling CONFIG_STRICT_DEVMEM implements strict access to /dev/mem so that it only allows user-space access to memory mapped peripherals. With this option disabled, the root user from user-space can access all kernel and user-space memory through /dev/mem. Try with qemu-system-arm. The QEMU emulator supports the VersatilePB platform, that contains an ARM926EJ-S core. Memory map can be read : cat /proc/iomem

    devmem present in busybox: devmem is a small program that reads and writes from physical memory using /dev/mem
        Usage: devmem ADDRESS [WIDTH [VALUE]]
        Read/write from physical address
            ADDRESS Address to act upon
            WIDTH   Width (8/16/...)
            VALUE   Data to be written
        $ devmem 0x00000000 8
        $ devmem 0x00000000 16
        $ devmem 0x00000000 32

    UART0 is mapped: 0x101f1000. The code that emulates the serial port inside QEMU implements a subset of the functionalities of the PL011 Prime Cell UART from ARM. UARTDR register that is used to transmit (when writing in the register) and receive (when reading) bytes; this register is placed at offset 0x0.
        $ devmem 0x101f1000 8 0x61

    On Ubuntu: $ sudo apt install devmem2

    How to dump BIOS data on to a file?
        $ grep ROM /proc/iomem
            which results in:
            000c0000-000c7fff : Video ROM
            000e2000-000e2fff : Adapter ROM
            000f0000-000fffff : System ROM
        Starting Address = 0xf0000 = 960KB
        Ending Address = 0xfffff = 1024KB
        $ sudo dd if=/dev/mem of=pcbios.bin bs=1k skip=960 count=64
        $ cat pcbios.bin | strings -n 20

    If i say echo "linux is the future" and strings /dev/mem | grep "linux is the future"? will it be present in RAM? Yes

32)
    System Management BIOS is a standard developed by DMTF (Distributed Management Task Force). The purpose of this standard is to allow the operating system to retrieve information about the PC. The specification addresses how motherboard and system vendors present management information about their products in a standard format by extending the BIOS interface on Intel Architecture systems. https://www.dmtf.org/sites/default/files/standards/documents/DSP0134_3.2.0.pdf

    It provides information such as:
        a) Make, Model
        b) Serial Number
        c) BIOS Version
        d) Processor
        e) Memory Configuration
        f) .....

    Locating SMBIOS Entry Point Table: On boot, the SMBIOS will put a table somewhere in memory. The SMBIOS Entry Point Table is located somewhere between the addresses 0xF0000 and 0xFFFFF, and must be on a 16-byte boundary. To find the specific location of the start of the table it is necessary to search that region of memory for the string "_SM_".

    Parsing the Entry Point Table: The entry point table has the following structure for SMBIOS 2 and below (the structure is different for SMBIOS 3):( ignore )
        struct SMBIOSEntryPoint {
            char EntryPointString[4];    //This is _SM_
            uchar Checksum;              //This value summed with all the values of the table, should be 0 (overflow)
            uchar Length;                //Length of the Entry Point Table. Since version 2.1 of SMBIOS, this is 0x1F
            uchar MajorVersion;          //Major Version of SMBIOS
            uchar MinorVersion;          //Minor Version of SMBIOS
            ushort MaxStructureSize;     //Maximum size of a SMBIOS Structure (we will se later)
            uchar EntryPointRevision;    //...
            char FormattedArea[5];       //...
            char EntryPointString2[5];   //This is _DMI_
            uchar Checksum2;             //Checksum for values from EntryPointString2 to the end of table
            ushort TableLength;          //Length of the Table containing all the structures
            uint TableAddress;       //Address of the Table
            ushort NumberOfStructures;   //Number of structures in the table
            uchar BCDRevision;           //Unused
         };

    TableAddress contains the address of the table that contains all the structures with information about the PC. All of the structures are located from [TableAddress] to [TableAddress + TableLength]. The structures are located directly adjacent to each other in memory, with a new structure beginning as soon as another one ends. Each structure is composed of a header, a structure specific table, and a string table. The format of the header is as follows.
         struct SMBIOSHeader {
            uchar Type;
            uchar Length;
            ushort Handle;
         };
    Located at TableAddress is a SMBIOS header. The value of Type indicates what element the structure contains information about. Length indicates the size of header + data table. The strings are not included in the length. Immediately after the end of the header is the data. At the end of the data table (Address + Length), the strings section starts. Each string is NULL terminated and is limited to 64 characters. Strings are referenced within tables by using an index into the string table. The first string begins immediately after the data, and the second string begins immediately after that, etc. The string section itself is terminated by two consecutive zero bytes. The next table begins immediately after the end of the string section.
        Code    Description
        0   BIOS Information
        1   System Information
        2   Mainboard Information
        3   Enclosure/Chasis Information
        4   Processor Information
        7   Cache Information
        9   System Slots Information
        16  Physical Memory Array
        17  Memory Device Information
        19  Memory Array Mapped Address
        20  Memory Device Mapped Address (optional as of SMBIOS 2.5)
        32  System Boot Information

    dmidecode: dmidecode is a tool for dumping a computer's SMBIOS/DMI table contents in a human-readable format. $dmidecode. When you run dmidecode, it will try to locate the DMI table. It will first try to read the DMI table from sysfs, if failed try to read from memory directly /dev/mem. If dmidecode  succeeds  in locating a valid DMI table, it will then parse this table and display it.

    biosdecode: biosdecode parses the BIOS memory and prints information about all structures (or entry points) it knows of. Currently known entry point types are:
        SMBIOS/DMI
        SYSID
        PNP
        ACPI
        BIOS32
        ...

33)
    Early PC's: Peripheral devices in the early PCs used fixed i/o-ports and fixed memory-addresses. Intel introduced a new bus standard PCI (Peripheral Component InterConnect) in the early 1990'S. To avoid contention among equipment vendors for fixed I/O Addresses (0x0000 - 0xFFFF), one of the goals of PCI wasto create a flexible scheme for allocating addresses that future peripherals could use.

    Address Space: A PCI device can have up to three address spaces:
        a) Configuration Space: i/o-ports 0x0CF8-0x0CFF dedicated to accessing PCI Configuration Space (Required)
        b) I/O Space (Optional)
        c) Memory Space (Optional)
    Note: Every PCI device must implement the PCI configuration register dictated by the PCI specification. Otherwise, the device will not be regarded as valid PCI device. Each PCI device is identified by a
        a) Bus Number
        b) Device Number
        c) Function Number
    The PCI Specification permits a single system to host up to 256 buses. Each bus hosts up to 32 Devices. Each Device can have multi functionality. There are 8 possible funtions per device.

    PCI Configuration Space: Each PCI Device has a set of registers referred to as configuration space. Size of the Configuration space is 256 bytes.
        First 64 bytes (0x00 - 0x3f) are standardized.
        Next 192 bytes (0x40 - 0xff) are vendor specific.
            Registers 0x00, 0x01 are defined by PCI spec as vendor ID (16-Bit)
            Registers 0x02, 0x03 are defined by PCI spec as product ID (16-Bit)
        Vendor ID identifies the manufacturer of the device. Allocated by the PCI SIG to ensure each is unique .
        Device ID identifies the particular device, set by the vendor

    How do you access all registers present in the Configuration Space? Accessing these registers is like accessing RTC(CMOS) Memory.
        PCI Index Port  0xCF8h
        PCI Data Port   0xCFCh

        PCI Index Port (0xCF8)
        ============================
            31                                   0
            ---------------------------------------------------------------------------
            | |Reserved |Bus Number |Device Number|Function Number|Register Number|0|0|
            ---------------------------------------------------------------------------
                                  B      D         F                Offset
            Bit 31 when set, all reads and writes to CONFIG_DATA are PCI Configuration transactions
            Bits 30:24 are read-only and must return 0 when read
            Bits 23:16 select a specific Bus in the system (up to 256 buses)
            Bits 15:11 specify a Device on the given Bus (up to 32 devices)
            Bits 10:8 Specify the function of a device (up to 8 devices)
            Bits 7:0 Select an offset within the Configuration Space (256 bytes)
        Addresses are often given in B/D/F, Offset notation (also written as B:D:F, Offset)

    PCI Data Port (0xCFCh): Read and Write to 0xCFCh with Bit 31 enabled in 0xCF8 results in PCI configuration transaction. If the Bit 31 is not enabled, according to PCI Spec, transaction is forwarded out as Port I/O.

    How do you find out Bus, Device and Function of a PCI Device?
    $ lspci
    Each line starts with the PCI bus address formatted as bus:slot.function

    What happens if we try to read a PCI device(B/D/F) which doesn't exist? When a configuration access attempts to select a device that does not exist, the host bridge will complete the access without error, dropping all data on writes and returning all ones on reads.

    Base Address Registers (BAR): Base Address Registers holds the memory addresses used by the device. PCI Configuration Registers provides space for up to 6 BARs. Each BAR is 32-bits wide to support 32-bit address space locations. Concatenating two 32-bit BARs provides 64-bit addressing capability. Each region consists of either memory or I/O locations. So, RC can access exposed devicxe area using BAR configurations and not all from device memory. Device can access all RC memeory area, but those can be protected by SMMU/IOMMU to allow only certain region. RC can indicate area using doorbell. Doorbell : RC to device. MSI/MSI-X: device to RC. RC has ECAM region to map.

    How do you determine the amount of address space needed by a PCI device? A Base Address is half the information that’s needed. We need to get the size of the device.
        1. you must save the original value of the BAR
        2. write a value of all 1's to the register
        3. then read it back
        4. restore the original value.
    The amount of memory can then be determined by masking the information bits, performing a bitwise NOT ('~' in C), and incrementing the value by 1

    PCI Express: The PCI Express bus extends the Configuration Space from 256 bytes to 4096 bytes. This extended configuration space *cannot* be accessed using the legacy PCI method (through ports 0xCF8 and 0xCFC). The enhanced configuration mechanism makes use of memory mapped address space range/s to access PCI configuration space. On x86 and x64 platforms, the address of each memory area is determined by the ACPI 'MCFG' table
    [ ls /sys/firmware/acpi/tables/ ]
    [ hexdump -C /sys/firmware/acpi/tables/MCFG ]

    More reading: https://www.programmersought.com/article/78017399476/, https://ctf.re/windows/kernel/pcie/tutorial/2023/02/14/pcie-part-1/, https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/sect-pci_devices-pci_passthrough etc.

34)
    sysfs: sysfs is a virtual file system. It provides information about
        a) various kernel subsystems
        b) hardware devices
        c) and associated device drivers
    from the kernel's device model to user space through virtual files.

    CONFIG_SYSFS: By default syfs is compiled in the Linux Kernel. It depends on CONFIG_SYSFS being enabled in the Linux kernel configuration. $ cat /boot/config-`uname -r` | grep CONFIG_SYSFS

    Mounting Sysfs: $ mount | grep sysfs. If sysfs is not already mounted, you can mount it with the below command $ mount -t sysfs sysfs /sys

    Source Code: Present in fs/sysfs directory
        Documentation/filesystems/sysfs.txt

    sysfs: The root of the sysfs contains at least 10 directories
        - block: Contains one directory for each of the registered block devices on the system. Each of those directories, in turn, contains any partitions on the block device. Connect a USB drive and you should see, directory should be created in /sys/block. Disconnect USB drive, and directory should be removed.
        - bus: provides a view of the system buses. Inside each of these directories are two subdirectories
            a) devices:
                Correspond to devices discovered on this bus.
            b) drivers:
                Device drivers loaded for this bus.
            Connect a USB drive, and you should see directory in /sys/bus/usb/devices. Disconnect USB drive, and directory should be removed.
        - class: contains a view of the devices on the system organized by high-level function. Examples: input, network, backlight, sound etc.
        - dev: Contains two sub directories:
                - block
                - char
            Inside each of the sub directories are symbolic links of the form major-id:minor-id. Load a character driver and see the behavior is /sys/dev/char. try adding and removing char driver and check that entry is coming and getting deleted after unloading.
        - devices: Contains information about all devices represented in the device model. Most of the data present in other directories is symlink to this directory.
        - firmware: contains interfaces for viewing and manipulating firmware-specific objects and attributes. Eg. ACPI, EDD, EFI. If you booted with UEFI enabled, you will be seeing a /sys/firmware/efi folder. check by running ls into it. We will see efi, dmi, acpi etc.
        - fs: contains a view of registered filesystems.
        - kernel: contains various files and subdirectories that provide information about the running kernel
        - module: contains one subdirectory for each module that is loaded into the kernel. name of each directory is the name of the module. /sys/module/<modulename>/parameters: each file containing the value of the corresponding parameter.
        - power: Contains information about power management sleep states
        - hypervisor:

    Linux Device Model: Purpose: To provide information like:
        1. what devices are in the system
        2. how they are in terms of power management
        3. what bus they are attached to
        4. what drivers they have
    The device model provides a single mechanism for representing devices and describing their topology in the system.

    Benefits:
        --> The capability to enumerate all the devices in the system, view their status, and see to what bus they attach
        --> The capability to generate a complete and valid tree of the entire device structure of the system, including all buses and interconnections
        --> The capability to link devices to their drivers and vice versa
        --> The capability to categorize devices by their class, such as input device, without the need to understand the physical device topology
        --> The capability to walk the tree of devices from the leaves up to the root, powering down devices in the correct order

    kobject (Kernel Object): Linux Device Model provides a number of structures to ensure the interaction between a hardware device and a device driver. kobject is the heart of the device model. kobject provides various functionalities:
        ---> Reference Counting
        ---> Parent Pointer which enables creation of hierarchy of objects
        struct kobject {
                const char              *name;
                struct list_head        entry;
                struct kobject          *parent;
                struct kset             *kset;
                struct kobj_type        *ktype;
                struct kernfs_node      *sd; /* sysfs directory entry */
                struct kref             kref;
        #ifdef CONFIG_DEBUG_KOBJECT_RELEASE
                struct delayed_work     release;
        #endif
                unsigned int state_initialized:1;
                unsigned int state_in_sysfs:1;
                unsigned int state_add_uevent_sent:1;
                unsigned int state_remove_uevent_sent:1;
                unsigned int uevent_suppress:1;
        };
            name --> Points to the name of this kobject, will show in sysfs
            parent --> Points to this kobject's parent, this makes object hierarchical structure
            kref --> Provides reference counting
            ktype,kset --> Used to group kobjects
    Kobjects are usually embedded in other structures and are generally not interesting on their own. For example, the cdev structure has the following definition <linux/cdev.h>
        /* cdev structure - object representing a character device */
        struct cdev {
            struct kobject kobj;
            struct module *owner;
            const struct file_operations *ops;
            struct list_head list;
            dev_t dev;
            unsigned int count;
        };

    kobject Operations. kobject_init:
        void kobject_init(struct kobject *kobj, struct kobj_type *ktype);

    Initialize a kobject structure.
        kobj --> kobject to initialize. Before calling this function, the kobject must be zeroed.
            struct kobject *kobj;
            kobj = kmalloc(sizeof (*kobj), GFP_KERNEL);
            if (!kobj)
                return -ENOMEM;
            memset(kobj, 0, sizeof (*kobj));
            kobj->kset = my_kset;
            kobject_init(kobj, my_ktype);

    kobject_create: The above multistep effort is handled by kobject_create().
        struct kobject * kobject_create(void);
        struct kobject *kobj;
        kobj = kobject_create();
        if (!kobj)
            return –ENOMEM;

    Adding and Removing kobjects from sysfs: Initialized kobjects are not automatically exported to sysfs. To represent a kobject to sysfs,you use kobject_add():
        int kobject_add(struct kobject *kobj, struct kobject *parent, const char *fmt, ...);

    A given kobject’s location in sysfs depends on the kobject’s location in the object hierarchy. If the kobject’s parent pointer is set, the kobject maps to a subdirectory in sysfs inside its parent. If the parent pointer is not set, the kobject maps to a subdirectory inside kset->kobj. If neither the parent nor the kset fields are set in the given kobject, the kobject is assumed to have no parent and maps to a root-level directory in sysfs. Regardless, the name of the directory representing the kobject in sysfs is given by fmt, which accepts a printf()-style format string.

    kobject_create_and_add(): combines the work of kobject_create() and kobject_add() into one function.
        struct kobject * kobject_create_and_add(const char *name, struct kobject *parent);

    kobject_del(): Removing a kobject’s sysfs representation is done via kobject_del():
        void kobject_del(struct kobject *kobj);

    Implementation: lib/kobject.c
    using [ grep ] for kobject_create_and_add will tell which file is creating particular sysfs entry.

    Adding Files to sysfs: kobjects map to directories, what about files? Attributes map kernel data to files in sysfs.
        Header File: <linux/sysfs.h>
        struct attribute {
                const char              *name;
                umode_t                 mode;
        };
        name --> will be the file name of the resulting file in sysfs
        mode --> permissions of the file in sysfs
    A bare attribute contains no means to read or write the value of the attribute.
        struct kobj_attribute {
                struct attribute attr;
                ssize_t (*show)(struct kobject *kobj, struct kobj_attribute *attr,
                                char *buf);
                ssize_t (*store)(struct kobject *kobj, struct kobj_attribute *attr,
                                 const char *buf, size_t count);
        };
    When an attribute is opened, a PAGE_SIZE buffer is allocated for transferring the data between the kernel and userspace.
        show(): Invoked when the sysfs entry is read from user space. It should copy the value of attribute given by attr into buffer(third argument). size of buffer is PAGE_SIZE
            Return Value:
                    Success, Number of bytes written
                    Negative error code on Failure
        store(): Read the size bytes from the buffer into the variable represented by attribute attr. size of buffer is PAGE_SIZE or less
            Return Value:
                    Success, Number of bytes read
                    Negative error code on Failure

    Creating New Attributes:
        int sysfs_create_file(struct kobject *kobj, const struct attribute *attr);
        This function associates the attribute structure pointed at by attr with the kobject pointed at by kobj. Before it is invoked, the given attribute should be filled out.
        Return Value:
            Success - 0
            Failure - Negative error code

        void sysfs_remove_file(struct kobject *kobj, const struct attribute *attr);

    Attribute Groups: The attribute group interface is a simplified interface for easily adding and removing a set of attributes with a single call.
        struct attribute_group {
                const char              *name;
                umode_t                 (*is_visible)(struct kobject *,
                                                      struct attribute *, int);
                umode_t                 (*is_bin_visible)(struct kobject *,
                                                          struct bin_attribute *, int);
                struct attribute        **attrs;
                struct bin_attribute    **bin_attrs;
        };

    An attribute group is simply an array of attributes to be added to an object, as represented by the attrs field. Why they were created?
            - To make it easier to keep track of errors when registering multiple attributes at one time, and
            - To make it more compelling to clean up all attributes that a piece of code may create for an object
        static int sysfs_create_group(struct kobject *kobj, const struct attribute_group *grp)

    Object Relationships: Objects throughout the kernel are referenced by multiple subsystems. When a block device is registered, a symbolic link is created to the device’s directory in the physical hierarchy. A symbolic link is also created in the device’s directory that points to the corresponding directory under the block directory. go in /sys/block and execute [ ls -la ] and check all are symbolic links.
            int sysfs_create_link(struct kobject *kobj, struct kobject *target, char *name);
    This function creates a link named name in the directory mapped from kobj to the directory mapped from target
        Return Value:
                Success - 0
                Failure - Negative Error Code

    void sysfs_remove_link(struct kobject *kobj, char *name);

    Binary Attributes: Convention of sysfs is to have a single value in human readable text format for each attribute. Other kind of attributes is binary attributes for handling larger chunks of binary data. Examples: Passing Firmware from user space program
          PCI Configuration Space Registers
                struct bin_attribute {
                        struct attribute        attr;
                        size_t                  size;
                        void                    *private;
                        ssize_t (*read)(struct file *, struct kobject *, struct bin_attribute *,
                                        char *, loff_t, size_t);
                        ssize_t (*write)(struct file *, struct kobject *, struct bin_attribute *,
                                         char *, loff_t, size_t);
                        int (*mmap)(struct file *, struct kobject *, struct bin_attribute *attr,
                                    struct vm_area_struct *vma);
                };
                attr -> name and permissions
                size -> Maximum size of the binary attribute
                read,write,mmap -> Work similar to char driver

    Creating a binary attribute:
        int sysfs_create_bin_file(struct kobject *kobj, struct bin_attribute *attr);

    Removing a binary attribute:
        int sysfs_remove_bin_file(struct kobject *kobj, struct bin_attribute *attr);

35)
    debugfs: debugfs helps kernel developers export large amounts of debug data into user-space.
        procfs  - Used for information about process
        sysfs   - Has strict one-value-per-file rule
        debugfs - no rules, any information can be placed by developers

    CONFIG_DEBUG_FS: To compile a Linux kernel with the debugfs facility, the CONFIG_DEBUG_FS option must be set to yes. $ cat /boot/config-`uname -r` | grep CONFIG_DEBUG_FS

    Mounting debugfs: $ mount | grep debugfs. If debugfs is not already mounted, you can mount it with the below command, $ mount -t debugfs none /sys/kernel/debug

    Note:
        1. Debugfs directory is accessible to only root user by default, can be changed with uid/gid/mode options while mounting.
        2. Can be used by only GPL Modules

    Why can't i use printk instead of putting data into debugfs ? Sometimes putting in a few printk() calls is sufficient, but, often, that is not the best way to go. The debugging information may only be useful occasionally, but the printed output clogs up the logs all the time. Using printk() also does not help if the developer wishes to be able to change values from user space.

    Creating a directory in debugfs:
        Header File: <linux/debugfs.h>
        struct dentry *debugfs_create_dir(const char *name, struct dentry *parent);
            If parent is NULL, the directory will be created in the debugfs root.
        Return Value:
            Success - struct dentry * which can be used to create files in the directory
            Failure - ERR_PTR(-ERROR)
                ERR_PTR(-ENODEV) is returned, when kernel has been built without debugfs

    To remove directory:
        void debugfs_remove(struct dentry *dentry);

    Create a file within debugfs:
        struct dentry *debugfs_create_file(const char *name, umode_t mode,
                               struct dentry *parent, void *data,
                               const struct file_operations *fops);
        name -> Name of the file to create
        mode -> access permissions the file should have
        parent -> the directory which should hold the file
        data -> will be stored in the i_private field of the resulting inode structure
        fops -> set of file operations which implement the file's behavior
    At a minimum, the read() and/or write() operations should be provided; others can be included as needed.
        Return Value:
            Success: dentry pointer to the created file
            Failure: ERR_PTR(-ERROR)

    What happens if i forget to delete call debugfs_remove? There is no automatic cleanup of any directories created in debugfs. If a module is unloaded without explicitly removing debugfs entries, the result will be a lot of stale pointers.

    debugfs_remove_recursive:
       void debugfs_remove_recursive(struct dentry *dentry);

    If you pass a pointer for the dentry corresponding to the top-level directory, the entire hierarchy below that directory will be removed.

    Debugfs code provides a number of helper functions for simple situations. If you need to write to and read from a single value, you can use this to create an unsigned 8-bit value:
        struct dentry *debugfs_create_u8(const char *name, mode_t mode, struct dentry *parent, u8 *value);
            name - a pointer to a string containing the name of the file to create.
            mode - the permission that the file should have
            parent - a pointer to the parent dentry for this file
            value  - a pointer to a variable that needs to be read and written to.

    A few other helper functions to create files with single integer values are:
        struct dentry *debugfs_create_u16(const char *name, umode_t mode,
                          struct dentry *parent, u16 *value);
        struct dentry *debugfs_create_u32(const char *name, umode_t mode,
                          struct dentry *parent, u32 *value);
        struct dentry *debugfs_create_u64(const char *name, umode_t mode,
                          struct dentry *parent, u64 *value);
    Implementation: fs/debugfs/file.c

    How to provide a read only file? By appropriately setting the mode bits.

    The values in the previous files are in decimal; if hexadecimal is more appropriate, the following functions can be used instead:
        struct dentry *debugfs_create_x8(const char *name, umode_t mode,
                         struct dentry *parent, u8 *value);
        struct dentry *debugfs_create_x16(const char *name, umode_t mode,
                          struct dentry *parent, u16 *value);
        struct dentry *debugfs_create_x32(const char *name, umode_t mode,
                          struct dentry *parent, u32 *value);
        struct dentry *debugfs_create_x64(const char *name, umode_t mode,
                          struct dentry *parent, u64 *value);

    Boolean Values: Boolean values can be placed in debugfs with:
        struct dentry *debugfs_create_bool(const char *name, umode_t mode,
                               struct dentry *parent, bool *value);
            Read: Y (for non-zero values) or N, followed by a newline
            Write: Upper or lower case values, or 1 or 0

    Pointers and Error Values: Many internal kernel functions return a pointer value to the caller. Many of those functions can also fail. In most cases, failure is indicated by returning a NULL pointer value. This technique works, but it is unable to communicate the exact nature of the problem. Some interfaces really need to return an actual error code so that the caller can make the right decision based on what actually went wrong. A number of kernel interfaces return this information by encoding the error code in a pointer value. A function returning a pointer type can return an error value with:
        void *ERR_PTR(long error);
            where error is the usual negative error code.
        Header File: <linux/err.h>
        The caller can use IS_ERR to test whether a returned pointer is an error code or not:

36)
    Minimal Linux with QEMU/Busybox: QEMU (short for Quick Emulator) is a free and open-source  hypervisor that performs hardware virtualization.

    Busybox : combines tiny versions of many common UNIX utilities into a single small executable. BusyBox provides a fairly complete environment for any small or embedded system. Note: The utilities in BusyBox generally have fewer options than their full-featured GNU cousins. How it is useful? Building a minimal Linux kernel and booting it on an emulator allows developers to quickly build additional Linux kernel features.

    initrd vs initramfs: initrd (initial ramdisk) is a scheme for loading a temporary root file system into memory, which may be used as part of the Linux startup process. initrd and initramfs refer to two different methods of achieving this. Both are commonly used to make preparations before the real root file system can be mounted. An image of this initial root file system (along with the kernel image) must be stored somewhere accessible by the Linux bootloader or the boot firmware of the computer. The bootloader will
        load the kernel,
        initial root file system image into memory and
        then start the kernel
    At the end of its boot sequence, the kernel tries to determine the format of the image from its first few blocks of data, which can lead either to the initrd or initramfs scheme. In the initramfs scheme (available since the Linux kernel 2.6.13), the image is a cpio archive (optionally compressed).

    cpio - copy in and out

    The archive is unpacked by the kernel into a special instance of a tmpfs that becomes the initial root file system the kernel executes /init as its first process that is not expected to exit.

    Create Initramfs:
        Step1 : Create initramfs directory
            $ mkdir initramfs; $ cd initramfs
        Step2: Create directory structure
            $ mkdir -pv {bin,sbin,etc,proc,sys,usr/{bin,sbin}}
        Step3: Copy busbox install directory
            $ cp -av busybox-1.31.0/build/_install/* initramfs/
            -av: a means -dR --preserve=all, v means explain what is being done
            -dR --preserve=all:
            d means --no-dereference --preserve=links
            R means copy directories recursively
        Step4: Create init and make it executable
            $ vi initramfs/init
            #!/bin/sh
            mount -t proc none /proc
            mount -t sysfs none /sys
            mount -t debugfs none /sys/kernel/debug
            echo -e "\nBoot took $(cut -d' ' -f1 /proc/uptime) seconds\n"
            exec /bin/sh
            $ chmod +x initramfs/init
        Step5: Create initramfs
            $ cd initramfs
            $ find . -print0 | cpio --null -ov --format=newc | gzip -9 > ../initramfs.cpio.gz

    Boot the kernel/initramfs with QEMU:
        $ qemu-system-x86_64 -kernel linux_build/arch/x86/boot/bzImage -initrd initramfs.cpio.gz -m 512
            kernel option specifies the kernel image
            initrd option specifies the initramfs
            m option to specify the memory size (optional)

    check boot time. It is very less. also check /proc/meminfo and find that it is using very less memory.

37)
    Flashing Raspbian Image on Raspberry Pi3:
        - Copy the kernel and Device Tree blobs onto the SD card
        - Copy the modules into a folder
            $ sudo mount /dev/sdb2 /mnt /* mount root partition */ /* check using [ ls /mnt ] */
            $ sudo make ARCH=arm CROSS_COMPILE=arm-linux-gnueabihf- INSTALL_MOD_PATH=/mnt modules_install
        - Finally Plug the Card and boot it

    Cross Compiling Linux Kernel Module:
        $ make ARCH=arm CROSS_COMPILE=arm-linux-gnueabihf- -C ~/raspberrypi/linux/ M=${PWD} modules
        $ make ARCH=arm CROSS_COMPILE=arm-linux-gnueabihf- -C ~/raspberrypi/linux/ M=${PWD} clean
    then copy generated .ko using ssh and then install it using [ sudo insmod ]

38)
    GPIO Registers on Raspberry Pi3: $ sudo cat /proc/iomem | grep gpio

39)
    What is Interrupt? Interrupt is an input signal to the processor, sent by the hardware peripherals when they need processor attention.

    What is the purpose of Interrupts? we want to perform an action with an incoming packet from the network card as soon as the packet arrives. If you don't want to continuously ask the network card «Has my packet arrived?» and waste your processor time, you can use external hardware interrupt IRQ. The interrupt line from a device should be connected to the INTR line of the CPU, and after each packet is received, the network card will make a signal over this line. The CPU will sense this signal and know that the network card has information for it. Only after that the CPU will read the incoming packet.

    Types of Interrupt:
        1. Hardware/Asynchronous:
            --> Generated by hardware devices
            --> Occurs at arbitrary times(asynchronously) with respect to clock signals
            --> Examples: Pressing a key on the keyboard, Mouse Movement, Timer fired,
                      Network cards report the arrival of packet with interrupt.
        2. Software/Synchronous:
            --> Generated by executing instructions
            --> Occurs synchronously with respect to processor clock
            --> Also called as exceptions/traps
            --> Examples: Divide-By-Zero, system call, page fault

    Exceptions: Exceptions are classified as faults, traps and abort, depending on the way they are reported and whether the instruction that caused the exception can be restarted without loss of program. traps increment the instruction pointer, faults do not, and aborts 'explode'.
        - Faults: These can be corrected and the program may continue as if nothing happened. Eg. page fault
        - Traps: Traps are reported immediately after the execution of the trapping instruction. Eg. int instruction, updating instruction pointer
        - Aborts: Some severe unrecoverable error. Eg. hardware failure

    General Protection Fault: A General Protection Fault may occur for various reasons. The most common are:
        Segment error (privilege, type, limit, read/write rights).
        Executing a privileged instruction while CPL != 0.
        Writing a 1 in a reserved register field.
        Referencing or accessing a null-descriptor.
        Trying to access an unimplemented register (like: mov cr6, eax)
        The saved instruction pointer points to the instruction which caused the exception.

    How debuggers work?
    To implement breakpoints on the x86 architecture, software interrupts (also known as "traps") are used. Breakpoints are implemented on the CPU by a special trap called int 3. int is x86 jargon for "trap instruction" - a call to a predefined interrupt handler. x86 supports the int instruction with a 8-bit operand specifying the number of the interrupt that occurred. Run the program in gdb, compile with debugging information.

    Triggering Methods: Each interrupt signal input is designed to be triggered by either a logical signal level or a particular signal edge (level transition).
        - Level Triggered:
            Interrupt is requested by holding the interrupt signal at its particular (high or low) active logic level. Level triggered interrupts happen as long as the interrupt line is held at active level. As long as the line is active, you get interrupt, when you serve the interrupt and return, if the interrupt line is still active, you get the interrupt again immediately. Level-triggered inputs allow multiple devices to share a common interrupt signal via wired-OR connections.
        - Edge Triggered:
            Interrupt is requested by a level transition on the interrupt line.
                Falling Edge (high to low)
                Rising Edge  (Low to High)
    These interrupts are issued per transition and not repeated. e.g. in networking when the packet queue goes from empty to non-empty. This makes it critical to never miss an edge triggered interrupt, because failure to handle one interrupt may result in no further interrupts from happening.

    Masking: Processors typically have an internal interrupt mask register. This allows selective enabling and disabling of hardware interrupts. Each interrupt signal is associated with a bit in the mask register.

    When the interrupt is disabled, the associated interrupt signal will be ignored by the processor.
        - Maskable Interrupts: Interrupts which can be enabled/disabled
        - NonMaskable Interrupts: Interrupts which cannot be disabled. Example: NMI, timeout signal from watchdog timer

    The 8086 processor has two hardware interrupt signals
        – NMI non-maskable interrupt
        – INTR Interrupt request (maskable interrupt)

    How to support more than two interrupts? It would be very unproductive to make a ton of INTR pins on the CPU for all of them. To solve this problem a special chip was invented — an interrupt controller. System software, such as the BIOS or operating system, is responsible for programming the interrupt router.

    Get info about NVIC & GIC. Arm specific.

    APIC (Advanced PIC): The PIC method only works for a single processor systems. PIC can only send interrupts to one CPU, and in a multiprocessor system it is desired to load CPUs in a balanced way. The solution to this problem was the new APIC interface (Advanced PIC). Comprises of two components:
        1. IO-APIC - Interfaces with Devices
        2. LAPIC   - Interfaces with CPU

    LAPIC:
        Each processor in a multiprocessor system consists of a one LAPIC. Responsible for:
            - receiving various interrupt requests and delivering them to the processor
            - handling prioritization of interrupts
            - sending interrupts to other processors (known as inter processor interrupts or IPIs)
        LAPIC can be connected directly to I/O devices via local interrupt inputs (timer, thermal sensor) or through IOAPIC via external interrupt inputs. LAPIC can generate interrupts due to interrupt requests received from various sources.
    I/O APIC:
        connects to the devices to allow device interrupt requests to be routed to LAPIC(s). There can be one or more IOAPIC in the system. IOAPIC receives interrupt requests from the devices and sends them to LAPIC(s) based upon the redirection table entries (RTE) programmed in the IOAPIC.

    Most of the information in cpuid is reported by the kernel in cooked form either in /proc/cpuinfo. $ cat /proc/cpuinfo | grep -i apicid.  apicid: A unique ID given to each logical processor upon startup

    What happens when there is an interrupt:
        Device Asserts IRQ of I/O APIC, I/O APIC transfer interrupt to LAPIC, LAPIC asserts CPU interrupts, after current instruction completes CPU senses interrupt line and obtains IRQ number from LAPIC, jumps to interrupt handler.

    How does the hardware finds the interrupt handler? Interrupt Vector: On x86 Each interrupt or exception is identified by a number between 0 and 255. Intel calls this number a vector. The interrupt vector is used by the interrupt-handling mechanism to locate the system-software service routine assigned to the exception or interrupt. Up to 256 unique interrupt vectors are available in x86. The number of interrupt vectors or entry points supported by a CPU differs based on the CPU architecture. The first 32 vectors are reserved for predefined exception and interrupt conditions. Look into arch/x86/include/asm/traps.h.

    Interrupt Descriptor Table: The IDT is a linear table of 256 entries which associates an interrupt handler with each interrupt vector. When an interrupt is fired, the CPU looks at the IDT table, and finds what method needs to be called. Each descriptor is of size 8 bytes (on x86) and 16 bytes (on x86_64). During early boot, the architecture-specific branch of the kernel code sets up the IDT in memory and programs the IDTR register (special x86 register)of the processor with the physical start address and length of the IDT.

    Interrupt Handling in Linux Kernel:
        1. Whenever an interrupt occurs, assembly instructions in linux kernel are executed, which
            locates relevant vector descriptor by multiplying reported vector number by size of vector number(8/16)
            and adding the result to the base address of IDT.
        2. common_interrupt: arch/x86/entry/entry_64.S:
            a. saves the context of the running process
            b. This includes instruction pointer (IP), stack pointer and other registers needed to resume the process again
            c. This context is usually saved on the stack.
            d. Then the context is changed to interrupt stack.
        3. Finally it arrives at do_IRQ(). do_IRQ() is the common function for all hardware interrupts
            arch/x86/kernel/irq.c
        4. Finds IRQ number in saved %EAX register
        5. Calls handle_irq which will finally call our registered interrupt handler.

    /proc/interrupts: Contains statistics related to interrupts on the system
               CPU0       CPU1       CPU2       CPU3       CPU4       CPU5
       0:          2          0          0          0          0          0   IO-APIC    2-edge      timer
       1:        458          0          0          0       6154          0   IO-APIC    1-edge      i8042
       8:          0          0          0          0          0          1   IO-APIC    8-edge      rtc0
       9:          0          0          0          0          0          0   IO-APIC    9-fasteoi   acpi
      12:          0          0          0      22185          0          0   IO-APIC   12-edge      i8042
      14:          0          0          0          0          0          0   IO-APIC   14-edge      ata_piix
      15:          0          0          0          0          0          0   IO-APIC   15-edge      ata_piix
      16:          0      19541          0          0          0          0   IO-APIC   16-fasteoi   vmwgfx, snd_ens1371
      17:       9825          0      47519          0          0          0   IO-APIC   17-fasteoi   ehci_hcd:usb1, i

        Column 1:  IRQ number
                    the file shows only interrupts corresponding to installed handlers
        Column 2:  counter of the number of interrupts received.
                   A column is present for each processor on the system
        Column 3/4: Type of the interrupt and device that handles the interrupt.
                    For x86.
                    XT-PIC — This is the old AT computer interrupts. 8259
                    IO-APIC
        Column 5:  device associated with this interrupt
                    This name is supplied by the devname parameter to request_irq(),

    Difference between  IO-APIC-fasteoi and IO-APIC-edge? The difference lies in the way the interrupts are triggered. The -edge interrupt are edge triggered. The -fasteoi interrupts are level interrupts that are triggered until the interrupt event is acknowledged in the programmable interrupt controller (PIC). The EOI stands for End Of Interrupt.

    Watch Interrupts: To see the interrupts occurring on your system, run the command:
        # watch -n1 "cat /proc/interrupts"
    The watch command executes another command periodically, in this case "cat /proc/interrupts". The -n1 option tells watch to execute the command every second. -d option of watch highlight  the  differences  between successive updates

    # watch -n 0.1 -d 'cat /proc/interrupts'. --no-title / -t option of watch Turn off the header showing the interval, command, and current time at the top of the display, as well as the following blank line.

    # watch -n 0.1 -d --no-title 'cat /proc/interrupts'

    Interrupt Handlers: Interrupt handlers are the responsibility of the driver managing the hardware. If the device uses interrupts, then driver must register one interrupt handler. Registering an interrupt handler:
        Header File: <linux/interrupt.h>

        int request_irq(unsigned int irq,
                irq_handler_t handler,
                unsigned long flags,
                const char *name,
                void *dev);
        Parameters:
            irq     --> The interrupt number being requested
                        For some devices,for example legacy PC devices such as the system timer or keyboard, this value is typically hard-coded.
                        For most other devices, it is probed or otherwise determined programmatically and dynamically.
            handler   --> function pointer to the actual interrupt handler that services this interrupt.
                          invoked whenever the operating system receives the interrupt
                          typedef irqreturn_t (*irq_handler_t)(int, void *);
            flags     --> bitmask of options related to interrupt management.
            name      --> Name to be displayed in /proc/interrupts
            dev       --> Used for shared Interrupt Lines
        Return Value:
            Success  -->    Returns Zero
            Failure  -->    Non-Zero Value

        void free_irq(unsigned int irq_no, void *dev);
            When the interrupt is released, using the free_irq() function, you must send the same pointer value (dev) along with the same interrupt number (irq_no).

        Kernel registers all interrupt handlers with interrupt and one by one calls all and then finds if anyone returns done.

40)
    Return Value of Interrupt Handlers: Interrupt handlers return an irqreturn_t value.
        IRQ_NONE            interrupt was not from this device or was not handled
        IRQ_HANDLED         interrupt was handled by this device

    Interrupt Flags: The third parameter, flags of request_irq can be either zero or a bit mask of one or more flags defined in <linux/interrupt.h>.

    IRQF_SHARED informs the kernel that the interrupt can be shared with other devices. If this flag is not set, then if there is already a handler associated with the requested interrupt, the request for interrupt will fail.

    On success it returns 0

    Interrupt Flags:
        IRQF_TIMER          This flag specifies that this handler processes interrupts for the system timer.
        IRQF_PERCPU         Interrupt is per cpu
        IRQF_PROBE_SHARED   set by callers when they expect sharing mismatches to occur

    Interrupt Flags:
        IRQF NOBALANCING: Flag to exclude this interrupt from IRQ balancing. The purpose of IRQ balancing is to distribute hardware interrupts across processors on a multiprocessor system in order to increase performance. Setting this flag forbids to set any CPU affinity for the requested interrupt handler.

    /proc/irq: Linux has gained the ability to assign certain IRQs to specific processors (or groups of processors). This is known as SMP IRQ affinity. The interrupt affinity value for a particular IRQ number is stored in the associated /proc/irq/IRQ_NUMBER/smp_affinity file, which can be viewed and modified by the root user. The value stored in this file is a hexadecimal bit-mask representing all CPU cores in the system /proc/irq/irq_number/smp_affinity_list contains cpu list. Example:
        $ cat /proc/irq/1/smp_affinity

    00000000,00000000,00000000,00000038
    This mean keyboard interrupt can occur in CPU 3, 4, 5. Setting this value to 1, as follows, means that only CPU 0 can service this interrupt:

    # echo 1 >/proc/irq/1/smp_affinity
    # cat /proc/irq/1/smp_affinity
    1

    Commas can be used to delimit smp_affinity values for discrete 32-bit groups. This is required on systems with more than 32 cores. /proc/irq/default_smp_affinity specifies default affinity mask that applies to all non-active IRQs. Once IRQ is allocated/activated its affinity bitmask will be set to the default mask

    How can a device driver know if the interrupt handler was activated by an interrupt generated by the device it manages? All devices that offer interrupt support have a status register that can be read in the handling routine to see if the interrupt was or was not generated by the device. Example: For 8250 serial port, this status register is IIR - Interrupt Information Register.

    we should not pass null as device id. this, example will fail to reserve device.

41)
    Why do we need to disable interrupts? Disabling interrupts, you can guarantee that an interrupt handler will not preempt your current code. Disabling interrupts also disables kernel preemption.

    Note: Disabling kernel preemption doesnot provide protection from concurrent access from another processor. Use locks to prevent another processor from accessing shared data simultaneously.

    Enable/Disable Interrupt:
        Header File: <linux/irqflags.h>
            local_irq_disable(); //disable all interrupts on the current processor
            local_irq_enable(); //Enables all interrupts on the current processor
        On x86, local_irq_disable() is a simple cli, and local_irq_enable() is a simple sti instruction. cli and sti are the assembly calls to clear and set the allow interrupts flag, respectively.

    Enabling/Disabling Interrupts:
        local_irq_disable() routine is dangerous if some of interrupts were already disabled prior to its invocation. The corresponding call to local_irq_enable() unconditionally enables interrupts, despite the fact that they were off to begin with.

        local_irq_save(flags); saves the interrupt state ( which interrupts were already disabled/enabled ) on flags and disables interrupt on that processor.
            local_irq_restore(flags); restores the previous interrupt state and enables interrupt on that processor.

    Disabling a specific interrupt line: Disabling a specific interrupt line is also called as masking out an interrupt line. Example: you might want to disable delivery of a device’s interrupts before manipulating its state.
        - void disable_irq(unsigned int irq); //Disables a given interrupt line in interrupt controller. // this disables delivery of the given interrupt to all processors in system
        - void enable_irq(unsigned int irq); // this enables delivery of the given interrupt to all processors in system

    Note: disable_irq does not return until any executing handler completes.
        callers are assured that
            a) new interrupts will not be delivered on the given line,
            b) any already executing handlers have exited

    there is no API in Linux, to disable all interrupt on all processors in SMP. Only specific interrupt line on specific processor can be disabled.

    void disable_irq_nosync(unsigned int irq); The function disable_irq_nosync() does not wait for current handlers to complete.

    void synchronize_irq(unsigned int irq); The function synchronize_irq() waits for a specific interrupt handler to exit, if it is executing, before returning.

    synchronize_irq() spins until no interrupt handler is running for the given IRQ.

    What happens if i call disable_irq twice and enable_irq once? Calls to these functions nest.

    For each call to disable_irq() or disable_irq_nosync() on a given interrupt line, a corresponding call to enable_irq() is required. Only on the last call to enable_irq() is the interrupt line actually enabled. For example, if disable_irq() is called twice, the interrupt line is not actually reenabled until the second call to enable_irq().

    What happens if I disable interrupt line shared among multiple interrupt handlers? Disabling the line disables interrupt delivery for all devices on the line. Therefore, drivers for newer devices tend not to use these interfaces.

    Because PCI devices have to support interrupt line sharing by specification, they should not use these interfaces at all. Thus, disable_irq() and friends are found more often in drivers for older legacy devices, such as the PC parallel port.

    irqs_disabled(): The macro irqs_disabled(), returns nonzero if the interrupt system on the local processor is disabled.
        Header File: <linux/irqflags.h>

    Interrupt Context: When executing a interrupt handler, the kernel is in interrupt context. We know process context is the mode of operation the kernel is in while it is executing on behalf of a process. Eg. Executing a system call.

    As interrupt context is not backed with process, you cannot sleep in interrupt context. If a function sleeps, you cannot use it from your interrupt handler. Examples: kmalloc with GFP_KERNEL, ssleep.

    in_interrupt():
        Header File: <linux/preempt.h>
    To find out whether you are running in interrupt context or process context:
        - in_interrupt() returns non zero if the kernel is performing any type of interrupt handling.
        - in_interrupt() returns zero if the kernel is in process context

    Can we use current macro inside interrupt handler? It points to the interrupted process.

    /* if we add delay in interrupt context, it will have to wait. We should not have delay in interrupt */
    /* we shoudl not sleep in interrupt context */

43)
    Top Half and Bottom Half: Two important goals of interrupt handler are:
        - Execution Time: Handler of an interrupt must execute quickly. long running handlers can slow down the system and may also lead to losing interrupts. The faster the handler returns, the lower the interrupt latencies in the kernel, which is especially important for real-time systems.
        - Execution Context: Interrupt handlers are executed in hard-interrupt context – CPU-local interrupts remain disabled. locking is undesirable and sleeping must be avoided. large amount of work cannot be performed in interrupt handler.
    Both limitations lead to the fact that most interrupt handlers execute only a small amount of code and defer the rest of the work to a later point in time

    Handling of interrupts is divided into two parts:
        1. Top Half (Hard IRQ)
            Acknowledge the interrupt
            Copy the necessary stuff from the device
            schedule the bottom half
        2. Bottom Half (Soft IRQ)
            Remaining pending work

    Top and bottom halves with an example: Network Card on reception of packet from the network, issues an interrupt, kernel responds it by executing the handler.
        Top Half(Interrupt Handler):
            Acknowledges the interrupt,
            copies the new networking packets into main memory,
            pushes it up to the protocol layer
            readies the network card for more packets.
            schedule the bottom half
        Bottom Half:
            Rest of the processing and handling of the packets

    Various Mechanisms available for Bottom Half:
        1. Soft IRQ
        2. Tasklets
        3. Workqueue

    Threaded IRQs: An alternative to using formal bottom-half mechanisms is threaded interrupt handlers. Threaded interrupt handlers seeks to reduce the time spent with interrupts disabled to bare minimum, pushing the rest of the processing out into kernel threads. With threaded IRQs, the way you register an interrupt handler is a bit simplified. You do not even have to schedule the bottom half yourself. The core does that for us. The bottom half is then executed in a dedicated kernel thread.
        int request_threaded_irq (unsigned int irq, irq_handler_t handler, irq_handler_t thread_fn, unsigned long irqflags, const char * devname, void * dev_id);

    Difference between request_irq and request_threaded_irq:
        irq_handler_t thread_fn
    request_threaded_irq() breaks handler code in two parts,
        handler and
        thread function
    Now main functionality of handler is to intimate hardware that it has received the interrupt and wake up thread function. As soon as handler finishes, processor is in process context.
        kernel/irq/manage.c --- setup_irq_thread
    priority of the thread is set to MAX_USER_RT_PRIO/2 which is higher than regular processes

    Why is the threaded handler not being executed even after thread is created? When the hard-IRQ handler (handler function) function returns IRQ_WAKE_THREAD,  the kthread associated with this bottom half will be scheduled, invoking the thread_fn. The thread_fn function must return IRQ_HANDLED when complete.

    After being executed, the kthread will not be rescheduled again until the IRQ is triggered again and the hard-IRQ returns IRQ_WAKE_THREAD.

    IRQF_ONESHOT: The interrupt is not reenabled after the IRQ handler finishes. This flag is required for threaded interrupts which need to keep the interrupt line disabled until the threaded handler has run Specifying this flag is mandatory if the primary handler is set to NULL. The default primary handler does nothing more than to return IRQ WAKE THREAD to wake up a kernel thread to execute the thread fn IRQ handler.
        kernel/irq/manage.c     --> irq_default_primary_handler

    ret_from_fork - a newly-created thread of execution is first switched to, it starts out executing at this function

44)
    Softirqs: Softirqs are bottom halves that run at a high priority but with hardware interrupts enabled
        Implementation: kernel/softirq.c
        Header File: <linux/softirq.h>
        Data structures: Softirqs are represented by the softirq_action structure.
            struct softirq_action
            {
                    void    (*action)(struct softirq_action *);
            };

    A 10 entry array of this structure is declared in kernel/softirq.c
        static struct softirq_action softirq_vec[NR_SOFTIRQS];
            two for tasklet processing (HI_SOFTIRQ and TASKLET_SOFTIRQ),
            two for send and receive operations in networking (NET_TX_SOFTIRQ and NET_RX_SOFTIRQ),
            two for the block layer (asynchronous request completions),
            two for timers, and
            one each for the scheduler and
            read-copy-update processing

    From include/linux/interrupt.h
        enum
        {
                HI_SOFTIRQ=0,
                TIMER_SOFTIRQ,
                NET_TX_SOFTIRQ,
                NET_RX_SOFTIRQ,
                BLOCK_SOFTIRQ,
                IRQ_POLL_SOFTIRQ,
                TASKLET_SOFTIRQ,
                SCHED_SOFTIRQ,
                HRTIMER_SOFTIRQ, /* Unused, but kept as tools rely on the
                                    numbering. Sigh! */
                RCU_SOFTIRQ,    /* Preferable RCU should always be the last softirq */
                NR_SOFTIRQS
        };

    The number of registered softirqs is statically determined and cannot be changed dynamically.

    Preeemption: A softirq never preempts another softirq . The only event that can preempt a softirq is interrupt handler. Another softirq even the same one can run on another processor.

    /proc/softirqs: Shows Per CPU statistics
        Implementation: fs/proc/softirqs.c
    to watch all softirq handling, run [ $ watch -n1 grep RX /proc/softirqs ]

    softirq methods:
        Registering softirq handlers: Software interrupts must be registered before the kernel can execute them. open_softirq is used for associating the softirq instance with the corresponding bottom halve routine.
            void open_softirq(int nr, void (*action)(struct softirq_action *))
            {
                    softirq_vec[nr].action = action;
            }
            It is being called for example from networking subsystem.
            net/core/dev.c:
                open_softirq(NET_TX_SOFTIRQ, net_tx_action);
                open_softirq(NET_RX_SOFTIRQ, net_rx_action);

    Execution of softirq: The kernel maintains a per-CPU bitmask indicating which softirqs need processing at any given time: irq_stat[smp_processor_id].__softirq_pending.

    Drivers can signal the execution of soft irq handlers using a function raise_softirq(). This function takes the index of the softirq as argument.
        void raise_softirq(unsigned int nr)
        {
                unsigned long flags;

                local_irq_save(flags);
                raise_softirq_irqoff(nr);
                local_irq_restore(flags);
        }
        local_irq_save      --> Disables interrupts on the current processor where code is running
        raise_softirq_irqoff    --> sets the corresponding bit in the local CPUs softirq bitmask to mark the specified softirq as pending
        local_irq_restore   --> Enables the interrupts
    raise_softirq_irqoff if executed in non-interrupt context, will invoke wakeup_softirqd(), to wake up, if necessary the ksoftirqd kernel thread of that local CPU

    What is the benefit of per-CPU Bitmask? By using a processors specific bitmap, the kernel ensures that several softIRQs — even identical ones — can be executed on different CPUs at the same time.

    Executing Softirqs: The actual execution of softirqs is managed by do_softirq().
        Implementation : kernel/softirq.c
    do_softirq() will call __do_softirq(), if any bit in the local softirq bit mask is set. __do_softirq() then iterates over the softirq bit mask (least signicant bit) and invokes scheduled softirq handlers.

    Creating a new softirq: You declare softirqs statically at compile time via an enum in <linux/interrupt.h>. Creating a new softirq includes adding a new entry to this enum. The index is used by the kernel as priority. Softirqs with the lowest numerical priority execute before those with a higher numerical priority. Insert the new entry depending on the priority you want to give it.

    generally, tasklet should be used instead of creating new softirq. softirq are used only for driver is very high priority and needs to execute botom half on priority. softirq aren't recommended to be created.

    Registering your handler: Soft irq is registered at runtime via open_softirq(). It takes two parameters:
        a) Index
        b) Handler Function.

    Raising your softirq: To mark it pending, so it is run at the next invocation of do_softirq(), call raise_softirq(). Softirqs are most often raised from within interrupt handlers.

    Other Details: The softirq handlers run with interrupts enabled and cannot sleep. While a handler runs, softirqs on the current processor are disabled. Another processor, can however execute another softirq. If the same softirq is raised again while it is executing, another processor can run in it simultaneously. This means that any shared data even global data used only within the soft irq handler needs proper locking.

    most softirq handlers resort to per-processor data (data unique to each processor and thus not requiring locking) and other tricks to avoid explicit locking and provide excellent scalability.

    hard irq run with interrupts disabled, whereas softirqs run with interrupts enabled

45)
/* softirq doesn't run in hard irq context */
Important Points related to softirqs:
    1. Compile Time:
        Declared at compile time in an enumerator
        Not suitable for linux kernel modules
    2. Execution:
        Executed as early as possible
            After return of a top handler and before return to a system call
        This is achieved by giving a high priority to the executed softirq handlers
    3. Parallel:
        Softirqs can run in parallel
        Each processor has its own softirq bitmap
        One softirq cannot be scheduled twice on the same processor
        One softirq may run in parallel on other
    4. Priority:
        Kernel iterates over the softirq bitmap, least significant bit (LSB) first, and execute the associated
        softirq handlers

ksoftirqd: Softirqs are executed as long as the processor-local softirq bitmap is set. Since softirqs are bottom halves and thus remain interruptible during execution, the system can find itself in a state where it does nothing else than serving interrupts and softirqs. incoming interrupts may schedule softirqs what leads to another iteration over the bitmap.

Such processor-time monopolization by softirqs is acceptable under high workloads (e.g., high IO or network traffic), but it is generally undesirable for a longer period of time since (user) processes cannot be executed.

Solution to above problem by kernel: After the tenth iteration(MAX_SOFTIRQ_RESTART) over the softirq bitmap, the kernel schedules the so-called ksoftirqd kernel thread, which takes control over the execution of softirqs.

Each processor has its own kernel thread called ksoftirqd/n, where n is the number of the processor. This processor-local kernel thread then executes softirqs as long as any bit in the softirq bitmap is set.

The aforementioned processor-monopolization is thus avoided by deferring softirq execution into process context (i.e., kernel thread), so that the ksoftirqd can be preempted by any other (user) process.

ps -ef | grep ksoftirqd/

The spawn_ksoftirqd function starts these threads. It is called early in the boot process.
    static __init int spawn_ksoftirqd(void)
    {
            cpuhp_setup_state_nocalls(CPUHP_SOFTIRQ_DEAD, "softirq:dead", NULL,
                                      takeover_tasklets);
            BUG_ON(smpboot_register_percpu_thread(&softirq_threads));

            return 0;
    }
    early_initcall(spawn_ksoftirqd);
        File: kernel/softirq.c

Each ksoftirqd/n kernel thread runs the run_ksoftirqd().
    static void run_ksoftirqd(unsigned int cpu)
    {
            local_irq_disable();
            if (local_softirq_pending()) {
                    /*
                     * We can safely run softirq on inline stack, as we are not deep
                     * in the task stack here.
                     */
                    __do_softirq();
                    local_irq_enable();
                    cond_resched();
                    return;
            }
            local_irq_enable();
    }

local_softirq_pending(): It is 32-bit mask of pending softirqs.

When are pending softirqs run? Pending softirq handlers are checked and executed at various points in the kernel code.
    a) After the completion of hard interrupt handlers with IRQ Lines Enabled
        do_IRQ() function finishes handling an I/O interrupt and invokes the irq_exit()
    b) call to functions like local_bh_enable() or spin_unlock_bh()
    c) when one of the special ksoftirqd/n kernel threads is awakened

in_softirq: You can tell you are in a softirq (or tasklet) using the in_softirq() macro

Disabling/Enabling Softirqs: If a softirq shares data with user context, you have two problems.
    1) the current user context can be interrupted by a softirq
    2) the critical region could be entered from another CPU

Solution to first problem:
    void local_bh_disable()     Disable softirq and tasklet processing on the local processor
    void local_bh_enable()      Enable softirq and tasklet processing on the local processor
The calls can be nested only the final call to local_bh_enable() actually enables bottom halves.

Locking Between User Context and Softirqs:
    spin_lock_bh()  Disables softirqs on the CPU and then grabs the lock
    spin_unlock_bh() Release lock and enable softirqs

46)
    Tasklets: Tasklets are bottom half mechanism built on top of softirqs. Handlers of tasklets are executed by softirqs.

    Implementation: Tasklets are implemented on top of softirqs. Tasklets are represented by two softirqs: HI_SOFTIRQ and TASKLET_SOFTIRQ. The only difference in these types is that the HI_SOFTIRQ-based tasklets run prior to the TASKLET_SOFTIRQ based tasklets.

    Data Structure:
        tasklet_struct
        Header File: <linux/interrupt.h>
        struct tasklet_struct
        {
                struct tasklet_struct *next; /* next tasklet in the list */
                unsigned long state;   /* state of the tasklet */
                atomic_t count;     /* reference counter */
                void (*func)(unsigned long); /* tasklet handler function */
                unsigned long data;  /* argument to the tasklet function */
        };

    state field: It can be
        a) 0
        b) TASKLET_STATE_SCHED
        c) TASKLET_STATE_RUN

    TASKLET_STATE_SCHED denotes a tasklet that is scheduled to run.
    TASKLET_STATE_RUN denotes a tasklet that is running.

    TASKLET_STATE_RUN is used only on multiprocessor machines. It is used to protect tasklets against concurrent execution on several processors.

    count field: used as a reference count for the tasklet
        count = 0       the tasklet is enabled and can run if marked pending
        count = nonzero     the tasklet is disabled and cannot run

    Declaring Tasklets:
        Static Initialization:
            DECLARE_TASKLET
                #define DECLARE_TASKLET(name, func, data) \
                struct tasklet_struct name = { NULL, 0, ATOMIC_INIT(0), func, data }
            DECLARE_TASKLET_DISABLED
                #define DECLARE_TASKLET_DISABLED(name, func, data) \
                struct tasklet_struct name = { NULL, 0, ATOMIC_INIT(1), func, data }
            Both these macros statically create a struct tasklet_struct with the given name. When the tasklet is scheduled, the given function func is executed and passed data as argument. The difference between the two macros is the initial reference count.
        Dynamic:
            void tasklet_init(struct tasklet_struct *t, void (*func)(unsigned long), unsigned long data);

    Scheduling Tasklets: The kernel maintains two per-CPU tasklet linked lists for queuing scheduled tasklets
        kernel/softirq.c
            static DEFINE_PER_CPU(struct tasklet_head, tasklet_vec);
            static DEFINE_PER_CPU(struct tasklet_head, tasklet_hi_vec);
        tasklet_vec : for regular tasklets, run by TASKLET_SOFTIRQ and
        tasklet_hi_vec : for high-priority tasklets, run by HI_SOFTIRQ

    Both of these structures are linked lists of tasklet_struct structures. Each tasklet_struct structure in the list represents a different tasklet.
        struct tasklet_head {
                struct tasklet_struct *head;
                struct tasklet_struct **tail;
        };

    Tasklets are scheduled via the tasklet_schedule() and tasklet_hi_schedule().
        static inline void tasklet_schedule(struct tasklet_struct *t)
        {
                if (!test_and_set_bit(TASKLET_STATE_SCHED, &t->state))
                        __tasklet_schedule(t);
        }

    The above function checks whether the tasklet is already scheduled, if not it atomically sets the state to
    TASKLET_STATE_SCHED and invokes __tasklet_schedule to add the tasklet into the pending queue.

    Steps performed by tasklet_schedule:
        1. Check whether the tasklet’s state is TASKLET_STATE_SCHED. If it is, the tasklet is already scheduled to run and the function can immediately return.
        2. Call __tasklet_schedule()
        3. Save the state of the interrupt system, and then disable local interrupts by calling local_irq_save. This ensures that nothing on this processor will mess with the tasklet code while tasklet_schedule() is manipulating the tasklets
        4. Add the tasklet to be scheduled to the head of the tasklet_vec or tasklet_hi_vec linked list, which is unique to each processor in the system.
        5. Raise the TASKLET_SOFTIRQ or HI_SOFTIRQ softirq, so do_softirq() executes this tasklet in the near future.
        6. Restore interrupts to their previous state and return

        static void __tasklet_schedule_common(struct tasklet_struct *t,
                                              struct tasklet_head __percpu *headp,
                                              unsigned int softirq_nr)
        {
                struct tasklet_head *head;
                unsigned long flags;

                local_irq_save(flags);
                head = this_cpu_ptr(headp);
                t->next = NULL;
                *head->tail = t;
                head->tail = &(t->next);
                raise_softirq_irqoff(softirq_nr);
                local_irq_restore(flags);
        }

    Steps performed by tasklet softirq handlers:
        Handlers: tasklet_action()/tasklet_hi_action()
        1. Disable local interrupt delivery and get the tasklet_vec or tasklet_hi_vec list for this processor
        2. Clear the list for this processor by setting it equal to NULL.
        3. Enable local interrupt delivery.
                struct tasklet_struct *list;

                local_irq_disable();
                list = tl_head->head;
                tl_head->head = NULL;
                tl_head->tail = &tl_head->head;
                local_irq_enable();
        4. Loop over each pending tasklet in the retrieved list.
        5. Check for a zero count value, to ensure the tasklet is not disabled. If the tasklet is disabled, skip it and go to the next pending tasklet.
        6. Run the tasklet handler.
        7. Repeat the next pending tasklet, until there are no more scheduled tasklets waiting to run.
                while (list) {
                        struct tasklet_struct *t = list;
                        list = list->next;
                        if (tasklet_trylock(t)) {
                                if (!atomic_read(&t->count)) {
                                        if (!test_and_clear_bit(TASKLET_STATE_SCHED,
                                                                &t->state))
                                                BUG();
                                        t->func(t->data);
                                        tasklet_unlock(t);
                                        continue;
                                }
                                tasklet_unlock(t);
                        }
                        local_irq_disable();
                        t->next = NULL;
                        *tl_head->tail = t;
                        tl_head->tail = &t->next;
                        __raise_softirq_irqoff(softirq_nr);
                        local_irq_enable();
                }

    How kernel avoids running the same tasklet on multiple processors:
        tasklet_trylock
        tasklet_unlock
            static inline int tasklet_trylock(struct tasklet_struct *t)
            {
                    return !test_and_set_bit(TASKLET_STATE_RUN, &(t)->state);
            }
            static inline void tasklet_unlock(struct tasklet_struct *t)
            {
                    smp_mb__before_atomic();
                    clear_bit(TASKLET_STATE_RUN, &(t)->state);
            }

    On a multiprocessor machine, the kernel checks whether TASKLET_STATE_RUN is set (which means another processor is running this tasklet). If set, do not execute now and skip to the next pending tasklet. Else set the TASKLET_STATE_RUN flag so that another processor cannot execute. After the tasklet completes, clear the TASKLET_STATE_RUN flag.

    Can i sleep in tasklet handler? As tasklets are based on softirqs, you cannot sleep. You cannot use semaphores or other blocking functions in tasklet handler.

47)
    Are interrupts enabled when tasklet runs? /* interrupts are enabled in tasklet similar to softirq */

    /* tasklet runs in interrupt context same as softirq */

    What happens if i call tasklet_schedule twice? After a tasklet is scheduled, it runs once at some time in the near future. If the same tasklet is scheduled again, before it has had a chance to run, it still runs only once

    Tasklets are also guaranteed to run on the same CPU as the function that first schedules them. The same CPU on which softirq, interrupt handler and tasklet is running.

    Enabling/Disabling Tasklets:
        Disable tasklet
            tasklet_disable()
            tasklet_disable_nosync()
        Enable tasklet
            tasklet_enable()

    tasklet_disable() will wait if the tasklet is currently running and return only after it has finished execution.
    tasklet_disable_nosync() will not wait for the tasklet to complete prior to returning.

    static inline void tasklet_disable_nosync(struct tasklet_struct *t)
    {
            atomic_inc(&t->count);
            smp_mb__after_atomic();
    }
    static inline void tasklet_disable(struct tasklet_struct *t)
    {
            tasklet_disable_nosync(t);
            tasklet_unlock_wait(t);
            smp_mb();
    }
    static inline void tasklet_enable(struct tasklet_struct *t)
    {
            smp_mb__before_atomic();
            atomic_dec(&t->count);
    }

    tasklet_kill:
        void tasklet_kill(struct tasklet_struct *t);
    The function removes a tasklet from the pending queue.
        void tasklet_kill(struct tasklet_struct *t)
        {
                if (in_interrupt())
                        pr_notice("Attempt to kill tasklet from interrupt\n");

                while (test_and_set_bit(TASKLET_STATE_SCHED, &t->state)) {
                        do {
                                yield();
                        } while (test_bit(TASKLET_STATE_SCHED, &t->state));
                }
                tasklet_unlock_wait(t);
                clear_bit(TASKLET_STATE_SCHED, &t->state);
        }
    This function must not be used from interrupt context because it sleeps. If the tasklet specified is already scheduled by the time this call is invoked, then this function waits until its execution completes.

    tasklet_hi_schedule: In addition to normal tasklets, the kernel uses a second kind of tasklet of a higher priority. HI_SOFTIRQ is used as a softIRQ instead of TASKLET_SOFTIRQ. tasklet_hi_schedule() should be used if the tasklet should  run more urgently than networking, SCSI, timers.

    Softirqs vs tasklets
    ----------------------
                    Softirqs                    Tasklets

    Allocation:     Allocated at compile time   Can be dynamically registered

    Reentrancy:     Yes, same softirqs can run  No, Same tasklet will not be scheduled
                    on different processors     on different processors

48)
    Work Queues: They allow kernel functions to be activated and later executed by special kernel threads called worker threads. Worker threads run in process context. ( softirq and tasklet runs interrupt context. )

    It is the only choice when you need to sleep in your bottom half (I/O data, hold mutexes/semaphores and all other functions that internally sleep)

        Implementation: kernel/workqueue.c

    Design:
        work item: struct which hold the pointer to the function to be executed asynchronously
        work queue: a queue of work items

    Drivers add work item into the work queue. Worker Threads: Special purpose threads which execute the functions from the queue, one after the other. If no work is queued, the worker threads become idle. ps -ef | grep kworker.

    Worker Pools: A thread pool that is used to manage the worker threads. There are two worker-pools:
        - one for normal work items and
        - the other for high priority ones,
    some extra worker-pools to serve work items queued on unbound workqueues.
    create_worker is the function where kthreads are created

    Legacy Workqueues: Legacy workqueues have dedicated threads associated with them. The new workqueues do away with that. There are no threads dedicated to any specific workqueue. Instead, there is a global pool of threads attached to each CPU in the system. When a work item is enqueued, it will be passed to one of the global threads at the right time.

    How a target worker pool is determined when work item is queued into workqueue? According to the queue parameters and workqueue attributes.

    Data structures:
    workqueue       --  struct workqueue_struct
    work items      --  struct work_struct
        struct work_struct {
                atomic_long_t data;
                struct list_head entry;
                work_func_t func;
        };
    func is a pointer that takes the address of the deferred routine
        typedef void (*work_func_t)(struct work_struct *work);

    Initialization of Work Items:
        Header File: <linux/workqueue.h>
        Static: declare and initialize a work item:
            DECLARE_WORK(name, void (*function)(void *), void *data);
        Dynamic: initialize an already declared work item:
            INIT_WORK(struct work_struct *work, void(*function)(struct work_struct *));

    API's to queue Work: This function enqueues the given work item on the local CPU workqueue, but does not guarantee its execution on it.
        bool queue_work(struct workqueue_struct *wq, struct work_struct *work);
    Once queued, the function associated with the work item is executed on any of the available CPUs by the relevant
    kworker thread.

    To queue work on a specific CPU:
        bool queue_work_on(int cpu, struct workqueue_struct *wq, struct work_struct *work)
            cpu: CPU number to execute work on
            Return: false if work was already on a queue, true otherwise.

    Workqueues: Workqueue API provides two types of function interfaces to
        a) Create own workqueue
        b) Use System Workqueue (extern struct workqueue_struct *system_wq;)
            Header File: <linux/workqueue.h>
        System workqueue is shared by all kernel subsystems and services

    /* workqueue doesn't guarantee that it will get scheduled on same processor */
    /* since work item is already queued, adding second time will fail */

    Inline functions:
        schedule_work - put work task in global workqueue
        schedule_work_on - put work task on a specific cpu
            static inline bool schedule_work(struct work_struct *work)
            {
                    return queue_work(system_wq, work);
            }
            static inline bool schedule_work_on(int cpu, struct work_struct *work)
            {
                    return queue_work_on(cpu, system_wq, work);
            }

    Usually a work is enclosed in a larger structure (for driver private data)

49)
    /* workqueue runs in process context. tasklet and softirq runs interrupt context */
    /* irqs are not disable in workqueue because it runs in process context */

    Cancelling work: Work items can not be enabled/disabled but they can be canceled by calling cancel_work_sync().
        bool cancel_work_sync(struct work_struct *work);
    The call only stops the subsequent execution of the work item. If the work item is already running at the time of the call, it will continue to run.
        Return:
            true    -   if work was pending
            false   -   otherwise

    /* adding sleep will cause waiting */
    /* work queues will get executed in same order in which those got queued */

    Flushing work: bool flush_work(struct work_struct *work); wait for a work to finish executing the last queueing instance. returns true if waited for the work to finish execution, false if it was already idle

    Delayed work: Workqueue API allows you to queue work tasks whose execution is guaranteed to be delayed at least until a specified timeout. This is achieved by binding a work task with a timer, which can be initialized with an expiry timeout, until which time the work task is not scheduled into the queue.
        struct delayed_work {
                struct work_struct work;
                struct timer_list timer;

                /* target workqueue and CPU ->timer uses to queue ->work */
                struct workqueue_struct *wq;
                int cpu;
        };
    timer is an instance of a dynamic timer descriptor, which is initialized with the expiry interval and armed while scheduling a work task

    Initialization:
        Static:
            DECLARE_DELAYED_WORK(name, void(*function)(struct work_struct *));
        Dynamic:
            INIT_DELAYED_WORK(struct delayed_work *work, void(*function)(struct work_struct *));

    Scheduling:
        bool schedule_delayed_work(struct delayed_work *dwork, unsigned long delay);
        bool schedule_delayed_work_on(int cpu, struct delayed_work *dwork, unsigned long delay);
    delay needs to be provided in jiffies

        bool flush_delayed_work(struct delayed_work *dwork);
    delayed timer is cancelled and the pending work is queued for immediate execution

    cancel delayed work: bool cancel_delayed_work(struct delayed_work *dwork)

    Differences between Tasklets, softirqs and Workqueues
    ========================================================
                  ------------------------------------------------------------------------------
                  Softirqs                        Tasklets                           Workqueues
                  -----------------------------------------------------------------------------
    Execution     Interrupt context               Interrupt Context                  Process Context
    Context
    ----------------------------------------------------------------------------------------------------------------
    Reentrancy    Yes(can run simultaneously      Cannot run same tasklets           Yes (can run simultaneously
                  on different CPUs)              on different CPUs. Different       on different CPUs)
                                                  CPUs can run different tasklets
    --------------------------------------------------------------------------------------------------------------
    Sleep         Cannot sleep                    Cannot Sleep                       Can Sleep
    --------------------------------------------------------------------------------------------------------------
    Preemption    Cannot be preempted/scheduled   Cannot be preempted/scheduled      May be preempted/scheduled
    -----------------------------------------------------------------------------------------------------------
    Ease of use   Not easy to use                 Easy to use                        Easy to use
    -------------------------------------------------------------------------------------------------------------
    When to use   If deferred work will not       If deferred work will not go       If deferred work needs to sleep
                  go to sleep and have crucial    to sleep
                  scalability or speed
                  requirements
    ----------------------------------------------------------------------------------------------------------------

50)
    kworker: In last sessions, we used system work queues, now we will use our own work queue. kworker processes are kernel worker processes which actually execute work items. From Documentation/kernel-per-CPU-kthreads.txt.

    Naming convention: kworker/%u:%d%s (cpu, id, priority)
    Worker threads are also two types:
        CPU Bound  : It is named as kworker/<corenumber>:<id>
        CPU Unbound: It is named as kworker/u<poolnumber>:<id>

    $ ps -ef | grep 'kworker'
        kworker/2:0H      -->   running on CPU2 and threadID 0 and is high priority bounded
    The u designates a special CPU, the unbound cpu, meaning that the kthread is currently unbound
        kworker/u257:0-   -->   unbound thread runs on any CPU

    Use taskset to find the CPU Affinity: taskset is used to set or retrieve the CPU affinity of a running process given its PID or to launch a new COMMAND with a given CPU affinity.
    $ taskset -p [PID]

    To find out what any kworker is doing: $ cat /proc/$(pid_of_kworker)/stack

    Dedicated Workqueues: Timing of the execution of work items scheduled onto the global workqueue is not predictable. one long-running work item can always cause indefinite delays for the rest. Alternatively, the workqueue framework allows the allocation of dedicated workqueues.
        struct workqueue_struct *alloc_workqueue(const char *fmt, unsigned int flags, int max_active, ...);
    The above function allocates a workqueue.
        Parameters:
            fmt: printf format for the name of the workqueue
            flags: control how work items are assigned execution resources, scheduled and executed.
            max_active: This parameter limits the number of work items which can be executed simultaneously from this workqueue on any given CPU.
            remaining args: args for @fmt

    destroy a workqueue:
        void destroy_workqueue(struct workqueue_struct *wq)
    safely terminate a workqueue; all work currently pending will be done first.

    WQ_UNBOUND: Workqueues created with this flag are managed by kworker-pools that are not bound to any specific CPU. Scheduled work items to this queue can run on any available processor. Work items in this queue are executed as soon as possible by kworker pools.

    /* max active workqueue are 2, so 2 can run parallely */

    WQ_HIGHPRI: This flag is used to mark a workqueue as high priority. Queued to the highpri worker-pool of the target cpu. Highpri worker-pools are served by worker threads with elevated nice level.

    Nice value: Nice value ranges from -20 (highest priority level) to 19 (lowest priority level). Default value is 0. To check the nice value:
        $ ps ax -o pid,ni,cmd
    The above command prints pid, nice value and process name

51)
        static inline struct delayed_work *to_delayed_work(struct work_struct *work)
        {
                return container_of(work, struct delayed_work, work);
        }
        Header File: <linux/workqueue.h>

    Perform periodic task using workqueues: To perform some task periodically, requeue the work from the work function itself.

    WQ_SYSFS: A given workqueue can be made visible in the sysfs filesystem by passing the WQ_SYSFS to that workqueue's alloc_workqueue().
        $ ls sys/devices/virtual/workqueue
        # echo 1 > /sys/devices/virtual/workqueue/cpumask

    # echo 2 > /sys/devices/virtual/workqueue/my_queue/cpumask. Can be used to run work queues to run on particular CPU.

    Other Flags:
        WQ_FREEZABLE:
            A freezable wq participates in the freeze phase of the system suspend operations. This flag is used in the context of power management and file systems, and is especially important for creating the system image in the suspend phase. workqueues which can run tasks as part of the suspend/resume process should not have this flag set. You can find more information about this topic in Documentation/power/freezing-of-tasks.txt.
        WQ_MEM_RECLAIM:
            All workqueues which might be used in the memory reclaim paths must have this flag set. The workqueue is guaranteed to have at least one woker, a so-called rescuer thread, regardless of memory pressure. Let us consider the following scenario:
                Workqueue W has 3 items A, B and C. A does some work and then waits until C has finished some work. Afterwards, B does some GFP KERNEL allocations and blocks as there is not enough memory available. As a result, C cannot run since B still occupies the W's worker. another worker cannot be created because there is not enough memory. A pre-allocated rescuer thread can solve this problem, by executing C which then wakes up A. B will continue as soon as there is enough available memory to allocate.
        WQ_CPU_INTENSIVE:
            tasks on this workqueue can be expected to use a fair amount of CPU time. In other words, runnable CPU intensive work items will not prevent other work items in the same worker pool from starting execution.

    flush_workqueue:
        void flush_workqueue(struct workqueue_struct *wq);
    This function sleeps until all work items which were queued on entry have been finished. typically used in driver shutdown handlers.

    WQ_MAX_UNBOUND_PER_CPU - number of unbound workqueues that can be associated with a cpu.
    WQ_UNBOUND_MAX_ACTIVE -  total number of unbound workqueues.

    alloc_ordered_workqueue:
        #define alloc_ordered_workqueue(fmt, flags, args...)
    Allocate an ordered workqueue. An ordered workqueue executes at most one work item at any given time in the queued order. They are implemented as unbound workqueues with max_active of one.

    What happens when you call queue_work? Use cscope/ctags to find out.

    IMP: watch video https://youtu.be/Ew3KI0-ybGc?list=PLCFZYUMGKxqCOP3tSrSW1XgNUI8RBOxCH at 20:00

52)
Inline Assembly: In Linux Kernel, you normally see statements like:
    asm volatile ("outb %al, $0x80");
    asm volatile("mov %%cr0,%0" : "=r" (cr0));
This is inline assembly. In other words assembly code integrated in C/C++ language.

Advantages of Inline Assembly:
    a) Optimization: Programmers can use assembly language code to implement the most performance-sensitive parts of their program's algorithms code that is apt to be more efficient than what might otherwise be generated by the compiler.
    b) Access to processor specific instructions: Most processors offer special instructions, such as Compare and Swap and Test and Set instructions which may be used to construct semaphores or other synchronization and locking primitives.
    c) System calls: High-level languages rarely have a direct facility to make arbitrary system calls, so assembly code is used.
    d) Inline assembly code can use any C variable or function name that is in scope, so it is easy to integrate it with your C code.

54)
    Disabling Interrupts can be a way to achieve mutual exclusion on single processor system. But this is a worst way.
        Disable interrupts on current CPU:
            local_irq_disable(): Finally calls native_irq_disable()
                asm volatile("cli": : :"memory");
        Enable interrupts on current CPU:
            local_irq_enable(): Finally calls native_irq_enable()
                asm volatile("sti": : :"memory");

    Saving and Restoring Flags: local_irq_save() and local_irq_restore() takes care of saving and restoring the interrupt states.

    An atomic operation is an operation that will always be executed without any other process being able to read or change state that is read or changed during the operation. Methods for ensuring an operation is atomic:
        - Single processor-single core systems: On a single processor system, if an operation is implemented in a single CPU instruction, it is always atomic. Ex: xchg, inc. If operation is implemented in multiple CPU instructions, then it can be interrupted in middle of execution.
                a) Interrupts
                b) Context switch
            Techniques for atomicity:
                a. spinlock
                b. Disabling interrupts
        - Multiprocessor or multicore systems: On multiprocessor systems, ensuring atomicity exists is a little harder. Disabling interrupts or using a single instruction will not guarantee atomic access. It is still possible to use a lock (e.g. a spinlock).

    To ensure that no other processor or core in the system attempts to access the data you are working with, use LOCK signal on the bus, which prevents any other processor in the system from accessing the memory at the same time. On x86 processors, some instructions automatically lock the bus (e.g. 'XCHG'). while others require you to specify a 'LOCK' prefix to the instruction to achieve this (e.g. 'CMPXCHG', which you should write as 'LOCK CMPXCHG op1, op2').

    Locking atomically: We need to make sure no one gets between load and store.
        Solution: atomic compare and swap compare and swap operation should be performed atomically. compares the contents of a memory location with a given value and, only if they are the same, modifies the contents of that memory location to a new given value.
            tmp = *addr ; //load
            if (tmp == old)
                *addr = new; //store
        The above operation should be performed atomically. x86 provides instruction cmpxchg. Using lock prefix guarantees atomicity. Look into definition of spin_lock() using cscope.
            static __always_inline void queued_spin_lock(struct qspinlock *lock)
            {
                    u32 val = 0;

                    if (likely(atomic_try_cmpxchg_acquire(&lock->val, &val, _Q_LOCKED_VAL)))
                            return;

                    queued_spin_lock_slowpath(lock, val);
            }
        spinlock also internally uses cmpxchg instruction only

56)
    What is System Call?  System calls provide userspace processes a way to request services from the kernel or It is a way of requesting the kernel to do something on your behalf.

    What kind of services? Services which are managed by operating system like storage, memory, network, process management etc. Examples of Kernel Service
        1. Write/read file
        2. Listen for connections on a socket
        3. Create/Delete directory
        4. creating and executing new processes,
    On Unix systems, the C library provides wrapper around these system calls

    What happens inside a system call ? A kernel code snippet is run on request of a user process. This code runs in ring 0 (with current privilege level -CPL- 0), which is the highest level of privilege in x86 architecture. All user processes run in ring 3 (CPL 3). To implement system call mechanism, we need
        1) a way to call ring 0 code from ring 3 and
        2) some kernel code to service the request.

    Why can't the application directly access hardware, why do we need system calls? Advantage of having system calls:
        1. Makes programming easier by freeing users from studying low-level programming characteristics of hardware devices.
        2. It greatly increases system security, because kernel can check the accuracy of the request at the interface level before attempting to satisfy it.

    Difference between System Call and Function Call: System call code is run inside kernel, program has to switch from user mode into kernel mode. Switching to kernel mode can happen in different ways (eg. via syscall). Special instructions are needed to make the processor perform a transition to ring 0 (privileged mode). System call is identified by a system call number rather than function address.

    System Call Number: Each and every system call has a system call number which is known by both the userspace and the kernel. Example: read() has a system call number 3, open() has system call number 5. System call number is different for different architectures. Example: exit() has a system call number 60 on x86_64 and 1 on x86.

    System Call Table: Kernel uses a table which maps the system call number to system call handler. This table is called as System Call Table.

    System call vs IOCTL:
        System calls are used to request kernel specific services.
        IOCTLs are used to send special commands to devices other than standard methods.

    Location of System Call Table: arm: arch/arm/tools/syscall.tbl

    Passing Parameters: Like ordinary functions, system calls often require some input/output parameters. Parameters of ordinary C functions are usually passed by writing their values in the active program stack. Because system calls are a special kind of function that cross over from user land to kernel land, neither the user mode or kernel mode stack can be used. Rather, system call parameters are written in the CPU registers before issuing the system call.

    The kernel then copies the parameters stored in the CPU registers on to the kernel mode stack before invoking the system call service routine.

    On Linux, there are several ways to make a system calls:
        - through software interrupt
                In Intel, to execute a system call user process will
                    copy desired system call number to %eax
                    copy parameters if present into registers as per ABI
                    execute int $0x80
                    return value is read from %eax
                This will generate interrupt 0x80 and an interrupt service routine will be called. What happens in kernel space at int $0x80 ? x86 trap is registered for 0x80 in function idt_setup_traps(). File:arch/x86/kernel/idt.c
                    /* this sets up interrupt descriptor table */
                    void __init idt_setup_traps(void)
                    {
                            idt_setup_from_table(idt_table, def_idts, ARRAY_SIZE(def_idts), true);
                    }
                    static const __initconst struct idt_data def_idts[] = {
                    ....
                    ....
                            SYSG(IA32_SYSCALL_VECTOR,       entry_INT80_32)
                    };
                    IA32_SYSCALL_VECTOR is simply a define for the number 128 (0x80)
                    #define IA32_SYSCALL_VECTOR             0x80
                    So, the function entry_INT80_32 is called (arch/x86/entry/entry_32.S)
            Problems with int $0x80: As, it uses software interrupt, this is slow. Because it has to find ISR and execute it and return back. It will involve lot of switching.
        - Fast System Calls: An interrupt is extremely expensive, especially when you invoke a system call like getpid() that only returns the value of one variable. The newer method has the below advantages:
                a) Don't involve a software interrupt
                b) Much faster than using a software interrupt
            The method comprises of two instructions.
                    One to enter the kernel
                    One to leave the kernel
            On 32-bit Systems: sysenter and sysexit
            On 64-bit systems: syscall and sysret

    Differences between legacy system call and sysenter: Using sysenter to make a system call is more complicated than using the legacy interrupt method. It involves more coordination between the user program (via glibc) and the kernel. Prior to executing the SYSENTER instruction, software must specify
            - the privilege level 0 code segment
            - code entry point
            - the privilege level 0 stack segment and stack pointer
    to the following MSRs
        IA32_SYSENTER_CS (MSR address 174H)
        IA32_SYSENTER_EIP (MSR address 176H)
                This MSR is where the kernel should specify the address of the function that will execute when a sysenter instruction is executed by a user program
        IA32_SYSENTER_ESP (MSR address 175H)
        From arch/x86/kernel/cpu/common.c

    The __kernel_vsyscall( ) function saves on the User Mode stack the contents of ebp, edx, and ecx (these registers are going to be used by the system call handler), copies the user stack pointer in ebp, then executes the sysenter instruction.
        arch/x86/entry/vdso/vdso32/system_call.S
        arch/x86/entry/entry_32.S
    How can we call __kernel_vsyscall from userspace ? Kernel creates a single page in the memory and attaches it to all processes' address space when they are loaded into memory. Kernel calls this page virtual dynamic shared object (vdso). This has definition of __kernel_vsyscal. When you compile the kernel, it will automatically compile and link the vDSO code for you. You will frequently find it under the architecture-specific directory.
        $ find arch/$ARCH/ -name '*vdso*.so*'
        $ nm vdso32.so.dbg | grep kernel_vsyscall
    ldd on user space application shows (linux-vdso.so) shared library

57)
    How does the address of __kernel_vsyscall is determined? Userland processes (or C library on their behalf) call __kernel_vsyscall to execute system calls. Address of __kernel_vsyscall is not fixed. Kernel passes this address to userland processes using AT_SYSINFO elf parameter

    Return value of system calls: All system calls return long. Positive or 0 value denotes a successful termination of the system call. Negative value denotes an error condition. The value is the negation of the error code that must be returned to the application program in the errno variable. The errno variable is not set or used by the Linux Kernel. Instead, the wrapper routines handle the task of setting this variable after a return from a system call.

    syscall: syscall() is a small library function that invokes the system call. This is provided to use when invoking a system call that has no wrapper function in the C library.

    syscall()
        saves CPU registers before making the system call
        restores the registers upon return from the system call
        and stores any error code returned by the system call in errno if an error occurs

    What happens when we use a system call number which is not implemented ? Non implemented system calls returns -ENOSYS

    vDSO (Virtual Dynamic Shared Object): What is vDSO ? The  "vDSO" (virtual dynamic shared object) is a small shared library that the kernel automatically maps into the address space of all user-space applications.

    Why does the vDSO exist at all ? communication between userspace applications and the kernel is through system calls. Eg. fork(), vfork() etc.

    When a process invokes a system call:
        1. it executes a special instruction forcing the CPU to switch to kernel mode
        2. saves the contents of the registers on the kernel mode stack
        3. starts the execution of a kernel function
        4. When the system call has been serviced, the kernel restores the contents of the registers saved on the kernel mode stack
        5. executes another special instruction to resume execution of the user-space process.
    Numerous clock cycles are spent just to make these special kind of function calls.

    Issue" There are some system calls the kernel provides that user-space code ends up using frequently, to the point that such calls can dominate overall performance. Eg. gettimeofday(). Performance is reduced due to
        1. Frequency of the call
        2. Context-switch overhead that results from exiting user space and entering the kernel.
    Certain functions that do not write any data and merely return a value stored in the kernel, such as gettimeofday(), are relatively safe in nature. With vDSO, the kernel arranges for the required information to be placed in memory the process can access. Now a call to gettimeofday(2) changes from a system call to a normal function call and a few memory accesses.

    strace and vdso: When tracing systems calls with strace, system calls that are exported by the vDSO will not appear in the trace output. You can capture calls to system calls which have been implemented via the vDSO by using ltrace instead of strace.
        ldd ./prog
            linux-vdso.so.1 (0x00007ffce147a000)
            libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f6ef9e8e000)
            /lib64/ld-linux-x86-64.so.2 (0x00007f6efa481000)
        sudo find / -name 'linux-vdso.so*'
        Will do not give any results. why ? It's a virtual shared object that doesn't have any physical file on the disk. it's a part of the kernel that's exported into every program's address space when it's loaded. //look at /proc/pid/maps

    vsyscalls: The vsyscall or virtual system call is the first and oldest mechanism in the Linux kernel that is designed to accelerate execution of certain system calls.
        File: arch/x86/entry/vsyscall/vsyscall_emu_64.S
        System Calls:
            gettimeofday
            time
            getcpu

    How it works ? The Linux kernel maps into user space a page that contains some variables and the implementation of some system calls. $ cat /proc/self/maps | grep vsyscall

    Limitations:
        1. space only for 4 system calls
        2. security: Type cat /proc/self/maps multiple times.
            IMP: Note that the vDSO area gets changed every time, while the vsyscall page remains at the same location
            The location of the vsyscall page is nailed down in the kernel ABI, but the vDSO area is ASLR
    Note: The security issue has been mitigated by emulating a virtual system call, but the emulation introduces additional latency.

    vDSO is dynamically allocated, offers improved safety through memory space randomization, and supports more than 4 system calls. vDSO links are provided via the glibc library. If the kernel does not have vDSO support, a traditional syscall is made.

58)
    Tracing Read System Call: Implemented in fs/read_write.c

    SYSCALL_DEFINEn is a family of macros that make it easy to define a system call with N arguments. n suffix indicates the argument count.

    asmlinkage: When you compile a program the compiler's optimizer will occasionally put a function's parameters in registers instead of putting them on the program's stack. This optimization works because the compiler is emitting code for both the caller and the callee and so both sides are made aware of this slight-of-hand. This will not work for the kernel. Because kernel is running in different space and already compiled. Whereas, user space will get compiled later at users will. asmlinkage is #define for some gcc magic that tells the compiler that the function should not expect to find any of its arguments in registers (a common optimization), but only on the CPU's stack The actual implementation of asmlinkage is architecture-specific.

    How come userspace reaches sys_read: We know system calls are identified by system call number. System call table is where mapping between system call number and the entry point of system call is present. System call table for x86_64: arch/x86/entry/syscalls/syscall_64.tbl.

    Adding a New System Call:
        System Call which do not accept any arguments:
            1. cd <linux_source>
            2. Create a directory named 'my_syscalls' and change the directory to 'my_syscalls'
                $ mkdir my_syscalls
                $ cd my_syscalls
            3. Create a file 'hello.c'
                #include <linux/kernel.h>
                #include <linux/syscalls.h>
                SYSCALL_DEFINE0(hello) {
                    printk("Hello, world!\n");
                    return 0;
                }
            4. Create a 'Makefile' in the 'my_syscalls' directory.
                $ vi Makefile
                obj-y := hello.o
                This is to ensure that the hello.c file is compiled and included in the kernel source code.
            5. Add 'my_syscalls' to kernel Makefile
                $ cd ..
                search for core-y,  you’ll find this line as the second instance of your search:
                    core-y          += kernel/ certs/ mm/ fs/ ipc/ security/ crypto/ block/
                Add 'my_syscalls' here
                    core-y          += kernel/ certs/ mm/ fs/ ipc/ security/ crypto/ block/ my_syscalls
            6. Add the new system call to the system call table
                $ vi arch/x86/entry/syscalls/syscall_64.tbl
                434     common  hello                   __x64_sys_hello
            7. Add new system call to the system call header file
                vi include/linux/syscalls.h
                asmlinkage long sys_hello(void);
            8. Compile and Install the kernel
                $ make
                $ make modules_install
                $ make install
                Look at system.map and /proc/kallsyms for sys_hello ( sudo cat /proc/kallsyms | grep sys_hello )

59)
    Return value of system calls: Returning 0, notifies the calling process that the system call succeeded and that no errors occurred. The system C library will automatically take that negative value and put the appropriate value in the errno variable in user-space (and return -1 from the function).

    System call which accepts a string argument, logs it on the kernel buffer: As our system call has one argument, we use SYSCALL_DEFINE1, and our only parameter is a char * which we name string.
        Step1: Update my_syscalls/hello.c file
            SYSCALL_DEFINE1(hello_str, const char __user *, string){
                    char user_msg[256];
                pr_info("%s\n", __func__);
                    long copied = strncpy_from_user(user_msg, string, sizeof(user_msg));
                    if (copied < 0 || copied == sizeof(user_msg))
                            return -EFAULT;
                    pr_info("%s\n",user_msg);
                    return 0;
            }
            We are using strncpy_from_user, since we cannot directly access user space memory in kernel space. strncpy_from_user behaves like normal strncpy, but checks the user-space memory address first. If the string was too long or if there was a problem copying, we return EFAULT
        Step2: Update system call table
        Step3: Update include/linux/syscalls.h file
        Step4: Compile and install the kernel

    What happens if i pass a different type of argument? it fails with errornum.

    Generic System Call Table: Some architectures (e.g. x86) have their own architecture-specific syscall tables. Several other architectures share a generic syscall table. For your system call to be used by multiple architectures. Add your new system call to the generic list by adding an entry to the list in include/uapi/asm-generic/unistd.h
            #define __NR_hello 434
            __SYSCALL(__NR_hello, sys_hello)
        Also update the __NR_syscalls count to reflect the additional system call
            #define __NR_syscalls 435

    What happens when we compile the user space application for 32-bit will the code works? Note: Applications using the x32 ABI won't run on systems with 32-bit kernels.


    System Call Table: The system call table is represented by the sys_call_table array.
        Defined: arch/x86/entry/syscall_64.c
        asmlinkage const sys_call_ptr_t sys_call_table[__NR_syscall_max+1] = {
                /*
                 * Smells like a compiler bug -- it doesn't work
                 * when the & below is removed.
                 */
                [0 ... __NR_syscall_max] = &sys_ni_syscall,
        #include <asm/syscalls_64.h>
        };

    You can see, sys_call_table is an array of __NR_syscall_max + 1 size. __NR_syscall_max macro represents the maximum number of system calls for the given architecture. this macro in the header file generated by Kbuild during kernel compilation.

    all elements of array that contain pointers to the system call handlers point to the sys_ni_syscall. The sys_ni_syscall function represents not-implemented system calls. Implementation of the sys_ni_syscall is pretty easy, it just returns -errno or -ENOSYS in our case. To start with, all elements of the sys_call_table array point to the not-implemented system call.

    a GCC compiler extension called - Designated Initializers allows us to initialize elements in non-fixed order. we included the asm/syscalls_64.h header at the end of the array. This header file is generated by the special script at arch/x86/entry/syscalls/syscalltbl.sh and generates our header file from the syscall table.
        ./arch/x86/include/generated/asm/syscalls_64.h
        ./arch/x86/include/generated/asm/syscalls_32.h

    Compatibility System Calls" Most 64-bit implementations of system calls work fine when the user space program is 32-bit. There are a couple of situations where a compatibility layer is needed to cope with size differences between 32-bit and 64-bit. This is needed whenever a system call argument is
        a pointer to a pointer
        a pointer to a struct containing a pointer (e.g. struct iovec __user *)
        a pointer to a varying sized integral type (time_t, off_t, long, ...)
        a pointer to a struct containing a varying sized integral type.

60)
    Kernel Symbols: A symbol is either a variable or function. It is name representing an space in memory which stores data (variable for reading/writing) or instructions(function for executing). Kernel Symbols are visible at three different levels in the kernel source code.
        1. static  : visible only within their own source file
        2. extern  : visible to any other code built into the kernel
        3. exported: visible and available to any loadable module. Kernel code can make a symbol available to loadable modules with the EXPORT_SYMBOL() and EXPORT_SYMBOL_GPL() macros

    Kernel Symbol Table:
        1. System.map    : Contains symbols exported by code built into the kernel
        2. /proc/kallsyms: Contains symbols exported by code built into the kernel + loadable kernel modules

    How do you find out whether a function is static or global by looking into /proc/kallsyms?
        $ cat /proc/kallsysms | grep <function_name>
        $ cat /proc/kallsyms | grep msleep
    If you observe 't' it is static and 'T' then it is global symbol
        $ cat /proc/kallsyms | grep crc32c_mod_init
        /proc/kallsysms follows nm syntax
    All exported symbols will have another symbol prefixed with __ksymtab_

    A peek inside a LKM: All modules have their own symbols. Suppose we have a module hello.ko, with a lot of functions, variables and so on. After compiling it and obtaining the module’s object file, we can get a list of its symbols by running nm.
        $ nm ./hello.ko
            U -> Undefined
        $ nm ./hello.ko | grep static
            0000000000000000 d static_var
        $ nm ./hello.ko | grep global
            0000000000000000 T global_fn
            0000000000000004 D global_var
        $ nm ./hello.ko | grep local
    You cannot access to local variables from object files, because gcc does not save information about it.

    kallsyms_lookup_name:
        unsigned long kallsyms_lookup_name(const char *name);
    This takes symbol name as argument and returns its address in memory. The calling code can dereference the pointer to make use of that symbol. If the symbol isn’t found, the function returns NULL.
        $ sudo cat /proc/kallsyms | grep sys_call_table

    Dynamically find the address at which the kernel is loaded:
        $ sudo cat /proc/iomem | grep -i kernel

    kallsyms_on_each_symbol: Can be used to dump all the kernel symbols along with their addresses.
        int kallsyms_on_each_symbol(int (*fn)(void *, const char *, struct module *, unsigned long), void *data);
        fn -> callback function that is called for each symbol. callback function has the following prototype:
            int fn(void *data, const char *sym_name, struct module *module, unsigned long address);
        data: will contain pointer to your private data you passed as last argument to kallsyms_on_each_symbol()
        sym_name: will contain name of current kernel symbol
        module: name of the module
        address: will contain address of current kernel symbol

    Can i find the size of the function? Yes, using sprint_symbol.
        int sprint_symbol(char *buffer, unsigned long address);
    This function looks up a kernel symbol with 'address' and stores its name, offset, size, module name to buffer. If no symbol was found just saves its 'address' as is. This function returns the number of bytes stored in 'buffer'

    symbol_name+offset/size [module_name]

61)
    Can i use the function pointer returned by the kallsyms_lookup_name() to call the function? Yes

    Can we access non exported symbols using this logic? For example, print_cpu_info(). Yes, we can access those.

    Can i update the symbol address ? Just like read, you can also write to a symbol's address.
        Note: Be careful, some addresses are in rodata section or text section, which cannot be written If you try to write to a readonly address, you will probably get a kernel oops.

    In Intel x86 CPUs, the Control Register CR0 has a special bit (called Write Protect bit), to control whether the CPU can write to read-only pages while running in privilege level 0. syscall table is inside a page that is read-only, and by default the "Write Protect" bit is set, you are prevented from writing to it.

    In order to update the syscall correctly you will need to disable write protection by setting the "Write Protect" bit of the CR0 register to 0 before overwriting the table entry, and re-enable it after you're done

    How do i access the parameters passed to system call ? pt_regs.

62)
    How to find out how long your system is up? Command : uptime
        $ uptime
            19:33:25 up  2:52,  1 user,  load average: 0.08, 0.05, 0.01

    load average: 0.08, 0.05, 0.01 --- the average CPU load (average number of jobs in your system's run queue)for the 1, 5 and 15 minutes. Interpreting load average:
        If the averages are 0.0, then your system is idle. If the 1 minute average is higher than the 5 or 15 minute averages, then load is increasing. If the 1 minute average is lower than the 5 or 15 minute averages, then load is decreasing. If they are higher than your CPU count, then you might have a performance problem

    /proc/uptime: $ cat /proc/uptime
        10354.21 61534.94
            10354.21    --> Uptime in seconds
            61534.94    --> Total time spent by all processors in idle process

    /proc/loadavg: $ cat /proc/loadavg
        0.07 0.02 0.00 1/672 9133

    The first three fields in this file are load average figures giving the number of jobs in the run queue (state R) or waiting for disk I/O (state D) averaged over 1, 5, and 15 minutes. They are the same as the load average numbers given by uptime(1) and other programs. The fourth field consists of two numbers separated by a slash (/). The first of these is the number of currently executing kernel scheduling entities (processes, threads). The value after the slash is the number of kernel scheduling entities that currently exist on the system. The fifth field is the PID of the process that was most recently created on the system.

    Epoch Time/Unix Time: Number of seconds that have elapsed since January 1 1970
        Command: date +%s

    To get the epoch time from your C Program:
        #include <time.h>
            time_t time(time_t *tloc);
            time() returns the time as the number of seconds since the Epoch.
            If tloc is non-NULL, the return value is also stored in the memory pointed to by tloc
            Return value: On success, the value of time in seconds since the Epoch is returned.

    ctime: char *ctime(const time_t *timep);
        Header File: #include <time.h>
        The ctime() function returns the string representing the localtime based on the argument timer.

    gettimeofday(): gettimeofday() returns the current wall clock time and timezone.
        Header File: #include <sys/time.h>
        int gettimeofday(struct timeval *tv, struct timezone *tz);
            The tv argument is a struct timeval and gives the number of seconds and microseconds since the Epoch
            struct timeval {
                time_t      tv_sec;     /* seconds */
                suseconds_t tv_usec;    /* microseconds */
            };
        The tz argument is a struct timezone.
        struct timezone {
            int tz_minuteswest;     /* minutes west of Greenwich */
            int tz_dsttime;         /* type of DST correction */
        };
        From man gettimeofday:
            The use of the timezone structure is obsolete; the tz argument should normally  be  specified  as  NULL.

    Can I use gettimeofday() to determine elapsed time? If you want to display the time to a user of your program, you query the time of day. However, if your program needs to measure elapsed time, you need a timer that will give the right answer even if the user changes the time on the system clock.

    What if i change the system date using date command?
        To stop ntp service: $sudo systemctl stop systemd-timesyncd.service
        To start ntp service: $sudo systemctl start systemd-timesyncd.service

    The familiar system clock which tells the time of day is referred to as a real-time clock or a wall-clock. To safely measure elapsed time in a program, you need a clock that ticks out time continuously, without any jumps when a user sets the system time. This kind of clock is called a monotonic clock.
        int clock_gettime(clockid_t clk_id, struct timespec *tp);
    The above function can be used to retrieve the time from various clocks, including the real-time clock.

    clock_gettime vs gettimeofday:
        1. Higher Precision
            struct timespec {
                time_t   tv_sec;        /* seconds */
                long     tv_nsec;       /* nanoseconds */
            };
        2. Ability to request specific clocks.
            The clocks are identified by a clockid_t which is the first parameter of the function.
    For monotonic clock: clk_id = CLOCK_MONOTONIC

63)
    CLOCK_BOOTTIME: CLOCK_MONTONIC does not measure time spent in suspend, use CLOCK_BOOTTIME if you want to measure the time spend ins suspend also

    CLOCK_MONOTONIC_RAW: Similar to CLOCK_MONOTONIC, but provides access to a raw hardware-based time that is not  subject to NTP adjustments. CLOCK_MONOTONIC_RAW would be relevant mainly to cases where more accurate time is wanted over very short intervals. CLOCK_MONOTONIC would be preferable for longer-term timers measured in minutes, hours or days

    CLOCK_REALTIME: This clock represents the real time/wall time in the system. used for timestamping as well as providing actual time to the user. Can be both stepped and slewed by time adjustment code (e.g., NTP)

    CLOCK_PROCESS_CPUTIME_ID: Per-process CPU-time clock (measures CPU time consumed by all threads in the process). Useful for finding out whether your program is spending its total time in CPU or waiting for something else( a user input).

    CLOCK_THREAD_CPUTIME_ID: Measures the amount of CPU time consumed by the thread.

    clock_settime: The function clock_settime() sets the the time of the specified clock clk_id
        int clock_settime(clockid_t clk_id, const struct timespec *tp);

    NTP Service status:
        $ timedatectl status
        $ systemctl stop systemd-timesyncd.service

    clock_getres:
        int clock_getres(clockid_t clk_id, struct timespec *res);
            The function clock_getres() finds the resolution (precision) of the specified clock clk_id.

    times() system call:
        #include <sys/times.h>
        clock_t times(struct tms *buf);
            The times system call gets the user time and system time used by the current process and its children.
        User time: Time spent by the process executing in user space
        System Time:  Time spent by the process executing in kernel space.
                struct tms {
                    clock_t tms_utime;  /* user time */
                    clock_t tms_stime;  /* system time */
                    clock_t tms_cutime; /* user time of children */
                    clock_t tms_cstime; /* system time of children */
                };
        All times reported are in clock ticks. The number of clock ticks per second can be obtained using sysconf(_SC_CLK_TCK);

    Another option is getrusage: $ man getrusage
        struct timeval ru_utime; /* user CPU time used */
        struct timeval ru_stime; /* system CPU time used */

64)
    Timing Measurements in Linux Kernel: Many computer activities are based on timing measurements. E.g. Your Computer display is turned off, if you have not pressed a key or moved your mouse for a particular time. Linux timing subsystem mainly handles two types of timing activities:
        1. Keeping the current time and date
            a. time() , gettimeofday() and clock_gettime()
            b. Time stamps for files and network packets
        2. Maintaining Timers
            a. Mechanisms to notify kernel and user space (ex. alarm()) that a certain interval of time has elapsed.

    Hardware Devices: Linux depends on hardware devices to maintain time. These devices can be basically classified into two types:
        1. Clocks/Counters:  provide precise time measurements. Used to keep track of current time of day
        2. Timers: Issue interrupts at fixed, predefined frequency. Used for implementing software timers

    RTC (Real Time Clock): All PC's include a clock called Real Time Clock (RTC). It is independent of CPU and all other chips. It is backed by a small battery, which will allow it to tick even when the system is turned off. On boot, the kernel reads the RTC and uses it to initialize the wall time. The kernel does not typically read the value again. Can be accessed using /dev/rtc* file. Reading and setting the RTC is then done via calls to ioctl.

    Access RTC through command line: To get what is stored in the RTC hardware clock execute as superuser:$ hwclock

    To set the hardware clock to some date and time you can use: hwclock --set --date="10/08/89 13:19:00"
        $hwclock --debug
        dmesg | grep rtc
    status information of rtc is exposed through /proc/driver/rtc

    Why Real Time clock is not used after computer is powered on? The RTC is not used often because its registers are very slow to access.

    Time Stamp Counter (TSC): The time stamp counter (TSC) is a hardware feature found on a number of contemporary processors. The TSC is a special register which is simply incremented every clock cycle. Eg. If clock is 2GHz, the counter will increment twice per nanosecond. Since the clock is the fundamental unit of time as seen by the processor, the TSC provides the highest-resolution timing information available for that processor. It can thus be used for a number of applications, such as measuring the exact time cost of specific instructions or operations.
        $ dmesg | grep tsc

    Issues using TSC's with multi processor systems:
        1. each core had its own TSC, in order to be sure that two measurements were accurate relative to each
        other, it was neccessary to pin the measuring code to a single core.
        2. the TSC would run at the frequency of the CPU itself: if that changed due to power management, the TSC ON that CPU would also slow down or stop.
    Both of these problems are solved in more recent CPUs:
        • a constant TSC keeps all TSC’s synchronized across all cores in a system,
        • an invariant (or nonstop) TSC keeps the TSC running at a fixed rate regardless of changes in CPU frequency

    How to check whether my CPU supports this:
        $ cat /proc/cpuinfo | grep -i tsc
            Flag            Meaning
            tsc             The system has a TSC clock.
            rdtscp          The RDTSCP instruction is available.
            constant_tsc    The TSC is synchronized across all sockets/cores.
            nonstop_tsc     The TSC is not affected by power management code.

    The Timestamp Counter is a 64-bit internal register which is present in all Intel processors after the Pentium. It stores the number of cycles executed by the CPU after the latest reset. The time-stamp counter can be read by software using the RDTSC instruction

65)
    Programmable Interval Timer(PIT): In embedded systems, a programmable interval timer (PIT) is a counter that generates an output signal when it reaches a programmed count. The output signal may trigger an interrupt. PIT's may be:
        One-Shot: Signal only once and stop counting
        Periodic: Signal every time they reach a specific value and restart

    HPET (High Precision Event Timer): HPET is a timer chip developed by Intel and Microsoft. An HPET chip consists of a 64-bit up-counter (main counter) counting at a frequency of at least 10 MHz, and a set of (at least three, up to 256) comparators. These comparators are 32- or 64-bit-wide. Each comparator can generate an interrupt when the least significant bits are equal to the corresponding bits of the 64-bit main counter value. The comparators can be put into one-shot mode or periodic mode which means interrupts can be generated either once or periodically. The HPET is programmed via a memory mapped I/O and have relocatable address space. During system bootup, the BIOS sets up the registers address space and passes it to the kernel.
        $ sudo cat /proc/iomem | grep -i hpet

66)
    HZ.

    We know Linux works with timer interrupt. But how often should the interrupt trigger? The frequency of the timer interrupt is programmed on system boot based on static preprocessor define HZ.
        Example: If HZ = 100, Frequency = 100 hz, interrupt happens every 1/100 of second = 10ms
    The value of HZ differs for each supported architecture. Kernel defines the value in <asm/param.h>
        $ vi include/asm-generic/param.h

    How to check the value of HZ in the terminal?
        $ grep 'CONFIG_HZ=' /boot/config-$(uname -r)

    jiffies: Kernel maintains a global variable called "jiffies" which holds the number of ticks/timer interrupts from system boot. Every time a timer interrupt occurs, the value of an internal kernel counter is incremented.
        Header File: <linux/jiffies.h>
        extern unsigned long volatile jiffies;

    What is the size of jiffies: As jiffies is unsigned long.
        On 32-bit architectures size = 4 bytes
        On 64-bit architectures size = 8 bytes

    If the tick rate(HZ) is 100, 32-bit jiffies variable will overflow in about 497 days. If the tick rate(HZ) is 1000, 32-bit jiffies variable will overflow in about 49.7 days. Solution to avoid overflow is to store jiffies in 64-bit variable on all architectures.

    Why is jiffies not been directly declared as 64-bit unsigned long long integer on x86? Why is jiffies not been directly declared as 64-bit unsigned long long integer on x86?
        Access to 64-bit variables in 32-bit architectures cannot be done atomically.

    Every read operation on the whole 64-bit requires some synchronization technique to ensure that the counter is not updated while the two 32-bit half counters are read. A second variable is also defined in <linux/jiffies.h>
        extern u64 jiffies_64;

    In the case of x86 the jiffies will be the lower 32 bits of the jiffies_64 variable.
                        jiffies_64
    +-----------------------------------------------------+
    |                       |                             |
    |                       |                             |
    |                       |       jiffies on `x86`      |
    |                       |                             |
    |                       |                             |
    +-----------------------------------------------------+
    63                     31                             0

    Code that accesses jiffies simply reads the lower 32 bits of jiffies_64. The function get_jiffies_64() can be used to read the full 64-bit value.
        Defined in : kernel/time/jiffies.c
            #if (BITS_PER_LONG < 64)
            u64 get_jiffies_64(void)
            {
                    unsigned int seq;
                    u64 ret;

                    do {
                            seq = read_seqbegin(&jiffies_lock);
                            ret = jiffies_64;
                    } while (read_seqretry(&jiffies_lock, seq));
                    return ret;
            }
            EXPORT_SYMBOL(get_jiffies_64);
            #endif
    On 64-bit architectures, jiffies_64 and jiffies refer to the same thing. Code can either read jiffies or call get_jiffies_64() as both actions have the same effect.

    Jiffies Wraparound: The jiffies variable, like any C integer experiences overflow when its value is increased beyond its maximum storage limit. For a 32-bit unsigned integer, the maximum value is 2^32 - 1. When the tick count is equal to the maximum and it is incremented, it wraps around to zero. What happens if jiffies wrapped back to zero after setting timeout?
        unsigned long timeout = jiffies + 10 * HZ;
            if (timeout  > jiffies)
    Then the first condition will fail because the jiffies value would be smaller than timeout despite logically being larger.

    Kernel provides four macros for comparing tick counts that correctly handle the wrap around in the tick count.
        #define time_after(a,b)         \
                (typecheck(unsigned long, a) && \
                 typecheck(unsigned long, b) && \
                 ((long)((b) - (a)) < 0))
        #define time_before(a,b)        time_after(b,a)

        #define time_after_eq(a,b)      \
                (typecheck(unsigned long, a) && \
                 typecheck(unsigned long, b) && \
                 ((long)((a) - (b)) >= 0))
        #define time_before_eq(a,b)     time_after_eq(b,a)
    time_after(a,b) returns true if the time a is after time b

    Ex: time_after(10, 20) = (20 - 10) < 0 = 10 < 0 = false
        time_after(20, 10) = (10 - 20) < 0 = -10 < 0 = true

    time_before(a,b) returns true if the time a is before time b
    time_after_eq(a,b) returns true if the time a is after or equal to time b
    time_before_eq(a,b) returns true if the time a is before or equal to time b

    What is the initial value of jiffies on boot? At boot time jiffies is not initialized to zero, it is initialized to INITIAL_JIFFIES constant.
        Header File: <linux/jiffies.h>
          /*
           * Have the 32 bit jiffies value wrap 5 minutes after boot
           * so jiffies wrap bugs show up earlier.
           */
           #define INITIAL_JIFFIES ((unsigned long)(unsigned int) (-300*HZ))
    some bugs where found that occurred when the jiffies counter overflowed (wrapped back to zero). By initializing jiffies so the counter overflows after 5 mins, anybody testing a patch that creates a new jiffy-overflow-related bug  will see the problem during normal testing - and fix it. since it uses negative value while jiffies will be read as unsigned integer, it will be read as "very big" positive number. Thus, it will reach the upper limit of integer very quickly.

    How -300*HZ wraps 5 minutes after boot ? HZ is the number of clock ticks (jiffies) in 1 second. 300 seconds is 5 minutes. Therefore 300 x HZ is the number of jiffies in 5 minutes. Therefore after 5 minutes, an initial value of -300 * HZ will have been incremented to 0.

    Why do we have double casting in the below code.
        #define INITIAL_JIFFIES ((unsigned long)(unsigned int) (-300*HZ))
    the negative signed int (-300*HZ) is converted to an unsigned int with sign extension to whatever size an unsigned int is on that implementation (typically 32 bits). then the second cast (to unsigned long) casts that up to the size of an unsigned long (32 or 64 bits depending on implementation) with zero extension. In the case of 32 bit unsigned long, the second (leftmost) cast does nothing. In the case of 64 bit unsigned long, the second cast would cause the upper 32 bits to be set to 0.

    How can i convert jiffies in seconds/milliseconds? Jiffies can be converted to other time units such as milliseconds, microseconds, and nanoseconds using routines jiffies_to_msecs(), jiffies_to_usecs(), jiffies64_to_nsecs().
        unsigned int jiffies_to_msecs(const unsigned long j);
        unsigned int jiffies_to_usecs(const unsigned long j);
        u64 jiffies_to_nsecs(const unsigned long j);
            Header File: <linux/jiffies.h>
            Definition: kernel/time/time.c

67)
    Can i write to jiffies? yes, but the jiffies value is used internally by the scheduler, so bad things can happen modifying it.

    Timer Interrupt Processing: Constitutes of two parts
        a) Architecture Dependent
        b) Architecture Independent

    Architecture Dependent: For x86: arch/x86/kernel/time.c (Interrupt Handler for PIT/HPET)
            /*
             * Default timer interrupt handler for PIT/HPET
             */
            static irqreturn_t timer_interrupt(int irq, void *dev_id)
            {
                    global_clock_event->event_handler(global_clock_event);
                    return IRQ_HANDLED;
            }
        This finally calls architecture independent code

    Architecture Independent: Code: kernel/time/tick-common.c (tick_handle_periodic, tick_periodic). It does the following:
        1. Increment jiffies64
            void do_timer(unsigned long ticks)
            {
                    jiffies_64 += ticks;
                    calc_global_load(ticks);
            }
        2. Update statistics for the currently running process and the entire system (load average).
            void update_process_times(int user_tick);
        3. Run dynamic timers
        4. Run scheduler_tick() (Update timeslices information)

    Busy Waiting/Busy Looping: You can use jiffies to implement busy looping. Spin in a loop until the desired number of clock ticks/jiffies pass.

    Problem with this approach: The processor is tied up spinning in a silly loop—no useful work is accomplished!

    Better Solution: Reschedule your process to allow the processor to accomplish other work while your code waits.
        while (time_before(jiffies, delay))
            cond_resched();
    This solution conditionally invokes the scheduler only if there is some more important task to run. Note: Cannot use it from an interrupt handler, only in process context.

    Jiffie Conversions: User space programs represent time values with struct timeval and struct timespec. The kernel exports four helper functions to convert time values expressed as jiffies to and from those structures.
        Header File: #include <linux/time.h>
            unsigned long timespec_to_jiffies(struct timespec *value);
            void jiffies_to_timespec(unsigned long jiffies, struct timespec *value);
            unsigned long timeval_to_jiffies(struct timeval *value);
            void jiffies_to_timeval(unsigned long jiffies, struct timeval *value);

    How are busy loops implemented? kernel provides three functions for microsecond, nanosecond, and millisecond delays, defined in <linux/delay.h> and <asm/delay.h>
        void udelay(unsigned long usecs); //delays execution by busy looping for specified number of microseconds
        void ndelay(unsigned long nsecs); //delays execution by busy looping for specified number of nanoseconds
        void mdelay(unsigned long msecs); //delays execution by busy looping for specified number of milliseconds

    We know, 1 second = 1,000 milliseconds = 1,000,000 microseconds

    How is udelay implemented? They cannot use jiffies, as jiffies provide resolution in millisecond. On boot, kernel runs a function calibrate_delay() present in init/calibrate.c file. This function calculates and prints the value of "BogoMips". BogoMips value is the number of busy loop iterations the processor can perform in a given period. This value is stored in the loops_per_jiffy variable.
        $ cat /proc/cpuinfo | grep bogomips
        $ dmesg | grep -i bogo

    udelay() uses the loops_per_jiffy value to figure out how many busy loop iterations they need to execute to provide the required delay. mdelay() is then implemented in terms of udelay().
        Note: BogoMIPS have little to nothing to do with actual processor performance

    What is the disadvantage of having periodic ticks/interrupts in linux kernel ? The timer interrupts are good but has one major issue: Power consumption.
        With HZ option, kernel is interrupted HZ times per second in order to reschedule tasks. If HZ is set to 1000, there will be 1000 timer interrupts per second. This timer tick happens periodically irrespective of the processor state (idle vs busy). If the processor is idle, it has to wake up from its power saving sleep state every 1ms (if HZ=1000). Thus power consumption is increased.

    Dynamic Ticks/Tickless: Tickless/Dynamic Ticks is designed to solve the problem. In this case, kernel will program timers to only fire when necessary. Kernel sets timers up to fire at varying intervals depending on its requirements. Fewer interrupts means fewer wakeups, which means idle CPU's can be kept in low-power modes for longer periods, which saves energy. This feature significantly improves battery life in laptops.

    There are three main ways of managing scheduling-clock interrupts (clock ticks/ticks) ?
        1. Never omit scheduling-clock ticks
            CONFIG_HZ_PERIODIC=y or
            CONFIG_NO_HZ=n (on older CPUs)
        2. Omit ticks on Idle CPUs
            CONFIG_NO_HZ_IDLE=y or
            CONFIG_NO_HZ=n (on older CPUs)
            Most common approach, should be default
        3. Omit ticks on CPUs that are idle or that have only one runnable task
            CONFIG_NO_HZ_FULL=y
            Useful for real time application

    CONFIG_HZ_PERIODIC=y. Used by older versions from 1990s to 2000s

    Which scenarios this approach is useful?
        - heavy workloads with lots of tasks that use short bursts of CPU
        - where there are very frequent idle periods
        - but where these idle periods are also quite short (tens or hundreds of microseconds).

    Which scenarios this approach is not useful?
        - running a light workload with long idle periods
        - scheduling-clock interrupts will result in excessive power consumption
        - bad on battery-powered devices, where it results in extremely short battery lifetimes
    if you are running either a real-time workload or an HPC workload with short iterations the scheduling-clock interrupts can degrade your applications performance.

    CONFIG_NO_HZ_IDLE=y.  Primary purpose of clock interrupt is to force a busy CPU to shift its attention among multiple duties. If a CPU is idle, there is little point in sending it a scheduling-clock interrupt. The CONFIG_NO_HZ_IDLE=y Kconfig option causes the kernel to avoid sending scheduling-clock interrupts to idle CPUs.

    Advantage: A battery-powered device running a CONFIG_HZ_PERIODIC=y kernel would drain its battery very quickly, easily 2-3 times as fast as would the same device running a CONFIG_NO_HZ_IDLE=y kernel.

    Problems with this approach:
        1. It increases the number of instructions executed on the path to and from the idle loop.
        2. On many architectures, dyntick-idle mode also increases the  number of expensive clock-reprogramming operations.

    CONFIG_NO_HZ_FULL=y.  If a CPU has only one runnable task, there is little point in sending it a scheduling-clock interrupt because there is no other task to switch to. Note that omitting scheduling-clock ticks for CPUs with only one runnable task implies also omitting them for idle CPUs.

    Adaptive-ticks CPU: kernel avoiding sending scheduling-clock interrupts to CPUs with a single runnable task are called adaptive-tick cpu.

    Advantage for: applications with aggressive real-time response constraints because it allows them to improve their worst-case response times by the maximum duration of a scheduling-clock interrupt

68)
    Kernel Timers: Used to schedule execution of a function at a particular time in the near future

    Usages:
        1. Implementations of timeouts (schedule_timeout)
        2. Wait timeouts in TCP Implementation

    A timer is easy to use:
        1. Perform initialization
        2. Specify an expiration time
        3. A function to execute upon said expiration
        4. Activate the timer
    The given function runs after the timer expires
    Note: Timers are not cyclic. The timer is destroyed after it expires

    Types of Timers:
        1. Low Resolution Timer
        2. High Resolution Timer

    Initialization:
        Static:
            DEFINE_TIMER()
            #define DEFINE_TIMER(_name, _function)                          \
                    struct timer_list _name =                               \
                            __TIMER_INITIALIZER(_function, 0)
        Dynamic:
            void timer_setup( struct timer_list *timer, void (*function)(unsigned long), unsigned int flags);

    Setting up expiration time: When the timer is initialized, we need to set its expiration before the callback gets fired.
        int mod_timer( struct timer_list *timer, unsigned long expires);
    The expires field represents the jiffies value when the timer is expected to run. The kernel runs the timer handler when the current tick count is equal to or greater than the specified expiration. Although the kernel guarantees to run no timer handler prior to the timer’s expiration, there may be a delay in running the timer.

    Conversions from jiffies to seconds/nanoseconds:
        struct timeval {
            time_t tv_sec; /* seconds */
            suseconds_t tv_usec; /* microseconds */
        };
        struct timespec {
            time_t tv_sec; /* seconds */
            long tv_nsec; /* nanoseconds */
        };
    The timeval_to_jiffies and jiffies_to_timeval functions are used to convert between this representation and a jiffies value.
        Header File: <timer.h>

    Guess in which context timer handler runs? it runs in interrupt context, not in process context.

    Timer Implementation: The kernel executes timers in bottom-half context, as softirqs, after the timer interrupt completes. The timer interrupt handler runs update_process_times(), which calls run_local_timers():
        void run_local_timers(void)
        {
            .....
            raise_softirq(TIMER_SOFTIRQ); /* raise the timer softirq */
        }
    The TIMER_SOFTIRQ softirq is handled by run_timer_softirq().This function runs all the expired timers (if any) on the current processor.

    Are interrupts disabled while running timer handler? soft irqs runs with interrupts enabled, so interrupts are enabled while running timer handler. /* soft irq context, not in hard irq */

    Does kernel checks all the timer entries in linked list on each raise softirq? The kernel partitions timers into five groups based on their expiration value. Timers move down through the groups as their expiration time draws closer. The partitioning ensures that, in most executions of the timer softirq, the kernel has to do little work to
    find the expired timers.

    del_timer deactivates a timer.
        int del_timer(struct timer_list *timer);
            The function returns whether it has deactivated a pending timer or not. del_timer() of an inactive timer returns 0. del_timer() of an active timer returns 1.

    del_timer_sync:
        int del_timer_sync(struct timer_list *timer)
            deactivate a timer and wait for the handler to finish. This function only differs from del_timer() on SMP. The function returns whether it has deactivated a pending timer or not.

    Differences between del_timer and del_timer_sync: guarantees that when it returns, the timer function is not running on any CPU. del_timer_sync is used to avoid race conditions on SMP systems as timers run asynchronously with respect to the currently executing code, several potential race conditions exist.

    timer_pending: timer_pending will tell whether a given timer is currently pending or not.
        return value: 1 if the timer is pending, 0 if not

    Periodic timers: You can make the timer periodic by calling mod_timer in the timer handler.

    How can i synchronize between process context and timer handler? You need to use spin_lock API.

    Timer Flags:
        - TIMER_DEFERRABLE: A deferrable timer will work normally when the system is busy but will not cause a CPU to come out of idle just to service it. the timer will be serviced when the CPU eventually wakes up with a subsequent non-deferrable timer.
        - TIMER_IRQSAFE: An irqsafe timer is executed with IRQ disabled. with this, it will run with IRQ disabled

    schedule_timeout:
        long schedule_timeout (signed long timeout);
            Make the current task sleep until timeout jiffies have elapsed.
        Implementation: It internally uses kernel timers
            File: kernel/time/timer.c

    Problems with low resolution timers:
        1. Low resolution timers: in Linux are only supported at a resolution of 1 jiffy. The length of a jiffy is dependent on the value of HZ in the Linux kernel.
        2. Multimedia applications: timer resolution of several milliseconds is not good enough. need very precise timekeeping, for instance, to avoid frame skips in videos, or jumps during audio playback

    Implementation of low resolution timers: Timers are organized on lists, and the following data structure represents a timer on a list:
        Data Structure:
            struct timer_list;
            Header File: <linux/timer.h>
            Implementation: kernel/timer.c
                struct timer_list {
                        /*
                         * All fields that change during normal runtime grouped to the
                         * same cacheline
                         */
                        struct hlist_node       entry;
                        unsigned long           expires;
                        void                    (*function)(struct timer_list *);
                        u32                     flags;
                };
            function saves a pointer to the callback function invoked upon time-out expires specifies the time, in jiffies, at which the timer expires.

    Use cases in Linux Kernel: Timers are used to detect when a device or a network peer has failed to respond within the expected time; when, as usual, the expected response does happen, the timer is canceled

69)
    Problem with Low res timers: Low resolution timers  in Linux are only supported at a resolution of 1 jiffy. The length of a jiffy is dependent on the value of HZ in the Linux kernel. It is 1 millisecond on i386 and some other platforms, and 10 milliseconds on most embedded platforms. These are not suited for real-time applications.

    High Resolution Timers: Verify whether your kernel has support for High Res timers or not:
        $ cat /boot/config-`uname -r` | grep CONFIG_HIGH_RES_TIMERS
    You can also examine the timer_list, and see whether specific clocks are listed as supporting high resolution:
        $ sudo cat /proc/timer_list | grep resolution

    The .resolution entry must show 1 nsecs and the event_handler must show hrtimer_interrupts. Low Res timers are based on HZ(Jiffies), whereas High Res Timers based on ktime_t.
        Example of Low Resolution Timers: PIT
        Example of High Resolution Timers: LAPIC

    ktime_t: Data structure used to store a time value in nanoseconds.
        Header File : <linux/ktime.h>
    On 64-bit systems, a ktime_t is really just a 64-bit integer value in nanoseconds
        typedef s64     ktime_t;
    On 32-bit machines, however, it is a two-field structure:
        one 32-bit value holds the number of seconds and the other holds nanoseconds. The order of the two fields depends on whether the host architecture is big-endian or not. they are always arranged so that the two values can, when needed, be treated as a single, 64-bit value.

    Macros/functions on ktime_t: A whole set of functions and macros has been provided for working with ktime_t values. There are two ways of initializing them.
        1. DEFINE_KTIME(name);   /* Initialize to zero */ (not present)
        2. ktime_t kt;
            kt = ktime_set(long secs, long nanosecs);
                static inline ktime_t ktime_set(const s64 secs, const unsigned long nsecs)
                {
                        if (unlikely(secs >= KTIME_SEC_MAX))
                                return KTIME_MAX;

                        return secs * NSEC_PER_SEC + (s64)nsecs;
                }
                Return the ktime_t representation of the value

    ktime_t macros/functions: Various other functions exist for changing ktime_t values.
        ktime_t ktime_add(ktime_t kt1, ktime_t kt2);
        ktime_t ktime_sub(ktime_t kt1, ktime_t kt2);  /* kt1 - kt2 */
        ktime_t ktime_add_ns(ktime_t kt, u64 nanoseconds);
    All of these treat their arguments as read-only and return a ktime_t value as their result

    ktime accessors:
        ktime_t ktime_get_real(void);
        Header file: <linux/timekeeping.h>
        Clock used: CLOCK_REALTIME
            Returns the time in relative to the UNIX epoch starting in 1970 using the Coordinated Universal Time (UTC), same as gettimeofday() user space
            Use cases: timestamps that need to persist across a reboot, like inode times
        should be avoided for internal uses, since it can jump backwards due to a leap second update, NTP adjustment settimeofday() operation from user space.

70)
    nanosecond output:
        u64 ktime_get_ns(void);
        u64 ktime_get_boottime_ns(void);
        u64 ktime_get_real_ns(void);
    Same as the plain ktime_get functions, but returning a u64 number of nanoseconds in the respective time reference, which may be more convenient for some callers.

    Coarse variants:
            ktime_t ktime_get_coarse_boottime(void);
            ktime_t ktime_get_coarse_real(void);

            u64 ktime_get_coarse_boottime_ns(void);
            u64 ktime_get_coarse_real_ns(void);

            void ktime_get_coarse_ts64(struct timespec64 *);
            void ktime_get_coarse_boottime_ts64(struct timespec64 *);
            void ktime_get_coarse_real_ts64(struct timespec64 *);
        These are quicker than the non-coarse versions, but less accurate.

    Why is coarse variants faster than non-coarse variants: ktime_get() (unlike the ktime_get_coarse()) eventually invokes timekeeping_get_delta() which reads clocksource with tk_clock_read(). They doesn't read the hardware instead they read from a global timekeeping structure, which gets updated every timer tick. This can be 10ms in the past. Skipping the hardware clock access saves around 100 CPU cycles on most modern machines with a reliable cycle counter, but up to several microseconds on older hardware with an external clocksource.

71)
    Data Structures: hrtimers live on a time-sorted linked list, with the next timer to expire being at the head of the list. A separate red/black tree is also used to enable the insertion and removal of timer events without scanning through the list.
        Header File: <linux/hrtimer.h>
        Data Structures: struct hrtimer
        Implementation: kernel/time/hrtimer.c
        Documentation: Documentation/timers/hrtimer.txt
            struct hrtimer {
                    struct timerqueue_node          node;
                    ktime_t                         _softexpires;
                    enum hrtimer_restart            (*function)(struct hrtimer *);
                    struct hrtimer_clock_base       *base;
                    u8                              state;
                    u8                              is_rel;
                    u8                              is_soft;
            };
        node is used to keep the timer on red-black tree. base points to the timer base. softexpires is expiration time. function is callback function.

    clocks: Every hrtimer is bound to a specific clock. The system currently supports two clocks:
        1. CLOCK_MONOTONIC:
            a clock which is guaranteed always to move forward in time
            it starts at zero when the system boots and increases monotonically from there
        2. CLOCK_REALTIME:
            matches the current real-world time

    The difference between the two clocks can be seen when the system time is adjusted
        - as a result of administrator action
        - tweaking by the network time protocol code
        - suspending and resuming the system.

    In any of these situations,
        CLOCK_MONOTONIC will tick forward as if nothing had happened
        CLOCK_REALTIME may see discontinuous changes.

    Which clock to be used? Depends mainly on whether the timer needs to be tied to time as the rest of the world sees it or not

    Initialization: void hrtimer_init(struct hrtimer *timer, clockid_t clock_id, enum hrtimer_mode mode);
        timer:      the timer to be initialized
        clock_id:   the clock to be used. Every hrtimer is bound to a specific clock
        mode:       The modes which are relevant for intitialization

    clock_id:
        #define CLOCK_REALTIME                  0
        #define CLOCK_MONOTONIC                 1
        #define CLOCK_PROCESS_CPUTIME_ID        2
        #define CLOCK_THREAD_CPUTIME_ID         3
        #define CLOCK_MONOTONIC_RAW             4
        #define CLOCK_REALTIME_COARSE           5
        #define CLOCK_MONOTONIC_COARSE          6
        #define CLOCK_BOOTTIME                  7
        #define CLOCK_REALTIME_ALARM            8
        #define CLOCK_BOOTTIME_ALARM            9

    mode:
        HRTIMER_MODE_ABS             - Time value is absolute
        HRTIMER_MODE_REL             - Time value is relative to now
        HRTIMER_MODE_PINNED          - Timer is bound to CPU (is only considered when starting the timer)
        HRTIMER_MODE_SOFT            - Timer callback function will be executed in soft irq context

    Setting a timer:
        static inline void hrtimer_start(struct hrtimer *timer, ktime_t tim, const enum hrtimer_mode mode);
            timer:      the timer to be added
            tim:        expiry time
            mode:       timer mode: absolute (HRTIMER_MODE_ABS) or
                                    relative (HRTIMER_MODE_REL)
        $ sudo cat /proc/timer_list | grep my_hrtimer_function -C 3

    hrtimer_cancel: cancel a timer
        int hrtimer_cancel(struct hrtimer *timer);
        Returns:
                0 when the timer was not active
                1 when the timer was active

    Return value of callback function:
        HRTIMER_NORESTART   -  for a one-shot timer which should not be started again
        HRTIMER_RESTART     -  for a recurring timer
    In the restart case, the callback must set a new expiration time before returning. Usually, restarting timers are used by kernel subsystems which need a callback at a regular interval. The hrtimer code provides a function for advancing the expiration time to the next such interval:
        unsigned long hrtimer_forward(struct hrtimer *timer, ktime_t now, ktime_t interval);
            timer:      hrtimer to forward
            now:        forward past this time
            interval:   the interval to forward
        Returns the number of overruns.

    hrtimer_forward_now: Forward the timer expiry so it will expire after the current time of the hrtimer clock base. Returns the number of overruns.
        static inline u64 hrtimer_forward_now(struct hrtimer *timer, ktime_t interval);

    hrtimer_callback_running:
        static inline int hrtimer_callback_running(struct hrtimer *timer);
            Helper function to check, whether the timer is running the callback function
            Return: 1 if the callback is running, 0 if the callback is not running

    hrtimer_restart: A canceled timer can be restarted by passing it to hrtimer_restart().
        void hrtimer_restart(struct hrtimer *timer);

    hrtimer_get_remaining:
        ktime_t hrtimer_get_remaining(const struct hrtimer *timer);
            returns the amount of time left before a timer expires

    hrtimer_interrupt: When the clock event device responsible for high-resolution timers raises an interrupt, hrtimer_interrupt is called as event handler

    How can i wait for few usecs in linux kernel ?
        - Backed by busy-wait loop: udelay(unsigned long usecs)
        - Backed by hrtimers: usleep_range(unsigned long min, unsigned long max)

    Why is there no usleep / What is a good range? Since usleep_range is built on top of hrtimers, the wakeup will be very precise, thus a simple usleep function would likely introduce a large number of undesired interrupts.
