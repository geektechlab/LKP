Theory0:
Disabling Interrupts is a way to achieve mutual exclusion on single processor system

Interrupt Flags
-----------------

EFLAGS in x86 is a status register. It is a collection of bits which represents the state of the processor

9th bit of register is Interrupt Flag, which determines whether the processor will handle maskable hardware interrupts or not.

IF = 1, hardware interrupts will be handled
IF = 0, hardware interrupts will be ignored

Note: IF will not affect handling of NMIs and software interrupts generated by INT instruction.

Instructions
------------
CLI     --  Clear Interrupt Flag (Set IF = 0)
STI     --  Sets  Interrupt Flag (Set IF = 1)
POPF    --  Pops 32 bit off the stack into EFLAGS register.
                IF will be set or cleared depending on ninth bit on the top of the stack.

Disable interrupts on current CPU
-----------------------------------

local_irq_disable()

    Finally calls native_irq_disable()
        asm volatile("cli": : :"memory");

Enable interrupts on current CPU
---------------------------------

local_irq_enable()

    Finally calls native_irq_enable()
        asm volatile("sti": : :"memory");

Theory2:
Saving and Restoring Flags
-----------------------------

local_irq_save() and local_irq_restore() takes care of saving and restoring the interrupt states 

Instructions
---------------
popf    --  Pops 8/16/32/64 bit off the stack into EFLAGS register.                                                  
                 IF will be set or cleared depending on ninth bit on the top of the stack.

pushf   --  Pushes the entire flag register on to the stack.

local_irq_save()
----------------

 extern inline unsigned long native_save_fl(void)
 {
     unsigned long flags;
 
     /*
      * "=rm" is safe here, because "pop" adjusts the stack before
      * it evaluates its effective address -- this is part of the
      * documented behavior of the "pop" instruction.
      */
     asm volatile("# __raw_save_flags\n\t"
              "pushf ; pop %0"
              : "=rm" (flags)
              : /* no input */
              : "memory");
 
     return flags;
 }

Comments in Assembly starts with #

local_irq_restore()
-----------------------

extern inline void native_restore_fl(unsigned long flags)
{
    asm volatile("push %0 ; popf"
             : /* no output */
             :"g" (flags)
             :"memory", "cc");
}

Theory3:
Disabling and Enabling Interrupts from user space
-------------------------------------------------

CLI and STI are privileged instructions, which trigger a general protection fault if an unprivileged application attempts to execute it

The privilege level required to execute a CLI or STI instruction or set IF using POPF  is determined by the IOPL (I/O Privilege Level) in EFLAGS

Most modern operating systems set the IOPL to be 0 so only the kernel can execute CLI/STI.

Theory4:
Does executing CLI instruction disable interrupts on other processors
------------------------------------------------------------------------

In multiprocessor systems, executing a CLI instruction does not disable interrupts on other processors

Thus, a driver/interrupt handler race condition can still occur because other processors may service interrupts and execute the offending interrupt handler.

Other synchronization mechanisms such as locks must be used in addition to CLI/STI to prevent all race conditions.

Theory5:
I/O Address Space
------------------
x86 architecture has separate address space for up to 65536 I/O Ports

Instructions for accessing I/O Ports
-------------------------------------

IN and OUT transfer data between an I/O device and the microprocessor's accumulator (AL, AX or EAX)

AL - 8-bit I/O
AX - 16-bit I/O
EAX - 32-bit I/O

If a program specifies AL with the IN instruction, the processor transfers 8 bits from the selected port to AL.

If a program specifies AX with the IN instruction, the processor transfers 16 bits from the port to AX.

If a program specifies EAX with the IN instruction, the processor transfers 32 bits from the port to EAX.

The I/O address is stored in dx register.

File: arch/x86/include/asm/io.h

static inline void out##bwl(unsigned type value, int port)      \
{                                   \
    asm volatile("out" #bwl " %" #bw "0, %w1"           \
             : : "a"(value), "Nd"(port));           \
}                                   \
                                    \
static inline unsigned type in##bwl(int port)               \
{                                   \
    unsigned type value;                        \
    asm volatile("in" #bwl " %w1, %" #bw "0"            \
             : "=a"(value) : "Nd"(port));           \
    return value;                           \
}    


static inline void out##bwl##_p(unsigned type value, int port)      \
{                                   \
    out##bwl(value, port);                      \
    slow_down_io();                         \
}                                   \
                                    \
static inline unsigned type in##bwl##_p(int port)           \
{                                   \
    unsigned type value = in##bwl(port);                \
    slow_down_io();                         \
    return value;                           \
}          

Theory6:
iopl
---------

Instructions for accessing I/O ports are privileged, so user-space code cannot normally use them

One of the mechanism which allows user process access the I/O ports is to change the IOPL.

The I/O privilege level (IOPL) is a two-bit variable that controls how much privilege a process must have to access I/O ports

It is normally set to zero, meaning that this access is only available when running in kernel mode.

Setting it to three makes I/O-port operations available to ordinary user-space processes.

Changing the I/O privilege level for a specific process (done with the iopl() system call) can thus make all I/O ports available to that process

Theory7:
What is atomic operation
---------------------------

An atomic operation is an operation that will always be executed without any other process being able to read or change state that is read or changed during the operation. 

Methods for ensuring an operation is atomic
----------------------------------------------

Single processor-single core systems
------------------------------------

On a single processor system, if an operation is implemented in a single CPU instruction, it is always atomic
Ex: xchg, inc

If operation is implemented in multiple CPU instructions, then it can be interrupted in middle of execution.
a) Interrupts
b) Context switch

Techniques for atomicity
--------------------------
    a. spinlock
    b. Disabling interrupts

Multiprocessor or multicore systems
------------------------------------

On multiprocessor systems, ensuring atomicity exists is a little harder.

Disabling interrupts or using a single instruction will not guarantee atomic access

It is still possible to use a lock (e.g. a spinlock)

Technique
-----------

To ensure that no other processor or core in the system attempts to access the data you are working with, use LOCK signal on the bus, which prevents any other processor in the system from accessing the memory at the same time.

On x86 processors,
    some instructions automatically lock the bus (e.g. 'XCHG') 
    while others require you to specify a 'LOCK' prefix to the instruction to achieve this (e.g. 'CMPXCHG', which you should write as 'LOCK CMPXCHG op1, op2').

Note: The lock prefix only works for individual x86 instructions
If you have block of instructions, then you need to use other techniques

Theory8:
Incrementing counter takes multiple instructions.

So, we need atomic integer operations for incrementing counters.

Protecting a counter with a complex locking scheme(eg. spin_lock) is overkill

Linux kernel provides atomic operations (Data type:atomic_t , operations: atomic_inc/atomic_dec)

atomic_inc
------------

static __always_inline void arch_atomic_inc(atomic_t *v)
{
        asm volatile(LOCK_PREFIX "incl %0"
                     : "+m" (v->counter) :: "memory");
}

atomic_dec
-------------

static __always_inline void arch_atomic_dec(atomic_t *v)
{
        asm volatile(LOCK_PREFIX "decl %0"
                     : "+m" (v->counter) :: "memory");
}

Theory9:
Locking technique for multiple instructions
--------------------------------------------

Simplest Mechanism:
=====================

a lock is a 1-bit variable, a value of

	1 indicates a process is in the critical section

	0 indicates no process is in the critical section

while (lock == 1)	do_nothing; //busy-wait loop
lock = 1;
//critical section
lock = 0;

Assembly:

lock: mv mem(lock), %eax	#load value of lock into eax
      cmp $0, eax		# if 0 store 1
      bnz lock			# else try again
      mv $1, mem(lock)		

unlock: mv 0, mem(lock)		# load 0

Do you see any problem in the above logic?
----------------------------------------------

Locking technique for multiple instructions
--------------------------------------------

Assembly:

lock: mv mem(lock), %eax	#load value of lock into eax
      cmp $0, eax		# if 0 store 1
      bnz lock			# else try again
      mv $1, mem(lock)		

unlock: mv 0, mem(lock)		# load 0

Do you see any problem in the above logic?
----------------------------------------------

data race because load-test-store is not atomic!

processor 0 loads address of lock, observes 0
processor 1 loads address of lock, observes 0
processor 0 writes 1 to address lock
processor 1 writes 1 to address lock

Theory10:
Locking atomically
------------------

We need to make sure no one gets between load and store

Solution: atomic compare and swap

compare and swap operation should be performed atomically.
	compares the contents of a memory location with a given value and, only if they are the same, modifies the contents of that memory location to a new given value.

tmp = *addr ; //load
if (tmp == old)
	*addr = new; //store

The above operation should be performed atomically.

x86 provides instruction cmpxchg. Using lock prefix guarantees atomicity

Look into definition of spin_lock() using cscope

tatic __always_inline void queued_spin_lock(struct qspinlock *lock)
{
        u32 val = 0;

        if (likely(atomic_try_cmpxchg_acquire(&lock->val, &val, _Q_LOCKED_VAL)))
                return;

        queued_spin_lock_slowpath(lock, val);
}

---
spinlock also internally uses cmpxchg instruction only
